{"bibcode": "2001MNRAS.322..231K", "title": "On the variation of the initial mass function", "abstract": "A universal initial mass function (IMF) is not intuitive, but so far no convincing evidence for a variable IMF exists. The detection of systematic variations of the IMF with star-forming conditions would be the Rosetta Stone for star formation. In this contribution an average or Galactic-field IMF is defined, stressing that there is evidence for a change in the power-law index at only two masses: near 0.5M<SUB>solar</SUB> and near 0.08M<SUB>solar</SUB>. Using this supposed universal IMF, the uncertainty inherent in any observational estimate of the IMF is investigated by studying the scatter introduced by Poisson noise and the dynamical evolution of star clusters. It is found that this apparent scatter reproduces quite well the observed scatter in power-law index determinations, thus defining the fundamental limit within which any true variation becomes undetectable. The absence of evidence for a variable IMF means that any true variation of the IMF in well-studied populations must be smaller than this scatter. Determinations of the power-law indices α are subject to systematic errors arising mostly from unresolved binaries. The systematic bias is quantified here, with the result that the single-star IMFs for young star clusters are systematically steeper by Δα~0.5 between 0.1 and 1M<SUB>solar</SUB> than the Galactic-field IMF, which is populated by, on average, about 5-Gyr-old stars. The MFs in globular clusters appear to be, on average, systematically flatter than the Galactic-field IMF (Piotto &amp; Zoccali; Paresce &amp; De Marchi), and the recent detection of ancient white-dwarf candidates in the Galactic halo and the absence of associated low-mass stars (Ibata et al.; Méndez &amp; Minniti) suggest a radically different IMF for this ancient population. Star formation in higher metallicity environments thus appears to produce relatively more low-mass stars. While still tentative, this is an interesting trend, being consistent with a systematic variation of the IMF as expected from theoretical arguments.", "database": ["astronomy"], "keywords": ["BINARIES: GENERAL", "STARS: FORMATION", "STARS: KINEMATICS", "STARS: LUMINOSITY FUNCTION", "MASS FUNCTION", "GLOBULAR CLUSTERS: GENERAL", "OPEN CLUSTERS AND ASSOCIATIONS: GENERAL", "Astrophysics"], "year": "2001", "doctype": "article", "citation_count": 6933, "domain_category": "astronomy", "abstract_clean": "A universal initial mass function (IMF) is not intuitive, but so far no convincing evidence for a variable IMF exists. The detection of systematic variations of the IMF with star-forming conditions would be the Rosetta Stone for star formation. In this contribution an average or Galactic-field IMF is defined, stressing that there is evidence for a change in the power-law index at only two masses: near 0.5Msolar and near 0.08Msolar. Using this supposed universal IMF, the uncertainty inherent in any observational estimate of the IMF is investigated by studying the scatter introduced by Poisson noise and the dynamical evolution of star clusters. It is found that this apparent scatter reproduces quite well the observed scatter in power-law index determinations, thus defining the fundamental limit within which any true variation becomes undetectable. The absence of evidence for a variable IMF means that any true variation of the IMF in well-studied populations must be smaller than this scatter. Determinations of the power-law indices α are subject to systematic errors arising mostly from unresolved binaries. The systematic bias is quantified here, with the result that the single-star IMFs for young star clusters are systematically steeper by Δα~0.5 between 0.1 and 1Msolar than the Galactic-field IMF, which is populated by, on average, about 5-Gyr-old stars. The MFs in globular clusters appear to be, on average, systematically flatter than the Galactic-field IMF (Piotto & Zoccali; Paresce & De Marchi), and the recent detection of ancient white-dwarf candidates in the Galactic halo and the absence of associated low-mass stars (Ibata et al.; Méndez & Minniti) suggest a radically different IMF for this ancient population. Star formation in higher metallicity environments thus appears to produce relatively more low-mass stars. While still tentative, this is an interesting trend, being consistent with a systematic variation of the IMF as expected from theoretical arguments."}
{"bibcode": "1999PhRvL..83.4690R", "title": "An Alternative to Compactification", "abstract": "Conventional wisdom states that Newton's force law implies only four noncompact dimensions. We demonstrate that this is not necessarily true in the presence of a nonfactorizable background geometry. The specific example we study is a single 3-brane embedded in five dimensions. We show that even without a gap in the Kaluza-Klein spectrum, four-dimensional Newtonian and general relativistic gravity is reproduced to more than adequate precision.", "database": ["astronomy", "physics"], "keywords": ["High Energy Physics - Theory"], "year": "1999", "doctype": "article", "citation_count": 6866, "domain_category": "astronomy", "abstract_clean": "Conventional wisdom states that Newton's force law implies only four noncompact dimensions. We demonstrate that this is not necessarily true in the presence of a nonfactorizable background geometry. The specific example we study is a single 3-brane embedded in five dimensions. We show that even without a gap in the Kaluza-Klein spectrum, four-dimensional Newtonian and general relativistic gravity is reproduced to more than adequate precision."}
{"bibcode": "1998ARA&A..36..189K", "title": "Star Formation in Galaxies Along the Hubble Sequence", "abstract": "Observations of star formation rates (SFRs) in galaxies provide vital clues to the physical nature of the Hubble sequence and are key probes of the evolutionary histories of galaxies. The focus of this review is on the broad patterns in the star formation properties of galaxies along the Hubble sequence and their implications for understanding galaxy evolution and the physical processes that drive the evolution. Star formation in the disks and nuclear regions of galaxies are reviewed separately, then discussed within a common interpretive framework. The diagnostic methods used to measure SFRs are also reviewed, and a self-consistent set of SFR calibrations is presented as an aid to workers in the field.", "database": ["astronomy"], "keywords": ["Astrophysics"], "year": "1998", "doctype": "article", "citation_count": 6365, "domain_category": "astronomy", "abstract_clean": "Observations of star formation rates (SFRs) in galaxies provide vital clues to the physical nature of the Hubble sequence and are key probes of the evolutionary histories of galaxies. The focus of this review is on the broad patterns in the star formation properties of galaxies along the Hubble sequence and their implications for understanding galaxy evolution and the physical processes that drive the evolution. Star formation in the disks and nuclear regions of galaxies are reviewed separately, then discussed within a common interpretive framework. The diagnostic methods used to measure SFRs are also reviewed, and a self-consistent set of SFR calibrations is presented as an aid to workers in the field."}
{"bibcode": "1973PhRvD...7.2333B", "title": "Black Holes and Entropy", "abstract": "There are a number of similarities between black-hole physics and thermodynamics. Most striking is the similarity in the behaviors of black-hole area and of entropy: Both quantities tend to increase irreversibly. In this paper we make this similarity the basis of a thermodynamic approach to black-hole physics. After a brief review of the elements of the theory of information, we discuss black-hole physics from the point of view of information theory. We show that it is natural to introduce the concept of black-hole entropy as the measure of information about a black-hole interior which is inaccessible to an exterior observer. Considerations of simplicity and consistency, and dimensional arguments indicate that the black-hole entropy is equal to the ratio of the black-hole area to the square of the Planck length times a dimensionless constant of order unity. A different approach making use of the specific properties of Kerr black holes and of concepts from information theory leads to the same conclusion, and suggests a definite value for the constant. The physical content of the concept of black-hole entropy derives from the following generalized version of the second law: When common entropy goes down a black hole, the common entropy in the black-hole exterior plus the black-hole entropy never decreases. The validity of this version of the second law is supported by an argument from information theory as well as by several examples.", "database": ["astronomy", "physics"], "keywords": [], "year": "1973", "doctype": "article", "citation_count": 6360, "domain_category": "astronomy", "abstract_clean": "There are a number of similarities between black-hole physics and thermodynamics. Most striking is the similarity in the behaviors of black-hole area and of entropy: Both quantities tend to increase irreversibly. In this paper we make this similarity the basis of a thermodynamic approach to black-hole physics. After a brief review of the elements of the theory of information, we discuss black-hole physics from the point of view of information theory. We show that it is natural to introduce the concept of black-hole entropy as the measure of information about a black-hole interior which is inaccessible to an exterior observer. Considerations of simplicity and consistency, and dimensional arguments indicate that the black-hole entropy is equal to the ratio of the black-hole area to the square of the Planck length times a dimensionless constant of order unity. A different approach making use of the specific properties of Kerr black holes and of concepts from information theory leads to the same conclusion, and suggests a definite value for the constant. The physical content of the concept of black-hole entropy derives from the following generalized version of the second law: When common entropy goes down a black hole, the common entropy in the black-hole exterior plus the black-hole entropy never decreases. The validity of this version of the second law is supported by an argument from information theory as well as by several examples."}
{"bibcode": "2005MNRAS.364.1105S", "title": "The cosmological simulation code GADGET-2", "abstract": "We discuss the cosmological simulation code GADGET-2, a new massively parallel TreeSPH code, capable of following a collisionless fluid with the N-body method, and an ideal gas by means of smoothed particle hydrodynamics (SPH). Our implementation of SPH manifestly conserves energy and entropy in regions free of dissipation, while allowing for fully adaptive smoothing lengths. Gravitational forces are computed with a hierarchical multipole expansion, which can optionally be applied in the form of a TreePM algorithm, where only short-range forces are computed with the `tree' method while long-range forces are determined with Fourier techniques. Time integration is based on a quasi-symplectic scheme where long-range and short-range forces can be integrated with different time-steps. Individual and adaptive short-range time-steps may also be employed. The domain decomposition used in the parallelization algorithm is based on a space-filling curve, resulting in high flexibility and tree force errors that do not depend on the way the domains are cut. The code is efficient in terms of memory consumption and required communication bandwidth. It has been used to compute the first cosmological N-body simulation with more than 10<SUP>10</SUP> dark matter particles, reaching a homogeneous spatial dynamic range of 10<SUP>5</SUP> per dimension in a three-dimensional box. It has also been used to carry out very large cosmological SPH simulations that account for radiative cooling and star formation, reaching total particle numbers of more than 250 million. We present the algorithms used by the code and discuss their accuracy and performance using a number of test problems. GADGET-2 is publicly released to the research community.", "database": ["astronomy"], "keywords": ["methods: numerical", "galaxies: interactions", "dark matter", "Astrophysics"], "year": "2005", "doctype": "article", "citation_count": 6062, "domain_category": "astronomy", "abstract_clean": "We discuss the cosmological simulation code GADGET-2, a new massively parallel TreeSPH code, capable of following a collisionless fluid with the N-body method, and an ideal gas by means of smoothed particle hydrodynamics (SPH). Our implementation of SPH manifestly conserves energy and entropy in regions free of dissipation, while allowing for fully adaptive smoothing lengths. Gravitational forces are computed with a hierarchical multipole expansion, which can optionally be applied in the form of a TreePM algorithm, where only short-range forces are computed with the `tree' method while long-range forces are determined with Fourier techniques. Time integration is based on a quasi-symplectic scheme where long-range and short-range forces can be integrated with different time-steps. Individual and adaptive short-range time-steps may also be employed. The domain decomposition used in the parallelization algorithm is based on a space-filling curve, resulting in high flexibility and tree force errors that do not depend on the way the domains are cut. The code is efficient in terms of memory consumption and required communication bandwidth. It has been used to compute the first cosmological N-body simulation with more than 1010 dark matter particles, reaching a homogeneous spatial dynamic range of 105 per dimension in a three-dimensional box. It has also been used to carry out very large cosmological SPH simulations that account for radiative cooling and star formation, reaching total particle numbers of more than 250 million. We present the algorithms used by the code and discuss their accuracy and performance using a number of test problems. GADGET-2 is publicly released to the research community."}
{"bibcode": "2014ChPhC..38i0001O", "title": "Review of Particle Physics", "abstract": "The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 3,283 new measurements from 899 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as heavy neutrinos, supersymmetric and technicolor particles, axions, dark photons, etc. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as Supersymmetry, Extra Dimensions, Particle Detectors, Probability, and Statistics. Among the 112 reviews are many that are new or heavily revised including those on: Dark Energy, Higgs Boson Physics, Electroweak Model, Neutrino Cross Section Measurements, Monte Carlo Neutrino Generators, Top Quark, Dark Matter, Dynamical Electroweak Symmetry Breaking, Accelerator Physics of Colliders, High-Energy Collider Parameters, Big Bang Nucleosynthesis, Astrophysical Constants and Cosmological Parameters. <P />All tables, listings, and reviews (and errata) are also available on the Particle Data Group website: <A href=\"http://pdg.lbl.gov\">http://pdg.lbl.gov</A>. Contents Abstract, Contributors, Highlights and Table of Contents<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0001-0008.pdf\">Acrobat PDF (4.4 MB)</A> Introduction<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0009-0024.pdf\">Acrobat PDF (595 KB)</A> Particle Physics Summary Tables Gauge and Higgs bosons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0027-0029.pdf\">Acrobat PDF (204 KB)</A> Leptons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0030-0032.pdf\">Acrobat PDF (167 KB)</A> Quarks<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0033.pdf\">Acrobat PDF (115 KB)</A> Mesons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0034-0078.pdf\">Acrobat PDF (976 KB)</A> Baryons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0079-0093.pdf\">Acrobat PDF (384 KB)</A> Searches (Supersymmetry, Compositeness, etc.)<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0094-0095.pdf\">Acrobat PDF (120 KB)</A> Tests of conservation laws<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0096-0106.pdf\">Acrobat PDF (383 KB)</A> Reviews, Tables, and Plots Detailed contents for this section<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0107.pdf\">Acrobat PDF (73 KB)</A> Constants, Units, Atomic and Nuclear Properties<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0109-0121.pdf\">Acrobat PDF (395 KB)</A> Standard Model and Related Topics<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0122-0321.pdf\">Acrobat PDF (8.37 MB)</A> Astrophysics and Cosmology<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0322-0385.pdf\">Acrobat PDF (3.79 MB)</A> Experimental Methods and Colliders<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0386-0466.pdf\">Acrobat PDF (3.82 MB)</A> Mathematical Tools of Statistics, Monte Carlo, Group Theory <A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0467-0507.pdf\">Acrobat PDF (1.77 MB)</A> Kinematics, Cross-Section Formulae, and Plots<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0508-0544.pdf\">Acrobat PDF (3.57 MB)</A> Particle Listings Illustrative key and abbreviations<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0547-0556.pdf\">Acrobat PDF (325 KB)</A> Gauge and Higgs bosons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0557-0644.pdf\">Acrobat PDF (2.38 MB)</A> Leptons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0645-0721.pdf\">Acrobat PDF (2.03 MB)</A> Quarks<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0723-0770.pdf\">Acrobat PDF (1.51 MB)</A> Mesons: Light unflavored and strange<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0771-0964.pdf\">Acrobat PDF (4.91 MB)</A> Mesons: Charmed and bottom<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_0965-1247.pdf\">Acrobat PDF (9.03 MB)</A> Mesons: Other<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_1248-1367.pdf\">Acrobat PDF (4.03 MB)</A> Baryons<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_1369-1543.pdf\">Acrobat PDF (4.54 MB)</A> Miscellaneous searches<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_1545-1658.pdf\">Acrobat PDF (3.98 MB)</A> Index<A href=\"http://iopscience.iop.org/1674-1137/38/9/090001/media/rpp2014_1661-1676.pdf\">Acrobat PDF (276 KB)</A>", "database": ["astronomy", "physics"], "keywords": [], "year": "2014", "doctype": "article", "citation_count": 5841, "domain_category": "astronomy", "abstract_clean": "The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 3,283 new measurements from 899 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as heavy neutrinos, supersymmetric and technicolor particles, axions, dark photons, etc. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as Supersymmetry, Extra Dimensions, Particle Detectors, Probability, and Statistics. Among the 112 reviews are many that are new or heavily revised including those on: Dark Energy, Higgs Boson Physics, Electroweak Model, Neutrino Cross Section Measurements, Monte Carlo Neutrino Generators, Top Quark, Dark Matter, Dynamical Electroweak Symmetry Breaking, Accelerator Physics of Colliders, High-Energy Collider Parameters, Big Bang Nucleosynthesis, Astrophysical Constants and Cosmological Parameters. All tables, listings, and reviews (and errata) are also available on the Particle Data Group website: http://pdg.lbl.gov. Contents Abstract, Contributors, Highlights and Table of ContentsAcrobat PDF (4.4 MB) IntroductionAcrobat PDF (595 KB) Particle Physics Summary Tables Gauge and Higgs bosonsAcrobat PDF (204 KB) LeptonsAcrobat PDF (167 KB) QuarksAcrobat PDF (115 KB) MesonsAcrobat PDF (976 KB) BaryonsAcrobat PDF (384 KB) Searches (Supersymmetry, Compositeness, etc.)Acrobat PDF (120 KB) Tests of conservation lawsAcrobat PDF (383 KB) Reviews, Tables, and Plots Detailed contents for this sectionAcrobat PDF (73 KB) Constants, Units, Atomic and Nuclear PropertiesAcrobat PDF (395 KB) Standard Model and Related TopicsAcrobat PDF (8.37 MB) Astrophysics and CosmologyAcrobat PDF (3.79 MB) Experimental Methods and CollidersAcrobat PDF (3.82 MB) Mathematical Tools of Statistics, Monte Carlo, Group Theory Acrobat PDF (1.77 MB) Kinematics, Cross-Section Formulae, and PlotsAcrobat PDF (3.57 MB) Particle Listings Illustrative key and abbreviationsAcrobat PDF (325 KB) Gauge and Higgs bosonsAcrobat PDF (2.38 MB) LeptonsAcrobat PDF (2.03 MB) QuarksAcrobat PDF (1.51 MB) Mesons: Light unflavored and strangeAcrobat PDF (4.91 MB) Mesons: Charmed and bottomAcrobat PDF (9.03 MB) Mesons: OtherAcrobat PDF (4.03 MB) BaryonsAcrobat PDF (4.54 MB) Miscellaneous searchesAcrobat PDF (3.98 MB) IndexAcrobat PDF (276 KB)"}
{"bibcode": "2006IJMPD..15.1753C", "title": "Dynamics of Dark Energy", "abstract": "We review in detail a number of approaches that have been adopted to try and explain the remarkable observation of our accelerating universe. In particular we discuss the arguments for and recent progress made towards understanding the nature of dark energy. We review the observational evidence for the current accelerated expansion of the universe and present a number of dark energy models in addition to the conventional cosmological constant, paying particular attention to scalar field models such as quintessence, K-essence, tachyon, phantom and dilatonic models. The importance of cosmological scaling solutions is emphasized when studying the dynamical system of scalar fields including coupled dark energy. We study the evolution of cosmological perturbations allowing us to confront them with the observation of the Cosmic Microwave Background and Large Scale Structure and demonstrate how it is possible in principle to reconstruct the equation of state of dark energy by also using Supernovae Ia observational data. We also discuss in detail the nature of tracking solutions in cosmology, particle physics and braneworld models of dark energy, the nature of possible future singularities, the effect of higher order curvature terms to avoid a Big Rip singularity, and approaches to modifying gravity which leads to a late-time accelerated expansion without recourse to a new form of dark energy.", "database": ["astronomy", "physics"], "keywords": ["Dark energy", "cosmological constant", "scalar fields", "particle physics", "modified gravity", "High Energy Physics - Theory", "Astrophysics", "General Relativity and Quantum Cosmology", "High Energy Physics - Phenomenology"], "year": "2006", "doctype": "article", "citation_count": 5838, "domain_category": "astronomy", "abstract_clean": "We review in detail a number of approaches that have been adopted to try and explain the remarkable observation of our accelerating universe. In particular we discuss the arguments for and recent progress made towards understanding the nature of dark energy. We review the observational evidence for the current accelerated expansion of the universe and present a number of dark energy models in addition to the conventional cosmological constant, paying particular attention to scalar field models such as quintessence, K-essence, tachyon, phantom and dilatonic models. The importance of cosmological scaling solutions is emphasized when studying the dynamical system of scalar fields including coupled dark energy. We study the evolution of cosmological perturbations allowing us to confront them with the observation of the Cosmic Microwave Background and Large Scale Structure and demonstrate how it is possible in principle to reconstruct the equation of state of dark energy by also using Supernovae Ia observational data. We also discuss in detail the nature of tracking solutions in cosmology, particle physics and braneworld models of dark energy, the nature of possible future singularities, the effect of higher order curvature terms to avoid a Big Rip singularity, and approaches to modifying gravity which leads to a late-time accelerated expansion without recourse to a new form of dark energy."}
{"bibcode": "2013ApJS..208...19H", "title": "Nine-year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Cosmological Parameter Results", "abstract": "We present cosmological parameter constraints based on the final nine-year Wilkinson Microwave Anisotropy Probe (WMAP) data, in conjunction with a number of additional cosmological data sets. The WMAP data alone, and in combination, continue to be remarkably well fit by a six-parameter ΛCDM model. When WMAP data are combined with measurements of the high-l cosmic microwave background anisotropy, the baryon acoustic oscillation scale, and the Hubble constant, the matter and energy densities, Ω<SUB> b </SUB> h <SUP>2</SUP>, Ω<SUB> c </SUB> h <SUP>2</SUP>, and Ω<SUB>Λ</SUB>, are each determined to a precision of ~1.5%. The amplitude of the primordial spectrum is measured to within 3%, and there is now evidence for a tilt in the primordial spectrum at the 5σ level, confirming the first detection of tilt based on the five-year WMAP data. At the end of the WMAP mission, the nine-year data decrease the allowable volume of the six-dimensional ΛCDM parameter space by a factor of 68,000 relative to pre-WMAP measurements. We investigate a number of data combinations and show that their ΛCDM parameter fits are consistent. New limits on deviations from the six-parameter model are presented, for example: the fractional contribution of tensor modes is limited to r &lt; 0.13 (95% CL); the spatial curvature parameter is limited to \\Omega _k = -0.0027^{+ 0.0039}_{- 0.0038}; the summed mass of neutrinos is limited to ∑m <SUB>ν</SUB> &lt; 0.44 eV (95% CL); and the number of relativistic species is found to lie within N <SUB>eff</SUB> = 3.84 ± 0.40, when the full data are analyzed. The joint constraint on N <SUB>eff</SUB> and the primordial helium abundance, Y <SUB>He</SUB>, agrees with the prediction of standard big bang nucleosynthesis. We compare recent Planck measurements of the Sunyaev-Zel'dovich effect with our seven-year measurements, and show their mutual agreement. Our analysis of the polarization pattern around temperature extrema is updated. This confirms a fundamental prediction of the standard cosmological model and provides a striking illustration of acoustic oscillations and adiabatic initial conditions in the early universe.", "database": ["astronomy"], "keywords": ["cosmic background radiation", "cosmology: observations", "dark matter", "early universe", "instrumentation: detectors", "space vehicles", "space vehicles: instruments", "telescopes", "Astrophysics - Cosmology and Nongalactic Astrophysics"], "year": "2013", "doctype": "article", "citation_count": 5495, "domain_category": "astronomy", "abstract_clean": "We present cosmological parameter constraints based on the final nine-year Wilkinson Microwave Anisotropy Probe (WMAP) data, in conjunction with a number of additional cosmological data sets. The WMAP data alone, and in combination, continue to be remarkably well fit by a six-parameter ΛCDM model. When WMAP data are combined with measurements of the high-l cosmic microwave background anisotropy, the baryon acoustic oscillation scale, and the Hubble constant, the matter and energy densities, Ω b h 2, Ω c h 2, and ΩΛ, are each determined to a precision of ~1.5%. The amplitude of the primordial spectrum is measured to within 3%, and there is now evidence for a tilt in the primordial spectrum at the 5σ level, confirming the first detection of tilt based on the five-year WMAP data. At the end of the WMAP mission, the nine-year data decrease the allowable volume of the six-dimensional ΛCDM parameter space by a factor of 68,000 relative to pre-WMAP measurements. We investigate a number of data combinations and show that their ΛCDM parameter fits are consistent. New limits on deviations from the six-parameter model are presented, for example: the fractional contribution of tensor modes is limited to r < 0.13 (95% CL); the spatial curvature parameter is limited to \\Omega _k = -0.0027^{+ 0.0039}_{- 0.0038}; the summed mass of neutrinos is limited to ∑m ν < 0.44 eV (95% CL); and the number of relativistic species is found to lie within N eff = 3.84 ± 0.40, when the full data are analyzed. The joint constraint on N eff and the primordial helium abundance, Y He, agrees with the prediction of standard big bang nucleosynthesis. We compare recent Planck measurements of the Sunyaev-Zel'dovich effect with our seven-year measurements, and show their mutual agreement. Our analysis of the polarization pattern around temperature extrema is updated. This confirms a fundamental prediction of the standard cosmological model and provides a striking illustration of acoustic oscillations and adiabatic initial conditions in the early universe."}
{"bibcode": "1987PASP...99..191S", "title": "DAOPHOT: A Computer Program for Crowded-Field Stellar Photometry", "abstract": "The tasks of the DAOPHOT program, developed to exploit the capability of photometrically linear image detectors to perform stellar photometry in crowded fields, are discussed. Raw CCD images are prepared prior to analysis, and following the obtaining of an initial star list with the FIND program, synthetic aperture photometry is performed on the detected objects with the PHOT routine. A local sky brightness and a magnitude are computed for each star in each of the specified stellar apertures, and for crowded fields, the empirical point-spread function must then be obtained for each data frame. The GROUP routine divides the star list for a given frame into optimum subgroups, and then the NSTAR routine is used to obtain photometry for all the stars in the frame by means of least- squares profile fits. The process is illustrated with images of stars in a crowded field, and shortcomings and possible improvements of the program are considered.", "database": ["astronomy"], "keywords": ["Algorithms", "Astronomical Photometry", "Computer Programs", "Charge Coupled Devices", "Color-Magnitude Diagram", "Magellanic Clouds", "Point Spread Functions", "Television Cameras", "COMPUTER PROGRAMMING AND SOFTWARE", "data-handling techniques", "photometry (general)"], "year": "1987", "doctype": "article", "citation_count": 5416, "domain_category": "astronomy", "abstract_clean": "The tasks of the DAOPHOT program, developed to exploit the capability of photometrically linear image detectors to perform stellar photometry in crowded fields, are discussed. Raw CCD images are prepared prior to analysis, and following the obtaining of an initial star list with the FIND program, synthetic aperture photometry is performed on the detected objects with the PHOT routine. A local sky brightness and a magnitude are computed for each star in each of the specified stellar apertures, and for crowded fields, the empirical point-spread function must then be obtained for each data frame. The GROUP routine divides the star list for a given frame into optimum subgroups, and then the NSTAR routine is used to obtain photometry for all the stars in the frame by means of least- squares profile fits. The process is illustrated with images of stars in a crowded field, and shortcomings and possible improvements of the program are considered."}
{"bibcode": "2009ApJS..180..330K", "title": "Five-Year Wilkinson Microwave Anisotropy Probe Observations: Cosmological Interpretation", "abstract": "The Wilkinson Microwave Anisotropy Probe (WMAP) 5-year data provide stringent limits on deviations from the minimal, six-parameter Λ cold dark matter model. We report these limits and use them to constrain the physics of cosmic inflation via Gaussianity, adiabaticity, the power spectrum of primordial fluctuations, gravitational waves, and spatial curvature. We also constrain models of dark energy via its equation of state, parity-violating interaction, and neutrino properties, such as mass and the number of species. We detect no convincing deviations from the minimal model. The six parameters and the corresponding 68% uncertainties, derived from the WMAP data combined with the distance measurements from the Type Ia supernovae (SN) and the Baryon Acoustic Oscillations (BAO) in the distribution of galaxies, are: Ω<SUB> b </SUB> h <SUP>2</SUP> = 0.02267<SUP>+0.00058</SUP> <SUB>-0.00059</SUB>, Ω<SUB> c </SUB> h <SUP>2</SUP> = 0.1131 ± 0.0034, Ω<SUB>Λ</SUB> = 0.726 ± 0.015, n<SUB>s</SUB> = 0.960 ± 0.013, τ = 0.084 ± 0.016, and Δ_{R}^2 = (2.445± 0.096)× 10^{-9} at k = 0.002 Mpc<SUP>-1</SUP>. From these, we derive σ<SUB>8</SUB> = 0.812 ± 0.026, H <SUB>0</SUB> = 70.5 ± 1.3 km s<SUP>-1</SUP> Mpc<SUP>-1</SUP>, Ω<SUB> b </SUB> = 0.0456 ± 0.0015, Ω<SUB> c </SUB> = 0.228 ± 0.013, Ω<SUB> m </SUB> h <SUP>2</SUP> = 0.1358<SUP>+0.0037</SUP> <SUB>-0.0036</SUB>, z <SUB>reion</SUB> = 10.9 ± 1.4, and t <SUB>0</SUB> = 13.72 ± 0.12 Gyr. With the WMAP data combined with BAO and SN, we find the limit on the tensor-to-scalar ratio of r &lt; 0.22(95%CL), and that n<SUB>s</SUB> &gt; 1 is disfavored even when gravitational waves are included, which constrains the models of inflation that can produce significant gravitational waves, such as chaotic or power-law inflation models, or a blue spectrum, such as hybrid inflation models. We obtain tight, simultaneous limits on the (constant) equation of state of dark energy and the spatial curvature of the universe: -0.14 &lt; 1 + w &lt; 0.12(95%CL) and -0.0179 &lt; Ω<SUB> k </SUB> &lt; 0.0081(95%CL). We provide a set of \"WMAP distance priors,\" to test a variety of dark energy models with spatial curvature. We test a time-dependent w with a present value constrained as -0.33 &lt; 1 + w <SUB>0</SUB> &lt; 0.21 (95% CL). Temperature and dark matter fluctuations are found to obey the adiabatic relation to within 8.9% and 2.1% for the axion-type and curvaton-type dark matter, respectively. The power spectra of TB and EB correlations constrain a parity-violating interaction, which rotates the polarization angle and converts E to B. The polarization angle could not be rotated more than -5fdg9 &lt; Δα &lt; 2fdg4 (95% CL) between the decoupling and the present epoch. We find the limit on the total mass of massive neutrinos of ∑m <SUB>ν</SUB> &lt; 0.67 eV(95%CL), which is free from the uncertainty in the normalization of the large-scale structure data. The number of relativistic degrees of freedom (dof), expressed in units of the effective number of neutrino species, is constrained as N <SUB>eff</SUB> = 4.4 ± 1.5 (68%), consistent with the standard value of 3.04. Finally, quantitative limits on physically-motivated primordial non-Gaussianity parameters are -9 &lt; f <SUP>local</SUP> <SUB>NL</SUB> &lt; 111 (95% CL) and -151 &lt; f <SUP>equil</SUP> <SUB>NL</SUB> &lt; 253 (95% CL) for the local and equilateral models, respectively. <P />WMAP is the result of a partnership between Princeton University and NASA's Goddard Space Flight Center. Scientific guidance is provided by the WMAP Science Team.", "database": ["astronomy"], "keywords": ["cosmic microwave background", "cosmology: observations", "dark matter", "early universe", "instrumentation: detectors", "space vehicles: instruments", "telescopes", "Astrophysics"], "year": "2009", "doctype": "article", "citation_count": 5408, "domain_category": "astronomy", "abstract_clean": "The Wilkinson Microwave Anisotropy Probe (WMAP) 5-year data provide stringent limits on deviations from the minimal, six-parameter Λ cold dark matter model. We report these limits and use them to constrain the physics of cosmic inflation via Gaussianity, adiabaticity, the power spectrum of primordial fluctuations, gravitational waves, and spatial curvature. We also constrain models of dark energy via its equation of state, parity-violating interaction, and neutrino properties, such as mass and the number of species. We detect no convincing deviations from the minimal model. The six parameters and the corresponding 68% uncertainties, derived from the WMAP data combined with the distance measurements from the Type Ia supernovae (SN) and the Baryon Acoustic Oscillations (BAO) in the distribution of galaxies, are: Ω b h 2 = 0.02267+0.00058 -0.00059, Ω c h 2 = 0.1131 ± 0.0034, ΩΛ = 0.726 ± 0.015, ns = 0.960 ± 0.013, τ = 0.084 ± 0.016, and Δ_{R}^2 = (2.445± 0.096)× 10^{-9} at k = 0.002 Mpc-1. From these, we derive σ8 = 0.812 ± 0.026, H 0 = 70.5 ± 1.3 km s-1 Mpc-1, Ω b = 0.0456 ± 0.0015, Ω c = 0.228 ± 0.013, Ω m h 2 = 0.1358+0.0037 -0.0036, z reion = 10.9 ± 1.4, and t 0 = 13.72 ± 0.12 Gyr. With the WMAP data combined with BAO and SN, we find the limit on the tensor-to-scalar ratio of r < 0.22(95%CL), and that ns > 1 is disfavored even when gravitational waves are included, which constrains the models of inflation that can produce significant gravitational waves, such as chaotic or power-law inflation models, or a blue spectrum, such as hybrid inflation models. We obtain tight, simultaneous limits on the (constant) equation of state of dark energy and the spatial curvature of the universe: -0.14 < 1 + w < 0.12(95%CL) and -0.0179 < Ω k < 0.0081(95%CL). We provide a set of \"WMAP distance priors,\" to test a variety of dark energy models with spatial curvature. We test a time-dependent w with a present value constrained as -0.33 < 1 + w 0 < 0.21 (95% CL). Temperature and dark matter fluctuations are found to obey the adiabatic relation to within 8.9% and 2.1% for the axion-type and curvaton-type dark matter, respectively. The power spectra of TB and EB correlations constrain a parity-violating interaction, which rotates the polarization angle and converts E to B. The polarization angle could not be rotated more than -5fdg9 < Δα < 2fdg4 (95% CL) between the decoupling and the present epoch. We find the limit on the total mass of massive neutrinos of ∑m ν < 0.67 eV(95%CL), which is free from the uncertainty in the normalization of the large-scale structure data. The number of relativistic degrees of freedom (dof), expressed in units of the effective number of neutrino species, is constrained as N eff = 4.4 ± 1.5 (68%), consistent with the standard value of 3.04. Finally, quantitative limits on physically-motivated primordial non-Gaussianity parameters are -9 < f local NL < 111 (95% CL) and -151 < f equil NL < 253 (95% CL) for the local and equilateral models, respectively. WMAP is the result of a partnership between Princeton University and NASA's Goddard Space Flight Center. Scientific guidance is provided by the WMAP Science Team."}
{"bibcode": "2000ApJ...533..682C", "title": "The Dust Content and Opacity of Actively Star-forming Galaxies", "abstract": "We present far-infrared (FIR) photometry at 150 and 205 μm of eight low-redshift starburst galaxies obtained with the Infrared Space Observatory (ISO) ISOPHOT. Five of the eight galaxies are detected in both wave bands, and these data are used, in conjunction with IRAS archival photometry, to model the dust emission at λ&gt;~40 μm. The FIR spectral energy distributions (SEDs) are best fitted by a combination of two modified Planck functions, with T~40-55 K (warm dust) and T~20-23 K (cool dust) and with a dust emissivity index ɛ=2. The cool dust can be a major contributor to the FIR emission of starburst galaxies, representing up to 60% of the total flux. This component is heated not only by the general interstellar radiation field, but also by the starburst itself. The cool dust mass is up to ~150 times larger than the warm dust mass, bringing the gas-to-dust ratios of the starbursts in our sample close to Milky Way values, once rescaled for the appropriate metallicity. The ratio between the total dust FIR emission in the range 1-1000 μm and the IRAS FIR emission in the range 40-120 μm is ~1.75, with small variations from galaxy to galaxy. This ratio is about 40% larger than previously inferred from data at millimeter wavelengths. Although the galaxies in our sample are generally classified as ``UV bright,'' for four of them the UV energy emerging shortward of 0.2 μm is less than 15% of the FIR energy. On average, about 30% of the bolometric flux is coming out in the UV-to-near-IR wavelength range; the rest is emitted in the FIR. Energy balance calculations show that the FIR emission predicted by the dust reddening of the UV-to-near-IR stellar emission is within a factor of ~2 of the observed value in individual galaxies and within 20% when averaged over a large sample. If our sample of local starbursts is representative of high-redshift (z&gt;~1), UV-bright, star-forming galaxies, these galaxies' FIR emission will be generally undetected in submillimeter surveys, unless (1) their bolometric luminosity is comparable to or larger than that of ultraluminous FIR galaxies and (2) their FIR SED contains a cool dust component. Based on observations with ISO, an ESA project with instruments funded by ESA member states (especially the PI countries: France, Germany, the Netherlands, and the United Kingdom) with the participation of ISAS and NASA.", "database": ["astronomy"], "keywords": ["GALAXIES: STARBURST", "INFRARED: GALAXIES", "INFRARED: ISM: CONTINUUM", "ISM: DUST", "EXTINCTION", "Astrophysics"], "year": "2000", "doctype": "article", "citation_count": 5401, "domain_category": "astronomy", "abstract_clean": "We present far-infrared (FIR) photometry at 150 and 205 μm of eight low-redshift starburst galaxies obtained with the Infrared Space Observatory (ISO) ISOPHOT. Five of the eight galaxies are detected in both wave bands, and these data are used, in conjunction with IRAS archival photometry, to model the dust emission at λ>~40 μm. The FIR spectral energy distributions (SEDs) are best fitted by a combination of two modified Planck functions, with T~40-55 K (warm dust) and T~20-23 K (cool dust) and with a dust emissivity index ɛ=2. The cool dust can be a major contributor to the FIR emission of starburst galaxies, representing up to 60% of the total flux. This component is heated not only by the general interstellar radiation field, but also by the starburst itself. The cool dust mass is up to ~150 times larger than the warm dust mass, bringing the gas-to-dust ratios of the starbursts in our sample close to Milky Way values, once rescaled for the appropriate metallicity. The ratio between the total dust FIR emission in the range 1-1000 μm and the IRAS FIR emission in the range 40-120 μm is ~1.75, with small variations from galaxy to galaxy. This ratio is about 40% larger than previously inferred from data at millimeter wavelengths. Although the galaxies in our sample are generally classified as ``UV bright,'' for four of them the UV energy emerging shortward of 0.2 μm is less than 15% of the FIR energy. On average, about 30% of the bolometric flux is coming out in the UV-to-near-IR wavelength range; the rest is emitted in the FIR. Energy balance calculations show that the FIR emission predicted by the dust reddening of the UV-to-near-IR stellar emission is within a factor of ~2 of the observed value in individual galaxies and within 20% when averaged over a large sample. If our sample of local starbursts is representative of high-redshift (z>~1), UV-bright, star-forming galaxies, these galaxies' FIR emission will be generally undetected in submillimeter surveys, unless (1) their bolometric luminosity is comparable to or larger than that of ultraluminous FIR galaxies and (2) their FIR SED contains a cool dust component. Based on observations with ISO, an ESA project with instruments funded by ESA member states (especially the PI countries: France, Germany, the Netherlands, and the United Kingdom) with the participation of ISAS and NASA."}
{"bibcode": "2018PhRvD..98c0001T", "title": "Review of Particle Physics<SUP>*</SUP>", "abstract": "The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 2,873 new measurements from 758 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as supersymmetric particles, heavy bosons, axions, dark photons, etc. Particle properties and search limits are listed in Summary Tables. We give numerous tables, figures, formulae, and reviews of topics such as Higgs Boson Physics, Supersymmetry, Grand Unified Theories, Neutrino Mixing, Dark Energy, Dark Matter, Cosmology, Particle Detectors, Colliders, Probability and Statistics. Among the 118 reviews are many that are new or heavily revised, including a new review on Neutrinos in Cosmology. <P />Starting with this edition, the Review is divided into two volumes. Volume 1 includes the Summary Tables and all review articles. Volume 2 consists of the Particle Listings. Review articles that were previously part of the Listings are now included in volume 1. <P />The complete Review (both volumes) is published online on the website of the Particle Data Group (http://pdg.lbl.gov) and in a journal. Volume 1 is available in print as the PDG Book. A Particle Physics Booklet with the Summary Tables and essential tables, figures, and equations from selected review articles is also available. <P />The 2018 edition of the Review of Particle Physics should be cited as: M. Tanabashi et al. (Particle Data Group), Phys. Rev. D 98, 030001 (2018).", "database": ["astronomy", "physics"], "keywords": [], "year": "2018", "doctype": "article", "citation_count": 5383, "domain_category": "astronomy", "abstract_clean": "The Review summarizes much of particle physics and cosmology. Using data from previous editions, plus 2,873 new measurements from 758 papers, we list, evaluate, and average measured properties of gauge bosons and the recently discovered Higgs boson, leptons, quarks, mesons, and baryons. We summarize searches for hypothetical particles such as supersymmetric particles, heavy bosons, axions, dark photons, etc. Particle properties and search limits are listed in Summary Tables. We give numerous tables, figures, formulae, and reviews of topics such as Higgs Boson Physics, Supersymmetry, Grand Unified Theories, Neutrino Mixing, Dark Energy, Dark Matter, Cosmology, Particle Detectors, Colliders, Probability and Statistics. Among the 118 reviews are many that are new or heavily revised, including a new review on Neutrinos in Cosmology. Starting with this edition, the Review is divided into two volumes. Volume 1 includes the Summary Tables and all review articles. Volume 2 consists of the Particle Listings. Review articles that were previously part of the Listings are now included in volume 1. The complete Review (both volumes) is published online on the website of the Particle Data Group (http://pdg.lbl.gov) and in a journal. Volume 1 is available in print as the PDG Book. A Particle Physics Booklet with the Summary Tables and essential tables, figures, and equations from selected review articles is also available. The 2018 edition of the Review of Particle Physics should be cited as: M. Tanabashi et al. (Particle Data Group), Phys. Rev. D 98, 030001 (2018)."}
{"bibcode": "2005ApJ...622..759G", "title": "HEALPix: A Framework for High-Resolution Discretization and Fast Analysis of Data Distributed on the Sphere", "abstract": "HEALPix-the Hierarchical Equal Area isoLatitude Pixelization-is a versatile structure for the pixelization of data on the sphere. An associated library of computational algorithms and visualization software supports fast scientific applications executable directly on discretized spherical maps generated from very large volumes of astronomical data. Originally developed to address the data processing and analysis needs of the present generation of cosmic microwave background experiments (e.g., BOOMERANG, WMAP), HEALPix can be expanded to meet many of the profound challenges that will arise in confrontation with the observational output of future missions and experiments, including, e.g., Planck, Herschel, SAFIR, and the Beyond Einstein inflation probe. In this paper we consider the requirements and implementation constraints on a framework that simultaneously enables an efficient discretization with associated hierarchical indexation and fast analysis/synthesis of functions defined on the sphere. We demonstrate how these are explicitly satisfied by HEALPix.", "database": ["astronomy"], "keywords": ["Cosmology: Cosmic Microwave Background", "Cosmology: Observations", "Methods: Statistical", "Astrophysics"], "year": "2005", "doctype": "article", "citation_count": 5248, "domain_category": "astronomy", "abstract_clean": "HEALPix-the Hierarchical Equal Area isoLatitude Pixelization-is a versatile structure for the pixelization of data on the sphere. An associated library of computational algorithms and visualization software supports fast scientific applications executable directly on discretized spherical maps generated from very large volumes of astronomical data. Originally developed to address the data processing and analysis needs of the present generation of cosmic microwave background experiments (e.g., BOOMERANG, WMAP), HEALPix can be expanded to meet many of the profound challenges that will arise in confrontation with the observational output of future missions and experiments, including, e.g., Planck, Herschel, SAFIR, and the Beyond Einstein inflation probe. In this paper we consider the requirements and implementation constraints on a framework that simultaneously enables an efficient discretization with associated hierarchical indexation and fast analysis/synthesis of functions defined on the sphere. We demonstrate how these are explicitly satisfied by HEALPix."}
{"bibcode": "2009ApJS..182..543A", "title": "The Seventh Data Release of the Sloan Digital Sky Survey", "abstract": "This paper describes the Seventh Data Release of the Sloan Digital Sky Survey (SDSS), marking the completion of the original goals of the SDSS and the end of the phase known as SDSS-II. It includes 11,663 deg<SUP>2</SUP> of imaging data, with most of the ~2000 deg<SUP>2</SUP> increment over the previous data release lying in regions of low Galactic latitude. The catalog contains five-band photometry for 357 million distinct objects. The survey also includes repeat photometry on a 120° long, 2fdg5 wide stripe along the celestial equator in the Southern Galactic Cap, with some regions covered by as many as 90 individual imaging runs. We include a co-addition of the best of these data, going roughly 2 mag fainter than the main survey over 250 deg<SUP>2</SUP>. The survey has completed spectroscopy over 9380 deg<SUP>2</SUP> the spectroscopy is now complete over a large contiguous area of the Northern Galactic Cap, closing the gap that was present in previous data releases. There are over 1.6 million spectra in total, including 930,000 galaxies, 120,000 quasars, and 460,000 stars. The data release includes improved stellar photometry at low Galactic latitude. The astrometry has all been recalibrated with the second version of the USNO CCD Astrograph Catalog, reducing the rms statistical errors at the bright end to 45 milliarcseconds per coordinate. We further quantify a systematic error in bright galaxy photometry due to poor sky determination; this problem is less severe than previously reported for the majority of galaxies. Finally, we describe a series of improvements to the spectroscopic reductions, including better flat fielding and improved wavelength calibration at the blue end, better processing of objects with extremely strong narrow emission lines, and an improved determination of stellar metallicities.", "database": ["astronomy"], "keywords": ["atlases", "catalogs", "surveys", "Astrophysics"], "year": "2009", "doctype": "article", "citation_count": 5037, "domain_category": "astronomy", "abstract_clean": "This paper describes the Seventh Data Release of the Sloan Digital Sky Survey (SDSS), marking the completion of the original goals of the SDSS and the end of the phase known as SDSS-II. It includes 11,663 deg2 of imaging data, with most of the ~2000 deg2 increment over the previous data release lying in regions of low Galactic latitude. The catalog contains five-band photometry for 357 million distinct objects. The survey also includes repeat photometry on a 120° long, 2fdg5 wide stripe along the celestial equator in the Southern Galactic Cap, with some regions covered by as many as 90 individual imaging runs. We include a co-addition of the best of these data, going roughly 2 mag fainter than the main survey over 250 deg2. The survey has completed spectroscopy over 9380 deg2 the spectroscopy is now complete over a large contiguous area of the Northern Galactic Cap, closing the gap that was present in previous data releases. There are over 1.6 million spectra in total, including 930,000 galaxies, 120,000 quasars, and 460,000 stars. The data release includes improved stellar photometry at low Galactic latitude. The astrometry has all been recalibrated with the second version of the USNO CCD Astrograph Catalog, reducing the rms statistical errors at the bright end to 45 milliarcseconds per coordinate. We further quantify a systematic error in bright galaxy photometry due to poor sky determination; this problem is less severe than previously reported for the majority of galaxies. Finally, we describe a series of improvements to the spectroscopic reductions, including better flat fielding and improved wavelength calibration at the blue end, better processing of objects with extremely strong narrow emission lines, and an improved determination of stellar metallicities."}
{"bibcode": "2000PhR...323..183A", "title": "Large N field theories, string theory and gravity", "abstract": "We review the holographic correspondence between field theories and string/M theory, focusing on the relation between compactifications of string/M theory on Anti-de Sitter spaces and conformal field theories. We review the background for this correspondence and discuss its motivations and the evidence for its correctness. We describe the main results that have been derived from the correspondence in the regime that the field theory is approximated by classical or semiclassical gravity. We focus on the case of the N=4 supersymmetric gauge theory in four dimensions, but we discuss also field theories in other dimensions, conformal and non-conformal, with or without supersymmetry, and in particular the relation to QCD. We also discuss some implications for black hole physics.", "database": ["astronomy", "physics"], "keywords": ["High Energy Physics - Theory", "General Relativity and Quantum Cosmology", "High Energy Physics - Lattice", "High Energy Physics - Phenomenology"], "year": "2000", "doctype": "article", "citation_count": 4982, "domain_category": "astronomy", "abstract_clean": "We review the holographic correspondence between field theories and string/M theory, focusing on the relation between compactifications of string/M theory on Anti-de Sitter spaces and conformal field theories. We review the background for this correspondence and discuss its motivations and the evidence for its correctness. We describe the main results that have been derived from the correspondence in the regime that the field theory is approximated by classical or semiclassical gravity. We focus on the case of the N=4 supersymmetric gauge theory in four dimensions, but we discuss also field theories in other dimensions, conformal and non-conformal, with or without supersymmetry, and in particular the relation to QCD. We also discuss some implications for black hole physics."}
{"bibcode": "1974ApJ...187..425P", "title": "Formation of Galaxies and Clusters of Galaxies by Self-Similar Gravitational Condensation", "abstract": "We consider an expanding Friedmann cosmology containing a \"gas\" of self-gravitating masses. The masses condense into aggregates which (when sufficiently bound) we identify as single particles of a larger mass. We propose that after this process has proceeded through several scales, the mass spectrum of condensations becomes \"self-similar\" and independent of the spectrum initially assumed. Some details of the self-similar distribution, and its evolution in time, can be calculated with the linear perturbation theory. Unlike other authors, we make no ad hoc assumptions about the spectrum of long-wavelength initial perturbatidns: the nonlinear N-body interactions of the mass points randomize their positions and generate a perturbation to all larger scales; this should fix the self-similar distribution almost uniquely. The results of numerical experiments on 1000 bodies are presented; these appear to show new nonlinear effects: condensations can \"bootstrap\" their way up in size faster than the linear theory predicts. Our self-similar model predicts relations between the masses and radii of galaxies and clusters of galaxies, as well as their mass spectra. We compare the predictions with available data, and find some rather striking agreements. If the model is to explain galaxies, then isothermal \"seed\" masses of 3 x 1 0 M0 must have existed at recombination. To explain clusters of galaxies, the only necessary seeds are the galaxies themselves. The size of clusters determines, in principle, the deceleration parameter q0 presently available data give only very broad limits, unfortunately. Subject headings: cosmology - galaxies - galaxies, clusters of", "database": ["astronomy"], "keywords": [], "year": "1974", "doctype": "article", "citation_count": 4930, "domain_category": "astronomy", "abstract_clean": "We consider an expanding Friedmann cosmology containing a \"gas\" of self-gravitating masses. The masses condense into aggregates which (when sufficiently bound) we identify as single particles of a larger mass. We propose that after this process has proceeded through several scales, the mass spectrum of condensations becomes \"self-similar\" and independent of the spectrum initially assumed. Some details of the self-similar distribution, and its evolution in time, can be calculated with the linear perturbation theory. Unlike other authors, we make no ad hoc assumptions about the spectrum of long-wavelength initial perturbatidns: the nonlinear N-body interactions of the mass points randomize their positions and generate a perturbation to all larger scales; this should fix the self-similar distribution almost uniquely. The results of numerical experiments on 1000 bodies are presented; these appear to show new nonlinear effects: condensations can \"bootstrap\" their way up in size faster than the linear theory predicts. Our self-similar model predicts relations between the masses and radii of galaxies and clusters of galaxies, as well as their mass spectra. We compare the predictions with available data, and find some rather striking agreements. If the model is to explain galaxies, then isothermal \"seed\" masses of 3 x 1 0 M0 must have existed at recombination. To explain clusters of galaxies, the only necessary seeds are the galaxies themselves. The size of clusters determines, in principle, the deceleration parameter q0 presently available data give only very broad limits, unfortunately. Subject headings: cosmology - galaxies - galaxies, clusters of"}
{"bibcode": "1974Natur.248...30H", "title": "Black hole explosions?", "abstract": "QUANTUM gravitational effects are usually ignored in calculations of the formation and evolution of black holes. The justification for this is that the radius of curvature of space-time outside the event horizon is very large compared to the Planck length (Għ/c<SUP>3</SUP>)<SUP>1/2</SUP> ~ 10<SUP>-33</SUP> cm, the length scale on which quantum fluctuations of the metric are expected to be of order unity. This means that the energy density of particles created by the gravitational field is small compared to the space-time curvature. Even though quantum effects may be small locally, they may still, however, add up to produce a significant effect over the lifetime of the Universe ~ 10<SUP>17</SUP> s which is very long compared to the Planck time ~ 10<SUP>-43</SUP> s. The purpose of this letter is to show that this indeed may be the case: it seems that any black hole will create and emit particles such as neutrinos or photons at just the rate that one would expect if the black hole was a body with a temperature of (κ/2π) (ħ/2k) ~ 10<SUP>-6</SUP> (Msolar/M)K where κ is the surface gravity of the black hole<SUP>1</SUP>. As a black hole emits this thermal radiation one would expect it to lose mass. This in turn would increase the surface gravity and so increase the rate of emission. The black hole would therefore have a finite life of the order of 10<SUP>71</SUP> (Msolar/M)<SUP>-3</SUP> s. For a black hole of solar mass this is much longer than the age of the Universe. There might, however, be much smaller black holes which were formed by fluctuations in the early Universe<SUP>2</SUP>. Any such black hole of mass less than 10<SUP>15</SUP> g would have evaporated by now. Near the end of its life the rate of emission would be very high and about 10<SUP>30</SUP> erg would be released in the last 0.1 s. This is a fairly small explosion by astronomical standards but it is equivalent to about 1 million 1 Mton hydrogen bombs.", "database": ["astronomy", "physics", "general"], "keywords": [], "year": "1974", "doctype": "article", "citation_count": 4884, "domain_category": "astronomy", "abstract_clean": "QUANTUM gravitational effects are usually ignored in calculations of the formation and evolution of black holes. The justification for this is that the radius of curvature of space-time outside the event horizon is very large compared to the Planck length (Għ/c3)1/2 ~ 10-33 cm, the length scale on which quantum fluctuations of the metric are expected to be of order unity. This means that the energy density of particles created by the gravitational field is small compared to the space-time curvature. Even though quantum effects may be small locally, they may still, however, add up to produce a significant effect over the lifetime of the Universe ~ 1017 s which is very long compared to the Planck time ~ 10-43 s. The purpose of this letter is to show that this indeed may be the case: it seems that any black hole will create and emit particles such as neutrinos or photons at just the rate that one would expect if the black hole was a body with a temperature of (κ/2π) (ħ/2k) ~ 10-6 (Msolar/M)K where κ is the surface gravity of the black hole1. As a black hole emits this thermal radiation one would expect it to lose mass. This in turn would increase the surface gravity and so increase the rate of emission. The black hole would therefore have a finite life of the order of 1071 (Msolar/M)-3 s. For a black hole of solar mass this is much longer than the age of the Universe. There might, however, be much smaller black holes which were formed by fluctuations in the early Universe2. Any such black hole of mass less than 1015 g would have evaporated by now. Near the end of its life the rate of emission would be very high and about 1030 erg would be released in the last 0.1 s. This is a fairly small explosion by astronomical standards but it is equivalent to about 1 million 1 Mton hydrogen bombs."}
{"bibcode": "2003ApJS..148....1B", "title": "First-Year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Preliminary Maps and Basic Results", "abstract": "We present full-sky microwave maps in five frequency bands (23-94 GHz) from the Wilkinson Microwave Anisotropy Probe (WMAP) first-year sky survey. Calibration errors are less than 0.5%, and the low systematic error level is well specified. The cosmic microwave background (CMB) is separated from the foregrounds using multifrequency data. The sky maps are consistent with the 7° FWHM Cosmic Background Explorer (COBE) maps. We report more precise, but consistent, dipole and quadrupole values. The CMB anisotropy obeys Gaussian statistics with -58&lt;f<SUB>NL</SUB>&lt;134 (95% confidence level [CL]). The 2&lt;=l&lt;=900 anisotropy power spectrum is cosmic-variance-limited for l&lt;354, with a signal-to-noise ratio greater than 1 per mode to l=658. The temperature-polarization cross-power spectrum reveals both acoustic features and a large-angle correlation from reionization. The optical depth of reionization is τ=0.17+/-0.04, which implies a reionization epoch of t<SUB>r</SUB>=180<SUP>+220</SUP><SUB>-80</SUB> Myr (95% CL) after the big bang at a redshift of z<SUB>r</SUB>=20<SUP>+10</SUP><SUB>-9</SUB> (95% CL) for a range of ionization scenarios. This early reionization is incompatible with the presence of a significant warm dark matter density. <P />A best-fit cosmological model to the CMB and other measures of large-scale structure works remarkably well with only a few parameters. The age of the best-fit universe is t<SUB>0</SUB>=13.7+/-0.2 Gyr. Decoupling was t<SUB>dec</SUB>=379<SUP>+8</SUP><SUB>-7</SUB> kyr after the big bang at a redshift of z<SUB>dec</SUB>=1089+/-1. The thickness of the decoupling surface was Δz<SUB>dec</SUB>=195+/-2. The matter density of the universe is Ω<SUB>m</SUB>h<SUP>2</SUP>=0.135<SUP>+0.008</SUP><SUB>-0.009</SUB>, the baryon density is Ω<SUB>b</SUB>h<SUP>2</SUP>=0.0224+/-0.0009, and the total mass-energy of the universe is Ω<SUB>tot</SUB>=1.02+/-0.02. It appears that there may be progressively less fluctuation power on smaller scales, from WMAP to fine-scale CMB measurements to galaxies and finally to the Lyα forest. This may be accounted for with a running spectral index of scalar fluctuations, fitted as n<SUB>s</SUB>=0.93+/-0.03 at wavenumber k<SUB>0</SUB>=0.05 Mpc<SUP>-1</SUP> (l<SUB>eff</SUB>~700), with a slope of dn<SUB>s</SUB>/dlnk=-0.031<SUP>+0.016</SUP><SUB>-0.018</SUB> in the best-fit model. (For WMAP data alone, n<SUB>s</SUB>=0.99+/-0.04.) This flat universe model is composed of 4.4% baryons, 22% dark matter, and 73% dark energy. The dark energy equation of state is limited to w&lt;-0.78 (95% CL). Inflation theory is supported with n<SUB>s</SUB>~1, Ω<SUB>tot</SUB>~1, Gaussian random phases of the CMB anisotropy, and superhorizon fluctuations implied by the temperature-polarization anticorrelations at decoupling. An admixture of isocurvature modes does not improve the fit. The tensor-to-scalar ratio is r(k<SUB>0</SUB>=0.002Mpc<SUP>-1</SUP>)&lt;0.90 (95% CL). The lack of CMB fluctuation power on the largest angular scales reported by COBE and confirmed by WMAP is intriguing. WMAP continues to operate, so results will improve. <P />WMAP is the result of a partnership between Princeton University and the NASA Goddard Space Flight Center. Scientific guidance is provided by the WMAP Science Team.", "database": ["astronomy"], "keywords": ["Cosmology: Cosmic Microwave Background", "Cosmology: Observations", "Cosmology: Dark Matter", "Cosmology: Early Universe", "Instrumentation: Detectors", "Space Vehicles: Instruments", "Astrophysics"], "year": "2003", "doctype": "article", "citation_count": 4831, "domain_category": "astronomy", "abstract_clean": "We present full-sky microwave maps in five frequency bands (23-94 GHz) from the Wilkinson Microwave Anisotropy Probe (WMAP) first-year sky survey. Calibration errors are less than 0.5%, and the low systematic error level is well specified. The cosmic microwave background (CMB) is separated from the foregrounds using multifrequency data. The sky maps are consistent with the 7° FWHM Cosmic Background Explorer (COBE) maps. We report more precise, but consistent, dipole and quadrupole values. The CMB anisotropy obeys Gaussian statistics with -58<fNL<134 (95% confidence level [CL]). The 2<=l<=900 anisotropy power spectrum is cosmic-variance-limited for l<354, with a signal-to-noise ratio greater than 1 per mode to l=658. The temperature-polarization cross-power spectrum reveals both acoustic features and a large-angle correlation from reionization. The optical depth of reionization is τ=0.17+/-0.04, which implies a reionization epoch of tr=180+220-80 Myr (95% CL) after the big bang at a redshift of zr=20+10-9 (95% CL) for a range of ionization scenarios. This early reionization is incompatible with the presence of a significant warm dark matter density. A best-fit cosmological model to the CMB and other measures of large-scale structure works remarkably well with only a few parameters. The age of the best-fit universe is t0=13.7+/-0.2 Gyr. Decoupling was tdec=379+8-7 kyr after the big bang at a redshift of zdec=1089+/-1. The thickness of the decoupling surface was Δzdec=195+/-2. The matter density of the universe is Ωmh2=0.135+0.008-0.009, the baryon density is Ωbh2=0.0224+/-0.0009, and the total mass-energy of the universe is Ωtot=1.02+/-0.02. It appears that there may be progressively less fluctuation power on smaller scales, from WMAP to fine-scale CMB measurements to galaxies and finally to the Lyα forest. This may be accounted for with a running spectral index of scalar fluctuations, fitted as ns=0.93+/-0.03 at wavenumber k0=0.05 Mpc-1 (leff~700), with a slope of dns/dlnk=-0.031+0.016-0.018 in the best-fit model. (For WMAP data alone, ns=0.99+/-0.04.) This flat universe model is composed of 4.4% baryons, 22% dark matter, and 73% dark energy. The dark energy equation of state is limited to w<-0.78 (95% CL). Inflation theory is supported with ns~1, Ωtot~1, Gaussian random phases of the CMB anisotropy, and superhorizon fluctuations implied by the temperature-polarization anticorrelations at decoupling. An admixture of isocurvature modes does not improve the fit. The tensor-to-scalar ratio is r(k0=0.002Mpc-1)<0.90 (95% CL). The lack of CMB fluctuation power on the largest angular scales reported by COBE and confirmed by WMAP is intriguing. WMAP continues to operate, so results will improve. WMAP is the result of a partnership between Princeton University and the NASA Goddard Space Flight Center. Scientific guidance is provided by the WMAP Science Team."}
{"bibcode": "1998ApJ...498..541K", "title": "The Global Schmidt Law in Star-forming Galaxies", "abstract": "Measurements of Hα, H I, and CO distributions in 61 normal spiral galaxies are combined with published far-infrared and CO observations of 36 infrared-selected starburst galaxies, in order to study the form of the global star formation law over the full range of gas densities and star formation rates (SFRs) observed in galaxies. The disk-averaged SFRs and gas densities for the combined sample are well represented by a Schmidt law with index N = 1.4 +/- 0.15. The Schmidt law provides a surprisingly tight parametrization of the global star formation law, extending over several orders of magnitude in SFR and gas density. An alternative formulation of the star formation law, in which the SFR is presumed to scale with the ratio of the gas density to the average orbital timescale, also fits the data very well. Both descriptions provide potentially useful ``recipes'' for modeling the SFR in numerical simulations of galaxy formation and evolution.", "database": ["astronomy"], "keywords": ["GALAXIES: EVOLUTION", "GALAXIES: ISM", "GALAXIES: SPIRAL", "GALAXIES: STELLAR CONTENT", "GALAXIES: STARBURST", "STARS: FORMATION", "Galaxies: Evolution", "Galaxies: ISM", "Galaxies: Spiral", "Galaxies: Starburst", "Galaxies: Stellar Content", "Stars: Formation", "Astrophysics"], "year": "1998", "doctype": "article", "citation_count": 4815, "domain_category": "astronomy", "abstract_clean": "Measurements of Hα, H I, and CO distributions in 61 normal spiral galaxies are combined with published far-infrared and CO observations of 36 infrared-selected starburst galaxies, in order to study the form of the global star formation law over the full range of gas densities and star formation rates (SFRs) observed in galaxies. The disk-averaged SFRs and gas densities for the combined sample are well represented by a Schmidt law with index N = 1.4 +/- 0.15. The Schmidt law provides a surprisingly tight parametrization of the global star formation law, extending over several orders of magnitude in SFR and gas density. An alternative formulation of the star formation law, in which the SFR is presumed to scale with the ratio of the gas density to the average orbital timescale, also fits the data very well. Both descriptions provide potentially useful ``recipes'' for modeling the SFR in numerical simulations of galaxy formation and evolution."}
{"bibcode": "1995PASP..107..803U", "title": "Unified Schemes for Radio-Loud Active Galactic Nuclei", "abstract": "The appearance of active galactic nuclei (AGN) depends so strongly on orientation that our current classification schemes are dominated by random pointing directions instead of more interesting physical properties. Light from the centers of many AGN is obscrued by optically thick circumstellar matter, particularly at optical and ultraviolet wavelengths. In radio-loud AGN, bipolar jets emanating from the nucleus emit radio through gamma-ray light that is relativistically beamed along the jet axes. Understanding the origin and magnitude of radiation anistropies in AGN allows us to unify different classes of AGN; that is, to identify each single, underlying AGN type that gives rise to different classes through different orientations. This review describes the unification of radio-loud AGN, which includes radio galaxies, quasars, and blazars. We describe the classification and general properties of AGN. We summarize the evidence for anisotropic emission caused by circumstellar obscuration and relativistic beaming. We outline the two most plausible unified schemes for radio-loud AGN, one linking the high-luminosity sources (BL Lac objects and less luminous radio galaxies). Using the formalism appropriate to samples biased by relativistic beaming, we show the population statistics for two schemes are in accordance with available data. We analyze the possible connections between low- and high-luminosity radio-loud AGN and conclude they probably are powered by similar physical processes, at least within the relativistic jet. We review potential difficulties with unification and conclude that none currently constitutes a serious problem. We discuss likely complications to unified schemes that are suggested by realistic physical considerations; these will be important to consider when more comprehensive data for larger complete samples become available. We conclude with a list of the ten questions we believe are the most pressing in this field. (SECTION: Invited Review Paper)", "database": ["astronomy"], "keywords": ["GALAXIES: ACTIVE", "GALAXIES: FUNDAMENTAL PARAMETERS", "Astrophysics"], "year": "1995", "doctype": "article", "citation_count": 4811, "domain_category": "astronomy", "abstract_clean": "The appearance of active galactic nuclei (AGN) depends so strongly on orientation that our current classification schemes are dominated by random pointing directions instead of more interesting physical properties. Light from the centers of many AGN is obscrued by optically thick circumstellar matter, particularly at optical and ultraviolet wavelengths. In radio-loud AGN, bipolar jets emanating from the nucleus emit radio through gamma-ray light that is relativistically beamed along the jet axes. Understanding the origin and magnitude of radiation anistropies in AGN allows us to unify different classes of AGN; that is, to identify each single, underlying AGN type that gives rise to different classes through different orientations. This review describes the unification of radio-loud AGN, which includes radio galaxies, quasars, and blazars. We describe the classification and general properties of AGN. We summarize the evidence for anisotropic emission caused by circumstellar obscuration and relativistic beaming. We outline the two most plausible unified schemes for radio-loud AGN, one linking the high-luminosity sources (BL Lac objects and less luminous radio galaxies). Using the formalism appropriate to samples biased by relativistic beaming, we show the population statistics for two schemes are in accordance with available data. We analyze the possible connections between low- and high-luminosity radio-loud AGN and conclude they probably are powered by similar physical processes, at least within the relativistic jet. We review potential difficulties with unification and conclude that none currently constitutes a serious problem. We discuss likely complications to unified schemes that are suggested by realistic physical considerations; these will be important to consider when more comprehensive data for larger complete samples become available. We conclude with a list of the ten questions we believe are the most pressing in this field. (SECTION: Invited Review Paper)"}
{"bibcode": "2003RvMP...75..559P", "title": "The cosmological constant and dark energy", "abstract": "Physics welcomes the idea that space contains energy whose gravitational effect approximates that of Einstein’s cosmological constant, Λ; today the concept is termed dark energy or quintessence. Physics also suggests that dark energy could be dynamical, allowing for the arguably appealing picture of an evolving dark-energy density approaching its natural value, zero, and small now because the expanding universe is old. This would alleviate the classical problem of the curious energy scale of a millielectron volt associated with a constant Λ. Dark energy may have been detected by recent cosmological tests. These tests make a good scientific case for the context, in the relativistic Friedmann-Lemaître model, in which the gravitational inverse-square law is applied to the scales of cosmology. We have well-checked evidence that the mean mass density is not much more than one-quarter of the critical Einstein de Sitter value. The case for detection of dark energy is not yet as convincing but still serious; we await more data, which may be derived from work in progress. Planned observations may detect the evolution of the dark-energy density; a positive result would be a considerable stimulus for attempts at understanding the microphysics of dark energy. This review presents the basic physics and astronomy of the subject, reviews the history of ideas, assesses the state of the observational evidence, and comments on recent developments in the search for a fundamental theory.", "database": ["astronomy", "physics"], "keywords": ["95.35.+d", "98.80.Jk", "14.20.Cv", "01.30.Rr", "Dark matter", "Mathematical and relativistic aspects of cosmology", "Surveys and tutorial papers", "resource letters", "Astrophysics", "General Relativity and Quantum Cosmology", "High Energy Physics - Phenomenology", "High Energy Physics - Theory"], "year": "2003", "doctype": "article", "citation_count": 4763, "domain_category": "astronomy", "abstract_clean": "Physics welcomes the idea that space contains energy whose gravitational effect approximates that of Einstein’s cosmological constant, Λ; today the concept is termed dark energy or quintessence. Physics also suggests that dark energy could be dynamical, allowing for the arguably appealing picture of an evolving dark-energy density approaching its natural value, zero, and small now because the expanding universe is old. This would alleviate the classical problem of the curious energy scale of a millielectron volt associated with a constant Λ. Dark energy may have been detected by recent cosmological tests. These tests make a good scientific case for the context, in the relativistic Friedmann-Lemaître model, in which the gravitational inverse-square law is applied to the scales of cosmology. We have well-checked evidence that the mean mass density is not much more than one-quarter of the critical Einstein de Sitter value. The case for detection of dark energy is not yet as convincing but still serious; we await more data, which may be derived from work in progress. Planned observations may detect the evolution of the dark-energy density; a positive result would be a considerable stimulus for attempts at understanding the microphysics of dark energy. This review presents the basic physics and astronomy of the subject, reviews the history of ideas, assesses the state of the observational evidence, and comments on recent developments in the search for a fundamental theory."}
{"bibcode": "1977MNRAS.179..433B", "title": "Electromagnetic extraction of energy from Kerr black holes.", "abstract": "It is shown that if the magnetic field and angular momentum of a Kerr black hole are large enough, the vacuum surrounding the hole is unstable because any stray charged particles will be electrostatically accelerated and will radiate, with the radiation producing electron-positron pairs so freely that the electromagnetic field in the vicinity of the event horizon will become approximately force-free. Equations governing stationary force-free electromagnetic fields in Kerr spacetime are derived, and it is found that energy and angular momentum can be extracted from a rotating black hole by a purely electromagnetic mechanism. A perturbation technique is outlined for calculating approximate solutions under certain circumstances, and solutions are obtained for a split monopole magnetic field as well as for a paraboloidal magnetic field. The present concepts are applied to a model of an active galactic nucleus containing a massive black hole surrounded by an accretion disk.", "database": ["astronomy"], "keywords": ["Black Holes (Astronomy)", "Electromagnetic Fields", "Energy Sources", "Rotating Matter", "Active Galactic Nuclei", "Astrophysics", "Electron-Positron Pairs", "Pair Production", "Astrophysics"], "year": "1977", "doctype": "article", "citation_count": 4744, "domain_category": "astronomy", "abstract_clean": "It is shown that if the magnetic field and angular momentum of a Kerr black hole are large enough, the vacuum surrounding the hole is unstable because any stray charged particles will be electrostatically accelerated and will radiate, with the radiation producing electron-positron pairs so freely that the electromagnetic field in the vicinity of the event horizon will become approximately force-free. Equations governing stationary force-free electromagnetic fields in Kerr spacetime are derived, and it is found that energy and angular momentum can be extracted from a rotating black hole by a purely electromagnetic mechanism. A perturbation technique is outlined for calculating approximate solutions under certain circumstances, and solutions are obtained for a split monopole magnetic field as well as for a paraboloidal magnetic field. The present concepts are applied to a model of an active galactic nucleus containing a massive black hole surrounded by an accretion disk."}
{"bibcode": "2000ApJ...538..473L", "title": "Efficient Computation of Cosmic Microwave Background Anisotropies in Closed Friedmann-Robertson-Walker Models", "abstract": "We implement the efficient line-of-sight method to calculate the anisotropy and polarization of the cosmic microwave background for scalar and tensor modes in almost Friedmann-Robertson-Walker models with positive spatial curvature. We present new results for the polarization power spectra in such models.", "database": ["astronomy"], "keywords": ["Cosmology: Cosmic Microwave Background", "Cosmology: Theory", "Astrophysics"], "year": "2000", "doctype": "article", "citation_count": 4722, "domain_category": "astronomy", "abstract_clean": "We implement the efficient line-of-sight method to calculate the anisotropy and polarization of the cosmic microwave background for scalar and tensor modes in almost Friedmann-Robertson-Walker models with positive spatial curvature. We present new results for the polarization power spectra in such models."}
{"bibcode": "1982PhRvL..48.1220A", "title": "Cosmology for Grand Unified Theories with Radiatively Induced Symmetry Breaking", "abstract": "The treatment of first-order phase transitions for standard grand unified theories is shown to break down for models with radiatively induced spontaneous symmetry breaking. It is argued that proper analysis of these transitions which would take place in the early history of the universe can lead to an explanation of the cosmological homogeneity, flatness, and monopole puzzles.", "database": ["astronomy", "physics"], "keywords": ["Big Bang Cosmology", "Broken Symmetry", "Gravitation Theory", "Phase Transformations", "Magnetic Monopoles", "Phase Diagrams", "Radiative Transfer", "Astrophysics", "98.80.Bp", "11.15.Ex", "12.10.En", "Origin and formation of the Universe", "Spontaneous breaking of gauge symmetries"], "year": "1982", "doctype": "article", "citation_count": 4667, "domain_category": "astronomy", "abstract_clean": "The treatment of first-order phase transitions for standard grand unified theories is shown to break down for models with radiatively induced spontaneous symmetry breaking. It is argued that proper analysis of these transitions which would take place in the early history of the universe can lead to an explanation of the cosmological homogeneity, flatness, and monopole puzzles."}
{"bibcode": "2019ApJ...873..111I", "title": "LSST: From Science Drivers to Reference Design and Anticipated Data Products", "abstract": "We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg<SUP>2</SUP> field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320-1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg<SUP>2</SUP> region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.", "database": ["astronomy"], "keywords": ["astrometry", "cosmology: observations", "Galaxy: general", "methods: observational", "stars: general", "surveys", "Astrophysics"], "year": "2019", "doctype": "article", "citation_count": 4621, "domain_category": "astronomy", "abstract_clean": "We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320-1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics."}
{"bibcode": "1992AJ....104..340L", "title": "UBVRI Photometric Standard Stars in the Magnitude Range 11.5 &lt; V &lt; 16.0 Around the Celestial Equator", "abstract": "UBVRI photoelectric observations have been made on the Johnson-Kron-Cousins photometric system of 526 stars centered on the celestial equator. The program stars within a 298 number subset have sufficient measures so that they are capable of providing, for telescopes of intermediate and large size in both hemispheres, an internally consistent homogeneous broadband standard photometric system around the sky. The stars average 29 measures each on 19 nights. The majority of the stars in this paper fall in the magnitude range 11.5-16.0, and in the color range -0.3 to +2.3.", "database": ["astronomy"], "keywords": ["Equators", "Stellar Color", "Stellar Magnitude", "Stellar Spectrophotometry", "Ubv Spectra", "Astronomical Catalogs", "Color-Color Diagram", "Color-Magnitude Diagram", "Photographic Plates", "Astronomy", "STARS: POPULATION II", "GALAXY: STELLAR CONTENT"], "year": "1992", "doctype": "article", "citation_count": 4573, "domain_category": "astronomy", "abstract_clean": "UBVRI photoelectric observations have been made on the Johnson-Kron-Cousins photometric system of 526 stars centered on the celestial equator. The program stars within a 298 number subset have sufficient measures so that they are capable of providing, for telescopes of intermediate and large size in both hemispheres, an internally consistent homogeneous broadband standard photometric system around the sky. The stars average 29 measures each on 19 nights. The majority of the stars in this paper fall in the magnitude range 11.5-16.0, and in the color range -0.3 to +2.3."}
{"bibcode": "2005ApJ...633..560E", "title": "Detection of the Baryon Acoustic Peak in the Large-Scale Correlation Function of SDSS Luminous Red Galaxies", "abstract": "We present the large-scale correlation function measured from a spectroscopic sample of 46,748 luminous red galaxies from the Sloan Digital Sky Survey. The survey region covers 0.72 h<SUP>-3</SUP> Gpc<SUP>3</SUP> over 3816 deg<SUP>2</SUP> and 0.16&lt;z&lt;0.47, making it the best sample yet for the study of large-scale structure. We find a well-detected peak in the correlation function at 100 h<SUP>-1</SUP> Mpc separation that is an excellent match to the predicted shape and location of the imprint of the recombination-epoch acoustic oscillations on the low-redshift clustering of matter. This detection demonstrates the linear growth of structure by gravitational instability between z~1000 and the present and confirms a firm prediction of the standard cosmological theory. The acoustic peak provides a standard ruler by which we can measure the ratio of the distances to z=0.35 and z=1089 to 4% fractional accuracy and the absolute distance to z=0.35 to 5% accuracy. From the overall shape of the correlation function, we measure the matter density Ω<SUB>m</SUB>h<SUP>2</SUP> to 8% and find agreement with the value from cosmic microwave background (CMB) anisotropies. Independent of the constraints provided by the CMB acoustic scale, we find Ω<SUB>m</SUB>=0.273+/-0.025+0.123(1+w<SUB>0</SUB>)+0.137Ω<SUB>K</SUB>. Including the CMB acoustic scale, we find that the spatial curvature is Ω<SUB>K</SUB>=-0.010+/-0.009 if the dark energy is a cosmological constant. More generally, our results provide a measurement of cosmological distance, and hence an argument for dark energy, based on a geometric method with the same simple physics as the microwave background anisotropies. The standard cosmological model convincingly passes these new and robust tests of its fundamental properties.", "database": ["astronomy"], "keywords": ["Cosmology: Cosmic Microwave Background", "Cosmology: Cosmological Parameters", "Cosmology: Observations", "Cosmology: Distance Scale", "Galaxies: Elliptical and Lenticular", "cD", "Cosmology: Large-Scale Structure of Universe", "Astrophysics"], "year": "2005", "doctype": "article", "citation_count": 4571, "domain_category": "astronomy", "abstract_clean": "We present the large-scale correlation function measured from a spectroscopic sample of 46,748 luminous red galaxies from the Sloan Digital Sky Survey. The survey region covers 0.72 h-3 Gpc3 over 3816 deg2 and 0.16<z<0.47, making it the best sample yet for the study of large-scale structure. We find a well-detected peak in the correlation function at 100 h-1 Mpc separation that is an excellent match to the predicted shape and location of the imprint of the recombination-epoch acoustic oscillations on the low-redshift clustering of matter. This detection demonstrates the linear growth of structure by gravitational instability between z~1000 and the present and confirms a firm prediction of the standard cosmological theory. The acoustic peak provides a standard ruler by which we can measure the ratio of the distances to z=0.35 and z=1089 to 4% fractional accuracy and the absolute distance to z=0.35 to 5% accuracy. From the overall shape of the correlation function, we measure the matter density Ωmh2 to 8% and find agreement with the value from cosmic microwave background (CMB) anisotropies. Independent of the constraints provided by the CMB acoustic scale, we find Ωm=0.273+/-0.025+0.123(1+w0)+0.137ΩK. Including the CMB acoustic scale, we find that the spatial curvature is ΩK=-0.010+/-0.009 if the dark energy is a cosmological constant. More generally, our results provide a measurement of cosmological distance, and hence an argument for dark energy, based on a geometric method with the same simple physics as the microwave background anisotropies. The standard cosmological model convincingly passes these new and robust tests of its fundamental properties."}
{"bibcode": "2005PhR...405..279B", "title": "Particle dark matter: evidence, candidates and constraints", "abstract": "In this review article, we discuss the current status of particle dark matter, including experimental evidence and theoretical motivations. We discuss a wide array of candidates for particle dark matter, but focus on neutralinos in models of supersymmetry and Kaluza Klein dark matter in models of universal extra dimensions. We devote much of our attention to direct and indirect detection techniques, the constraints placed by these experiments and the reach of future experimental efforts.", "database": ["astronomy", "physics"], "keywords": ["High Energy Physics - Phenomenology", "Astrophysics"], "year": "2005", "doctype": "article", "citation_count": 4546, "domain_category": "astronomy", "abstract_clean": "In this review article, we discuss the current status of particle dark matter, including experimental evidence and theoretical motivations. We discuss a wide array of candidates for particle dark matter, but focus on neutralinos in models of supersymmetry and Kaluza Klein dark matter in models of universal extra dimensions. We devote much of our attention to direct and indirect detection techniques, the constraints placed by these experiments and the reach of future experimental efforts."}
{"bibcode": "1976Ap&SS..39..447L", "title": "Least-Squares Frequency Analysis of Unequally Spaced Data", "abstract": "The statistical properties of least-squares frequency analysis of unequally spaced data are examined. It is shown that, in the least-squares spectrum of gaussian noise, the reduction in the sum of squares at a particular frequency is aX<SUB arrange=\"stack\">2</SUB><SUP arrange=\"stack\">2</SUP> variable. The reductions at different frequencies are not independent, as there is a correlation between the height of the spectrum at any two frequencies,f<SUB>1</SUB> andf<SUB>2</SUB>, which is equal to the mean height of the spectrum due to a sinusoidal signal of frequencyf<SUB>1</SUB>, at the frequencyf<SUB>2</SUB>. These correlations reduce the distortion in the spectrum of a signal affected by noise. Some numerical illustrations of the properties of least-squares frequency spectra are also given.", "database": ["astronomy"], "keywords": ["Astronomy", "Data Reduction", "Least Squares Method", "Background Noise", "Power Spectra", "Sine Waves", "Spectrum Analysis", "Statistical Analysis", "Variable Stars", "Astronomy", "Gaussian Noise", "Frequency Spectrum", "Frequency Analysis", "Sinusoidal Signal", "Numerical Illustration"], "year": "1976", "doctype": "article", "citation_count": 4405, "domain_category": "astronomy", "abstract_clean": "The statistical properties of least-squares frequency analysis of unequally spaced data are examined. It is shown that, in the least-squares spectrum of gaussian noise, the reduction in the sum of squares at a particular frequency is aX22 variable. The reductions at different frequencies are not independent, as there is a correlation between the height of the spectrum at any two frequencies,f1 andf2, which is equal to the mean height of the spectrum due to a sinusoidal signal of frequencyf1, at the frequencyf2. These correlations reduce the distortion in the spectrum of a signal affected by noise. Some numerical illustrations of the properties of least-squares frequency spectra are also given."}
{"bibcode": "2006JPhG...33....1Y", "title": "Review of Particle Physics", "abstract": "This biennial Review summarizes much of particle physics. Using data from previous editions, plus 2633 new measurements from 689 papers, we list, evaluate, and average measured properties of gauge bosons, leptons, quarks, mesons, and baryons. We also summarize searches for hypothetical particles such as Higgs bosons, heavy neutrinos, and supersymmetric particles. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as the Standard Model, particle detectors, probability, and statistics. Among the 110 reviews are many that are new or heavily revised including those on CKM quark-mixing matrix, V<SUB>ud</SUB> &amp; V<SUB>us</SUB>, V<SUB>cb</SUB> &amp; V<SUB>ub</SUB>, top quark, muon anomalous magnetic moment, extra dimensions, particle detectors, cosmic background radiation, dark matter, cosmological parameters, and big bang cosmology. A booklet is available containing the Summary Tables and abbreviated versions of some of the other sections of this full Review. All tables, listings, and reviews (and errata) are also available on the Particle Data Group website: <A href=\"http://pdg.lbl.gov\">http://pdg.lbl.gov</A>.", "database": ["astronomy", "physics"], "keywords": [], "year": "2006", "doctype": "article", "citation_count": 4401, "domain_category": "astronomy", "abstract_clean": "This biennial Review summarizes much of particle physics. Using data from previous editions, plus 2633 new measurements from 689 papers, we list, evaluate, and average measured properties of gauge bosons, leptons, quarks, mesons, and baryons. We also summarize searches for hypothetical particles such as Higgs bosons, heavy neutrinos, and supersymmetric particles. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as the Standard Model, particle detectors, probability, and statistics. Among the 110 reviews are many that are new or heavily revised including those on CKM quark-mixing matrix, Vud & Vus, Vcb & Vub, top quark, muon anomalous magnetic moment, extra dimensions, particle detectors, cosmic background radiation, dark matter, cosmological parameters, and big bang cosmology. A booklet is available containing the Summary Tables and abbreviated versions of some of the other sections of this full Review. All tables, listings, and reviews (and errata) are also available on the Particle Data Group website: http://pdg.lbl.gov."}
{"bibcode": "1990ARA&A..28..215D", "title": "H I in the galaxy.", "abstract": "An observational review on H I in the Galaxy is presented. Attention is given to observations of interstellar atomic hydrogen from Lyman-alpha and the 21-cm line through indirect tracers such as dust and gamma rays; to what can be learned from such observations; to what the H I sky looks like; and to the state of knowledge in this area and the future prospects. Consideration is also given to results from recent H I investigations that bear on two important areas: the spatial organization of interstellar H I and its vertical distribution in the Galaxy.", "database": ["astronomy"], "keywords": ["H I Regions", "Milky Way Galaxy", "Absorption Spectra", "Lyman Alpha Radiation", "Molecular Clouds", "Radio Emission", "Ultraviolet Astronomy", "Very Long Base Interferometry", "Astrophysics"], "year": "1990", "doctype": "article", "citation_count": 4390, "domain_category": "astronomy", "abstract_clean": "An observational review on H I in the Galaxy is presented. Attention is given to observations of interstellar atomic hydrogen from Lyman-alpha and the 21-cm line through indirect tracers such as dust and gamma rays; to what can be learned from such observations; to what the H I sky looks like; and to the state of knowledge in this area and the future prospects. Consideration is also given to results from recent H I investigations that bear on two important areas: the spatial organization of interstellar H I and its vertical distribution in the Galaxy."}
{"bibcode": "2005Natur.435..629S", "title": "Simulations of the formation, evolution and clustering of galaxies and quasars", "abstract": "The cold dark matter model has become the leading theoretical picture for the formation of structure in the Universe. This model, together with the theory of cosmic inflation, makes a clear prediction for the initial conditions for structure formation and predicts that structures grow hierarchically through gravitational instability. Testing this model requires that the precise measurements delivered by galaxy surveys can be compared to robust and equally precise theoretical calculations. Here we present a simulation of the growth of dark matter structure using 2,160<SUP>3</SUP> particles, following them from redshift z = 127 to the present in a cube-shaped region 2.230 billion lightyears on a side. In postprocessing, we also follow the formation and evolution of the galaxies and quasars. We show that baryon-induced features in the initial conditions of the Universe are reflected in distorted form in the low-redshift galaxy distribution, an effect that can be used to constrain the nature of dark energy with future generations of observational surveys of galaxies.", "database": ["astronomy", "general"], "keywords": ["Astrophysics"], "year": "2005", "doctype": "article", "citation_count": 4362, "domain_category": "astronomy", "abstract_clean": "The cold dark matter model has become the leading theoretical picture for the formation of structure in the Universe. This model, together with the theory of cosmic inflation, makes a clear prediction for the initial conditions for structure formation and predicts that structures grow hierarchically through gravitational instability. Testing this model requires that the precise measurements delivered by galaxy surveys can be compared to robust and equally precise theoretical calculations. Here we present a simulation of the growth of dark matter structure using 2,1603 particles, following them from redshift z = 127 to the present in a cube-shaped region 2.230 billion lightyears on a side. In postprocessing, we also follow the formation and evolution of the galaxies and quasars. We show that baryon-induced features in the initial conditions of the Universe are reflected in distorted form in the low-redshift galaxy distribution, an effect that can be used to constrain the nature of dark energy with future generations of observational surveys of galaxies."}
{"bibcode": "2010RvMP...82..451S", "title": "f(R) theories of gravity", "abstract": "Modified gravity theories have received increased attention lately due to combined motivation coming from high-energy physics, cosmology, and astrophysics. Among numerous alternatives to Einstein’s theory of gravity, theories that include higher-order curvature invariants, and specifically the particular class of f(R) theories, have a long history. In the last five years there has been a new stimulus for their study, leading to a number of interesting results. Here f(R) theories of gravity are reviewed in an attempt to comprehensively present their most important aspects and cover the largest possible portion of the relevant literature. All known formalisms are presented—metric, Palatini, and metric affine—and the following topics are discussed: motivation; actions, field equations, and theoretical aspects; equivalence with other theories; cosmological aspects and constraints; viability criteria; and astrophysical applications.", "database": ["astronomy", "physics"], "keywords": ["04.50.Kd", "Modified theories of gravity", "General Relativity and Quantum Cosmology", "Astrophysics", "High Energy Physics - Theory"], "year": "2010", "doctype": "article", "citation_count": 4267, "domain_category": "astronomy", "abstract_clean": "Modified gravity theories have received increased attention lately due to combined motivation coming from high-energy physics, cosmology, and astrophysics. Among numerous alternatives to Einstein’s theory of gravity, theories that include higher-order curvature invariants, and specifically the particular class of f(R) theories, have a long history. In the last five years there has been a new stimulus for their study, leading to a number of interesting results. Here f(R) theories of gravity are reviewed in an attempt to comprehensively present their most important aspects and cover the largest possible portion of the relevant literature. All known formalisms are presented—metric, Palatini, and metric affine—and the following topics are discussed: motivation; actions, field equations, and theoretical aspects; equivalence with other theories; cosmological aspects and constraints; viability criteria; and astrophysical applications."}
{"bibcode": "1996AJ....112.1487H", "title": "A Catalog of Parameters for Globular Clusters in the Milky Way", "abstract": "A database of parameters for globular star clusters in the Milky Way is described which is available in electronic form through the WorldWideWeb. The information in the catalog includes up-to-date measurements for cluster distance, reddening, luminosity, colors and spectral types, velocity, structural and dynamical parameters, horizontal branch morphology, metallicity, and other quantities. This catalog will be updated regularly and maintained in electronic form for widest possible accessibility.", "database": ["astronomy"], "keywords": ["GLOBULAR CLUSTERS: GENERAL", "CATALOGS"], "year": "1996", "doctype": "article", "citation_count": 4245, "domain_category": "astronomy", "abstract_clean": "A database of parameters for globular star clusters in the Milky Way is described which is available in electronic form through the WorldWideWeb. The information in the catalog includes up-to-date measurements for cluster distance, reddening, luminosity, colors and spectral types, velocity, structural and dynamical parameters, horizontal branch morphology, metallicity, and other quantities. This catalog will be updated regularly and maintained in electronic form for widest possible accessibility."}
{"bibcode": "2000ApJ...539L...9F", "title": "A Fundamental Relation between Supermassive Black Holes and Their Host Galaxies", "abstract": "The masses of supermassive black holes correlate almost perfectly with the velocity dispersions of their host bulges, M<SUB>bh</SUB>~σ<SUP>α</SUP>, where α=4.8+/-0.5. The relation is much tighter than the relation between M<SUB>bh</SUB> and bulge luminosity, with a scatter no larger than expected on the basis of measurement error alone. Black hole masses recently estimated by Magorrian et al. lie systematically above the M<SUB>bh</SUB>-σ relation defined by more accurate mass estimates, some by as much as 2 orders of magnitude. The tightness of the M<SUB>bh</SUB>-σ relation implies a strong link between black hole formation and the properties of the stellar bulge.", "database": ["astronomy"], "keywords": ["Black Hole Physics", "Galaxies: Evolution", "Galaxies: Kinematics and Dynamics", "Astrophysics"], "year": "2000", "doctype": "article", "citation_count": 4236, "domain_category": "astronomy", "abstract_clean": "The masses of supermassive black holes correlate almost perfectly with the velocity dispersions of their host bulges, Mbh~σα, where α=4.8+/-0.5. The relation is much tighter than the relation between Mbh and bulge luminosity, with a scatter no larger than expected on the basis of measurement error alone. Black hole masses recently estimated by Magorrian et al. lie systematically above the Mbh-σ relation defined by more accurate mass estimates, some by as much as 2 orders of magnitude. The tightness of the Mbh-σ relation implies a strong link between black hole formation and the properties of the stellar bulge."}
{"bibcode": "1972ApJ...176....1G", "title": "On the Infall of Matter Into Clusters of Galaxies and Some Effects on Their Evolution", "abstract": "A theory of infall of material into clusters of galaxies is developed and applied to the Coma cluster. It is suggested that the infall phenomenon is responsible for the growth of cluster galaxies. The generation of a hot intracluster medium is discussed and its relation to the observed absence of normal spirals in rich clusters investigated. The inference made earlier by Gott and Gunn that the observed X-ray luminosity of Coma puts severe constraints on the deceleration parameter qo is further elucidated. We discuss the relation of these phenomena to the morphology of clusters, and find that some observed regularities in their observed properties can be explained.", "database": ["astronomy"], "keywords": [], "year": "1972", "doctype": "article", "citation_count": 4229, "domain_category": "astronomy", "abstract_clean": "A theory of infall of material into clusters of galaxies is developed and applied to the Coma cluster. It is suggested that the infall phenomenon is responsible for the growth of cluster galaxies. The generation of a hot intracluster medium is discussed and its relation to the observed absence of normal spirals in rich clusters investigated. The inference made earlier by Gott and Gunn that the observed X-ray luminosity of Coma puts severe constraints on the deceleration parameter qo is further elucidated. We discuss the relation of these phenomena to the morphology of clusters, and find that some observed regularities in their observed properties can be explained."}
{"bibcode": "1976PhRvD..14..870U", "title": "Notes on black-hole evaporation", "abstract": "This paper examines various aspects of black-hole evaporation. A two-dimensional model is investigated where it is shown that using fermion-boson cancellation on the stress-energy tensor reduces the energy outflow to zero, while other noncovariant techniques give the Hawking result. A technique for replacing the collapse by boundary conditions on the past horizon is developed which retains the essential features of the collapse while eliminating some of the difficulties. This set of boundary conditions is also suggested as the most natural set for a preexistent black hole. The behavior of particle detectors under acceleration is investigated where it is shown that an accelerated detector even in flat spacetime will detect particles in the vacuum. The similarity of this case with the behavior of a detector near the black hole is brought out, and it is shown that a geodesic detector near the horizon will not see the Hawking flux of particles. Finally, the work of Berger, Chitre, Nutku, and Moncrief on scalar geons is corrected, and the spherically symmetric coupled scalar-gravitation Hamiltonian is presented in the hope that someone can apply it to the problem of black-hole evaporation.", "database": ["astronomy", "physics"], "keywords": ["Astronomical Models", "Black Holes (Astronomy)", "Evaporation", "Gravitational Collapse", "Particle Production", "Bosons", "Boundary Conditions", "Boundary Value Problems", "Cosmology", "Fermions", "Neutrinos", "Stress Tensors", "Tensor Analysis", "Two Dimensional Models", "Astrophysics"], "year": "1976", "doctype": "article", "citation_count": 4208, "domain_category": "astronomy", "abstract_clean": "This paper examines various aspects of black-hole evaporation. A two-dimensional model is investigated where it is shown that using fermion-boson cancellation on the stress-energy tensor reduces the energy outflow to zero, while other noncovariant techniques give the Hawking result. A technique for replacing the collapse by boundary conditions on the past horizon is developed which retains the essential features of the collapse while eliminating some of the difficulties. This set of boundary conditions is also suggested as the most natural set for a preexistent black hole. The behavior of particle detectors under acceleration is investigated where it is shown that an accelerated detector even in flat spacetime will detect particles in the vacuum. The similarity of this case with the behavior of a detector near the black hole is brought out, and it is shown that a geodesic detector near the horizon will not see the Hawking flux of particles. Finally, the work of Berger, Chitre, Nutku, and Moncrief on scalar geons is corrected, and the spherically symmetric coupled scalar-gravitation Hamiltonian is presented in the hope that someone can apply it to the problem of black-hole evaporation."}
{"bibcode": "2019ApJ...875L...1E", "title": "First M87 Event Horizon Telescope Results. I. The Shadow of the Supermassive Black Hole", "abstract": "When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio ≳10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 10<SUP>9</SUP> M <SUB>⊙</SUB>. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible.", "database": ["astronomy"], "keywords": ["accretion", "accretion disks", "black hole physics", "galaxies: active", "galaxies: individual: M87", "galaxies: jets", "gravitation", "Astrophysics - Astrophysics of Galaxies", "Astrophysics - High Energy Astrophysical Phenomena", "General Relativity and Quantum Cosmology"], "year": "2019", "doctype": "article", "citation_count": 4203, "domain_category": "astronomy", "abstract_clean": "When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42 ± 3 μas, which is circular and encompasses a central depression in brightness with a flux ratio ≳10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5 ± 0.7) × 109 M ⊙. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible."}
{"bibcode": "1991ApJ...376..214B", "title": "A Powerful Local Shear Instability in Weakly Magnetized Disks. I. Linear Analysis", "abstract": "A broad class of astronomical accretion disks is presently shown to be dynamically unstable to axisymmetric disturbances in the presence of a weak magnetic field, an insight with consequently broad applicability to gaseous, differentially-rotating systems. In the first part of this work, a linear analysis is presented of the instability, which is local and extremely powerful; the maximum growth rate, which is of the order of the angular rotation velocity, is independent of the strength of the magnetic field. Fluid motions associated with the instability directly generate both poloidal and toroidal field components. In the second part of this investigation, the scaling relation between the instability's wavenumber and the Alfven velocity is demonstrated, and the independence of the maximum growth rate from magnetic field strength is confirmed.", "database": ["astronomy"], "keywords": ["Accretion Disks", "Magnetohydrodynamic Stability", "Stellar Magnetic Fields", "Stellar Mass Accretion", "Boussinesq Approximation", "Computational Astrophysics", "Linear Systems", "Astrophysics", "ACCRETION", "HYDRODYNAMICS", "HYDROMAGNETICS", "INSTABILITIES"], "year": "1991", "doctype": "article", "citation_count": 4197, "domain_category": "astronomy", "abstract_clean": "A broad class of astronomical accretion disks is presently shown to be dynamically unstable to axisymmetric disturbances in the presence of a weak magnetic field, an insight with consequently broad applicability to gaseous, differentially-rotating systems. In the first part of this work, a linear analysis is presented of the instability, which is local and extremely powerful; the maximum growth rate, which is of the order of the angular rotation velocity, is independent of the strength of the magnetic field. Fluid motions associated with the instability directly generate both poloidal and toroidal field components. In the second part of this investigation, the scaling relation between the instability's wavenumber and the Alfven velocity is demonstrated, and the independence of the maximum growth rate from magnetic field strength is confirmed."}
{"bibcode": "2006PhRvL..96r1602R", "title": "Holographic Derivation of Entanglement Entropy from the anti de Sitter Space/Conformal Field Theory Correspondence", "abstract": "A holographic derivation of the entanglement entropy in quantum (conformal) field theories is proposed from anti de Sitter/conformal field theory (AdS/CFT) correspondence. We argue that the entanglement entropy in d+1 dimensional conformal field theories can be obtained from the area of d dimensional minimal surfaces in AdS<SUB>d+2</SUB>, analogous to the Bekenstein-Hawking formula for black hole entropy. We show that our proposal agrees perfectly with the entanglement entropy in 2D CFT when applied to AdS<SUB>3</SUB>. We also compare the entropy computed in AdS<SUB>5</SUB>×S<SUP>5</SUP> with that of the free N=4 super Yang-Mills theory.", "database": ["astronomy", "physics"], "keywords": ["11.25.Tq", "03.65.Ud", "04.70.Dy", "11.25.Hf", "Gauge/string duality", "Entanglement and quantum nonlocality", "Quantum aspects of black holes evaporation thermodynamics", "Conformal field theory algebraic structures", "High Energy Physics - Theory", "Condensed Matter - Strongly Correlated Electrons", "General Relativity and Quantum Cosmology", "Quantum Physics"], "year": "2006", "doctype": "article", "citation_count": 4129, "domain_category": "astronomy", "abstract_clean": "A holographic derivation of the entanglement entropy in quantum (conformal) field theories is proposed from anti de Sitter/conformal field theory (AdS/CFT) correspondence. We argue that the entanglement entropy in d+1 dimensional conformal field theories can be obtained from the area of d dimensional minimal surfaces in AdSd+2, analogous to the Bekenstein-Hawking formula for black hole entropy. We show that our proposal agrees perfectly with the entanglement entropy in 2D CFT when applied to AdS3. We also compare the entropy computed in AdS5×S5 with that of the free N=4 super Yang-Mills theory."}
{"bibcode": "1996PhR...267..195J", "title": "Supersymmetric dark matter", "abstract": "There is almost universal agreement among astronomers that most of the mass in the Universe and most of the mass in the Galactic halo is dark. Many lines of reasoning suggest that the dark matter consists of some new, as yet undiscovered, weakly interacting massive particle (WIMP). There is now a vast experimental effort being surmounted to detect WIMPs in the halo. The most promising techniques involve direct detection in low-background laboratory detectors and indirect detection through observation of energetic neutrinos from annihilation of WIMPs that have accumulated in the Sun and/or the Earth. Of the many WIMP candidates, perhaps the best motivated and certainly the most theoretically developed is the neutralino, the lightest superpartner in many supersymmetric theories. We review the minimal supersymmetric extension of the standard model and discuss prospects for detection of neutralino dark matter. We review in detail how to calculate the cosmological abundance of the neutralino and the event rates for both direct- and indirect-detection schemes, and we discuss astrophysical and laboratory constraints on supersymmetric models. We isolate and clarify the uncertainties from particle physics, nuclear physics, and astrophysics that enter at each step in the calculations. We briefly review other related dark-matter candidates and detection techniques.", "database": ["astronomy", "physics"], "keywords": ["High Energy Physics - Phenomenology", "Astrophysics", "Nuclear Theory"], "year": "1996", "doctype": "article", "citation_count": 4129, "domain_category": "astronomy", "abstract_clean": "There is almost universal agreement among astronomers that most of the mass in the Universe and most of the mass in the Galactic halo is dark. Many lines of reasoning suggest that the dark matter consists of some new, as yet undiscovered, weakly interacting massive particle (WIMP). There is now a vast experimental effort being surmounted to detect WIMPs in the halo. The most promising techniques involve direct detection in low-background laboratory detectors and indirect detection through observation of energetic neutrinos from annihilation of WIMPs that have accumulated in the Sun and/or the Earth. Of the many WIMP candidates, perhaps the best motivated and certainly the most theoretically developed is the neutralino, the lightest superpartner in many supersymmetric theories. We review the minimal supersymmetric extension of the standard model and discuss prospects for detection of neutralino dark matter. We review in detail how to calculate the cosmological abundance of the neutralino and the event rates for both direct- and indirect-detection schemes, and we discuss astrophysical and laboratory constraints on supersymmetric models. We isolate and clarify the uncertainties from particle physics, nuclear physics, and astrophysics that enter at each step in the calculations. We briefly review other related dark-matter candidates and detection techniques."}
{"bibcode": "2011PhR...505...59N", "title": "Unified cosmic history in modified gravity: From F(R) theory to Lorentz non-invariant models", "abstract": "The classical generalization of general relativity is considered as the gravitational alternative for a unified description of the early-time inflation with late-time cosmic acceleration. The structure and cosmological properties of a number of modified theories, including traditional F(R) and Hořava-Lifshitz F(R) gravity, scalar-tensor theory, string-inspired and Gauss-Bonnet theory, non-local gravity, non-minimally coupled models, and power-counting renormalizable covariant gravity are discussed. Different representations of and relations between such theories are investigated. It is shown that some versions of the above theories may be consistent with local tests and may provide a qualitatively reasonable unified description of inflation with the dark energy epoch. The cosmological reconstruction of different modified gravities is provided in great detail. It is demonstrated that eventually any given universe evolution may be reconstructed for the theories under consideration, and the explicit reconstruction is applied to an accelerating spatially flat Friedmann-Robertson-Walker (FRW) universe. Special attention is paid to Lagrange multiplier constrained and conventional F(R) gravities, for latter F(R) theory, the effective ΛCDM era and phantom divide crossing acceleration are obtained. The occurrences of the Big Rip and other finite-time future singularities in modified gravity are reviewed along with their solutions via the addition of higher-derivative gravitational invariants.", "database": ["astronomy", "physics"], "keywords": ["General Relativity and Quantum Cosmology", "Astrophysics - Cosmology and Extragalactic Astrophysics", "High Energy Physics - Phenomenology", "High Energy Physics - Theory"], "year": "2011", "doctype": "article", "citation_count": 4102, "domain_category": "astronomy", "abstract_clean": "The classical generalization of general relativity is considered as the gravitational alternative for a unified description of the early-time inflation with late-time cosmic acceleration. The structure and cosmological properties of a number of modified theories, including traditional F(R) and Hořava-Lifshitz F(R) gravity, scalar-tensor theory, string-inspired and Gauss-Bonnet theory, non-local gravity, non-minimally coupled models, and power-counting renormalizable covariant gravity are discussed. Different representations of and relations between such theories are investigated. It is shown that some versions of the above theories may be consistent with local tests and may provide a qualitatively reasonable unified description of inflation with the dark energy epoch. The cosmological reconstruction of different modified gravities is provided in great detail. It is demonstrated that eventually any given universe evolution may be reconstructed for the theories under consideration, and the explicit reconstruction is applied to an accelerating spatially flat Friedmann-Robertson-Walker (FRW) universe. Special attention is paid to Lagrange multiplier constrained and conventional F(R) gravities, for latter F(R) theory, the effective ΛCDM era and phantom divide crossing acceleration are obtained. The occurrences of the Big Rip and other finite-time future singularities in modified gravity are reviewed along with their solutions via the addition of higher-derivative gravitational invariants."}
{"bibcode": "1988PhRvD..37.3406R", "title": "Cosmological consequences of a rolling homogeneous scalar field", "abstract": "The cosmological consequences of a pervasive, rolling, self-interacting, homogeneous scalar field are investigated. A number of models in which the energy density of the scalar field red-shifts in a specific manner are studied. In these models the current epoch is chosen to be scalar-field dominated to agree with dynamical estimates of the density parameter, Ω<SUB>dyn~0.2</SUB>, and zero spatial curvature. The required scalar-field potential is ``nonlinear'' and decreases in magnitude as the value of the scalar field increases. A special solution of the field equations which is an attractive, time-dependent, fixed point is presented. These models are consistent with the classical tests of gravitation theory. The Eötvös-Dicke measurements strongly constrain the coupling of the scalar field to light (nongravitational) fields. Nucleosynthesis proceeds as in the standard hot big-bang model. In linear perturbation theory the behavior of baryonic perturbations, in the baryon-dominated epoch, do not differ significantly from the canonical scenario, while the presence of a substantial amount of homogeneous scalar-field energy density at low red-shifts inhibits the growth of perturbations in the baryonic fluid. The energy density in the scalar field is not appreciably perturbed by nonrelativistic gravitational fields, either in the radiation-dominated, matter-dominated, or scalar-field-dominated epochs. On the basis of this effect, we argue that these models could reconcile the low dynamical estimates of the mean mass density with the negligibly small spatial curvature preferred by inflation.", "database": ["astronomy", "physics"], "keywords": ["98.80.Bp", "11.10.Ef", "12.25.+e", "98.80.Dr", "Origin and formation of the Universe", "Lagrangian and Hamiltonian approach"], "year": "1988", "doctype": "article", "citation_count": 4065, "domain_category": "astronomy", "abstract_clean": "The cosmological consequences of a pervasive, rolling, self-interacting, homogeneous scalar field are investigated. A number of models in which the energy density of the scalar field red-shifts in a specific manner are studied. In these models the current epoch is chosen to be scalar-field dominated to agree with dynamical estimates of the density parameter, Ωdyn~0.2, and zero spatial curvature. The required scalar-field potential is ``nonlinear'' and decreases in magnitude as the value of the scalar field increases. A special solution of the field equations which is an attractive, time-dependent, fixed point is presented. These models are consistent with the classical tests of gravitation theory. The Eötvös-Dicke measurements strongly constrain the coupling of the scalar field to light (nongravitational) fields. Nucleosynthesis proceeds as in the standard hot big-bang model. In linear perturbation theory the behavior of baryonic perturbations, in the baryon-dominated epoch, do not differ significantly from the canonical scenario, while the presence of a substantial amount of homogeneous scalar-field energy density at low red-shifts inhibits the growth of perturbations in the baryonic fluid. The energy density in the scalar field is not appreciably perturbed by nonrelativistic gravitational fields, either in the radiation-dominated, matter-dominated, or scalar-field-dominated epochs. On the basis of this effect, we argue that these models could reconcile the low dynamical estimates of the mean mass density with the negligibly small spatial curvature preferred by inflation."}
{"bibcode": "2021A&A...649A...1G", "title": "Gaia Early Data Release 3. Summary of the contents and survey properties", "abstract": "Context. We present the early installment of the third Gaia data release, Gaia EDR3, consisting of astrometry and photometry for 1.8 billion sources brighter than magnitude 21, complemented with the list of radial velocities from Gaia DR2. <BR /> Aims: A summary of the contents of Gaia EDR3 is presented, accompanied by a discussion on the differences with respect to Gaia DR2 and an overview of the main limitations which are present in the survey. Recommendations are made on the responsible use of Gaia EDR3 results. <BR /> Methods: The raw data collected with the Gaia instruments during the first 34 months of the mission have been processed by the Gaia Data Processing and Analysis Consortium and turned into this early third data release, which represents a major advance with respect to Gaia DR2 in terms of astrometric and photometric precision, accuracy, and homogeneity. <BR /> Results: Gaia EDR3 contains celestial positions and the apparent brightness in G for approximately 1.8 billion sources. For 1.5 billion of those sources, parallaxes, proper motions, and the (G<SUB>BP</SUB> − G<SUB>RP</SUB>) colour are also available. The passbands for G, G<SUB>BP</SUB>, and G<SUB>RP</SUB> are provided as part of the release. For ease of use, the 7 million radial velocities from Gaia DR2 are included in this release, after the removal of a small number of spurious values. New radial velocities will appear as part of Gaia DR3. Finally, Gaia EDR3 represents an updated materialisation of the celestial reference frame (CRF) in the optical, the Gaia-CRF3, which is based solely on extragalactic sources. The creation of the source list for Gaia EDR3 includes enhancements that make it more robust with respect to high proper motion stars, and the disturbing effects of spurious and partially resolved sources. The source list is largely the same as that for Gaia DR2, but it does feature new sources and there are some notable changes. The source list will not change for Gaia DR3. <BR /> Conclusions: Gaia EDR3 represents a significant advance over Gaia DR2, with parallax precisions increased by 30 per cent, proper motion precisions increased by a factor of 2, and the systematic errors in the astrometry suppressed by 30-40% for the parallaxes and by a factor ~2.5 for the proper motions. The photometry also features increased precision, but above all much better homogeneity across colour, magnitude, and celestial position. A single passband for G, G<SUB>BP</SUB>, and G<SUB>RP</SUB> is valid over the entire magnitude and colour range, with no systematics above the 1% level", "database": ["astronomy"], "keywords": ["catalogs", "astrometry", "parallaxes", "proper motions", "techniques: photometric", "techniques: radial velocities", "Astrophysics - Astrophysics of Galaxies"], "year": "2021", "doctype": "article", "citation_count": 4026, "domain_category": "astronomy", "abstract_clean": "Context. We present the early installment of the third Gaia data release, Gaia EDR3, consisting of astrometry and photometry for 1.8 billion sources brighter than magnitude 21, complemented with the list of radial velocities from Gaia DR2. Aims: A summary of the contents of Gaia EDR3 is presented, accompanied by a discussion on the differences with respect to Gaia DR2 and an overview of the main limitations which are present in the survey. Recommendations are made on the responsible use of Gaia EDR3 results. Methods: The raw data collected with the Gaia instruments during the first 34 months of the mission have been processed by the Gaia Data Processing and Analysis Consortium and turned into this early third data release, which represents a major advance with respect to Gaia DR2 in terms of astrometric and photometric precision, accuracy, and homogeneity. Results: Gaia EDR3 contains celestial positions and the apparent brightness in G for approximately 1.8 billion sources. For 1.5 billion of those sources, parallaxes, proper motions, and the (GBP − GRP) colour are also available. The passbands for G, GBP, and GRP are provided as part of the release. For ease of use, the 7 million radial velocities from Gaia DR2 are included in this release, after the removal of a small number of spurious values. New radial velocities will appear as part of Gaia DR3. Finally, Gaia EDR3 represents an updated materialisation of the celestial reference frame (CRF) in the optical, the Gaia-CRF3, which is based solely on extragalactic sources. The creation of the source list for Gaia EDR3 includes enhancements that make it more robust with respect to high proper motion stars, and the disturbing effects of spurious and partially resolved sources. The source list is largely the same as that for Gaia DR2, but it does feature new sources and there are some notable changes. The source list will not change for Gaia DR3. Conclusions: Gaia EDR3 represents a significant advance over Gaia DR2, with parallax precisions increased by 30 per cent, proper motion precisions increased by a factor of 2, and the systematic errors in the astrometry suppressed by 30-40% for the parallaxes and by a factor ~2.5 for the proper motions. The photometry also features increased precision, but above all much better homogeneity across colour, magnitude, and celestial position. A single passband for G, GBP, and GRP is valid over the entire magnitude and colour range, with no systematics above the 1% level"}
{"bibcode": "2014ARA&A..52..415M", "title": "Cosmic Star-Formation History", "abstract": "Over the past two decades, an avalanche of new data from multiwavelength imaging and spectroscopic surveys has revolutionized our view of galaxy formation and evolution. Here we review the range of complementary techniques and theoretical tools that allow astronomers to map the cosmic history of star formation, heavy element production, and reionization of the Universe from the cosmic “dark ages” to the present epoch. A consistent picture is emerging, whereby the star-formation rate density peaked approximately 3.5 Gyr after the Big Bang, at z≈1.9, and declined exponentially at later times, with an e-folding timescale of 3.9 Gyr. Half of the stellar mass observed today was formed before a redshift z = 1.3. About 25% formed before the peak of the cosmic star-formation rate density, and another 25% formed after z = 0.7. Less than ∼1% of today's stars formed during the epoch of reionization. Under the assumption of a universal initial mass function, the global stellar mass density inferred at any epoch matches reasonably well the time integral of all the preceding star-formation activity. The comoving rates of star formation and central black hole accretion follow a similar rise and fall, offering evidence for coevolution of black holes and their host galaxies. The rise of the mean metallicity of the Universe to about 0.001 solar by z = 6, one Gyr after the Big Bang, appears to have been accompanied by the production of fewer than ten hydrogen Lyman-continuum photons per baryon, a rather tight budget for cosmological reionization.", "database": ["astronomy"], "keywords": ["Astrophysics - Cosmology and Nongalactic Astrophysics"], "year": "2014", "doctype": "article", "citation_count": 4022, "domain_category": "astronomy", "abstract_clean": "Over the past two decades, an avalanche of new data from multiwavelength imaging and spectroscopic surveys has revolutionized our view of galaxy formation and evolution. Here we review the range of complementary techniques and theoretical tools that allow astronomers to map the cosmic history of star formation, heavy element production, and reionization of the Universe from the cosmic “dark ages” to the present epoch. A consistent picture is emerging, whereby the star-formation rate density peaked approximately 3.5 Gyr after the Big Bang, at z≈1.9, and declined exponentially at later times, with an e-folding timescale of 3.9 Gyr. Half of the stellar mass observed today was formed before a redshift z = 1.3. About 25% formed before the peak of the cosmic star-formation rate density, and another 25% formed after z = 0.7. Less than ∼1% of today's stars formed during the epoch of reionization. Under the assumption of a universal initial mass function, the global stellar mass density inferred at any epoch matches reasonably well the time integral of all the preceding star-formation activity. The comoving rates of star formation and central black hole accretion follow a similar rise and fall, offering evidence for coevolution of black holes and their host galaxies. The rise of the mean metallicity of the Universe to about 0.001 solar by z = 6, one Gyr after the Big Bang, appears to have been accompanied by the production of fewer than ten hydrogen Lyman-continuum photons per baryon, a rather tight budget for cosmological reionization."}
{"bibcode": "1999ApJS..123....3L", "title": "Starburst99: Synthesis Models for Galaxies with Active Star Formation", "abstract": "Starburst99 is a comprehensive set of model predictions for spectrophotometric and related properties of galaxies with active star formation. The models are an improved and extended version of the data set previously published by Leitherer &amp; Heckman. We have upgraded our code by implementing the latest set of stellar evolution models of the Geneva group and the model atmosphere grid compiled by Lejeune et al. Several predictions which were not included in the previous publication are shown here for the first time. The models are presented in a homogeneous way for five metallicities between Z=0.040 and 0.001 and three choices of the initial mass function. The age coverage is 10<SUP>6</SUP>-10<SUP>9</SUP> yr. We also show the spectral energy distributions which are used to compute colors and other quantities. The full data set is available for retrieval at a Web site, which allows users to run specific models with nonstandard parameters as well. We also make the source code available to the community.", "database": ["astronomy"], "keywords": ["GALAXIES: EVOLUTION", "GALAXIES: FUNDAMENTAL PARAMETERS", "GALAXIES: STARBURST", "GALAXIES: STELLAR CONTENT", "METHODS: NUMERICAL", "Galaxies: Evolution", "Galaxies: Fundamental Parameters", "Galaxies: Starburst", "Galaxies: Stellar Content", "Methods: Numerical", "Astrophysics"], "year": "1999", "doctype": "article", "citation_count": 4006, "domain_category": "astronomy", "abstract_clean": "Starburst99 is a comprehensive set of model predictions for spectrophotometric and related properties of galaxies with active star formation. The models are an improved and extended version of the data set previously published by Leitherer & Heckman. We have upgraded our code by implementing the latest set of stellar evolution models of the Geneva group and the model atmosphere grid compiled by Lejeune et al. Several predictions which were not included in the previous publication are shown here for the first time. The models are presented in a homogeneous way for five metallicities between Z=0.040 and 0.001 and three choices of the initial mass function. The age coverage is 106-109 yr. We also show the spectral energy distributions which are used to compute colors and other quantities. The full data set is available for retrieval at a Web site, which allows users to run specific models with nonstandard parameters as well. We also make the source code available to the community."}
{"bibcode": "2004ApJ...607..665R", "title": "Type Ia Supernova Discoveries at z &gt; 1 from the Hubble Space Telescope: Evidence for Past Deceleration and Constraints on Dark Energy Evolution", "abstract": "We have discovered 16 Type Ia supernovae (SNe Ia) with the Hubble Space Telescope (HST) and have used them to provide the first conclusive evidence for cosmic deceleration that preceded the current epoch of cosmic acceleration. These objects, discovered during the course of the GOODS ACS Treasury program, include 6 of the 7 highest redshift SNe Ia known, all at z&gt;1.25, and populate the Hubble diagram in unexplored territory. The luminosity distances to these objects and to 170 previously reported SNe Ia have been determined using empirical relations between light-curve shape and luminosity. A purely kinematic interpretation of the SN Ia sample provides evidence at the greater than 99% confidence level for a transition from deceleration to acceleration or, similarly, strong evidence for a cosmic jerk. Using a simple model of the expansion history, the transition between the two epochs is constrained to be at z=0.46+/-0.13. The data are consistent with the cosmic concordance model of Ω<SUB>M</SUB>~0.3,Ω<SUB>Λ</SUB>~0.7 (χ<SUP>2</SUP><SUB>dof</SUB>=1.06) and are inconsistent with a simple model of evolution or dust as an alternative to dark energy. For a flat universe with a cosmological constant, we measure Ω<SUB>M</SUB>=0.29+/-<SUP>0.05</SUP><SUB>0.03</SUB> (equivalently, Ω<SUB>Λ</SUB>=0.71). When combined with external flat-universe constraints, including the cosmic microwave background and large-scale structure, we find w=-1.02+/-<SUP>0.13</SUP><SUB>0.19</SUB> (and w&lt;-0.76 at the 95% confidence level) for an assumed static equation of state of dark energy, P=wρc<SUP>2</SUP>. Joint constraints on both the recent equation of state of dark energy, w<SUB>0</SUB>, and its time evolution, dw/dz, are a factor of ~8 more precise than the first estimates and twice as precise as those without the SNe Ia discovered with HST. Our constraints are consistent with the static nature of and value of w expected for a cosmological constant (i.e., w<SUB>0</SUB>=-1.0, dw/dz=0) and are inconsistent with very rapid evolution of dark energy. We address consequences of evolving dark energy for the fate of the universe. <P />Based on observations with the NASA/ESA Hubble Space Telescope, obtained at the Space Telescope Science Institute, which is operated by AURA, Inc., under NASA contract NAS5-26555.", "database": ["astronomy"], "keywords": ["Cosmology: Observations", "Cosmology: Distance Scale", "Galaxies: Distances and Redshifts", "Stars: Supernovae: General", "Astrophysics"], "year": "2004", "doctype": "article", "citation_count": 4005, "domain_category": "astronomy", "abstract_clean": "We have discovered 16 Type Ia supernovae (SNe Ia) with the Hubble Space Telescope (HST) and have used them to provide the first conclusive evidence for cosmic deceleration that preceded the current epoch of cosmic acceleration. These objects, discovered during the course of the GOODS ACS Treasury program, include 6 of the 7 highest redshift SNe Ia known, all at z>1.25, and populate the Hubble diagram in unexplored territory. The luminosity distances to these objects and to 170 previously reported SNe Ia have been determined using empirical relations between light-curve shape and luminosity. A purely kinematic interpretation of the SN Ia sample provides evidence at the greater than 99% confidence level for a transition from deceleration to acceleration or, similarly, strong evidence for a cosmic jerk. Using a simple model of the expansion history, the transition between the two epochs is constrained to be at z=0.46+/-0.13. The data are consistent with the cosmic concordance model of ΩM~0.3,ΩΛ~0.7 (χ2dof=1.06) and are inconsistent with a simple model of evolution or dust as an alternative to dark energy. For a flat universe with a cosmological constant, we measure ΩM=0.29+/-0.050.03 (equivalently, ΩΛ=0.71). When combined with external flat-universe constraints, including the cosmic microwave background and large-scale structure, we find w=-1.02+/-0.130.19 (and w<-0.76 at the 95% confidence level) for an assumed static equation of state of dark energy, P=wρc2. Joint constraints on both the recent equation of state of dark energy, w0, and its time evolution, dw/dz, are a factor of ~8 more precise than the first estimates and twice as precise as those without the SNe Ia discovered with HST. Our constraints are consistent with the static nature of and value of w expected for a cosmological constant (i.e., w0=-1.0, dw/dz=0) and are inconsistent with very rapid evolution of dark energy. We address consequences of evolving dark energy for the fate of the universe. Based on observations with the NASA/ESA Hubble Space Telescope, obtained at the Space Telescope Science Institute, which is operated by AURA, Inc., under NASA contract NAS5-26555."}
{"bibcode": "2013ARA&A..51..511K", "title": "Coevolution (Or Not) of Supermassive Black Holes and Host Galaxies", "abstract": "Supermassive black holes (BHs) have been found in 85 galaxies by dynamical modeling of spatially resolved kinematics. The Hubble Space Telescope revolutionized BH research by advancing the subject from its proof-of-concept phase into quantitative studies of BH demographics. Most influential was the discovery of a tight correlation between BH mass [Formula: see text] and the velocity dispersion σ of the bulge component of the host galaxy. Together with similar correlations with bulge luminosity and mass, this led to the widespread belief that BHs and bulges coevolve by regulating each other's growth. Conclusions based on one set of correlations from [Formula: see text] in brightest cluster ellipticals to [Formula: see text] in the smallest galaxies dominated BH work for more than a decade. New results are now replacing this simple story with a richer and more plausible picture in which BHs correlate differently with different galaxy components. A reasonable aim is to use this progress to refine our understanding of BH-galaxy coevolution. BHs with masses of 10<SUP>5</SUP>-10<SUP>6</SUP>M<SUB>⊙</SUB> are found in many bulgeless galaxies. Therefore, classical (elliptical-galaxy-like) bulges are not necessary for BH formation. On the other hand, although they live in galaxy disks, BHs do not correlate with galaxy disks. Also, any [Formula: see text] correlations with the properties of disk-grown pseudobulges and dark matter halos are weak enough to imply no close coevolution. The above and other correlations of host-galaxy parameters with each other and with [Formula: see text] suggest that there are four regimes of BH feedback. (1) Local, secular, episodic, and stochastic feeding of small BHs in largely bulgeless galaxies involves too little energy to result in coevolution. (2) Global feeding in major, wet galaxy mergers rapidly grows giant BHs in short-duration, quasar-like events whose energy feedback does affect galaxy evolution. The resulting hosts are classical bulges and coreless-rotating-disky ellipticals. (3) After these AGN phases and at the highest galaxy masses, maintenance-mode BH feedback into X-ray-emitting gas has the primarily negative effect of helping to keep baryons locked up in hot gas and thereby keeping galaxy formation from going to completion. This happens in giant, core-nonrotating-boxy ellipticals. Their properties, including their tight correlations between [Formula: see text] and core parameters, support the conclusion that core ellipticals form by dissipationless major mergers. They inherit coevolution effects from smaller progenitor galaxies. Also, (4) independent of any feedback physics, in BH growth modes 2 and 3, the averaging that results from successive mergers plays a major role in decreasing the scatter in [Formula: see text] correlations from the large values observed in bulgeless and pseudobulge galaxies to the small values observed in giant elliptical galaxies.", "database": ["astronomy"], "keywords": ["Astrophysics - Cosmology and Nongalactic Astrophysics"], "year": "2013", "doctype": "article", "citation_count": 4003, "domain_category": "astronomy", "abstract_clean": "Supermassive black holes (BHs) have been found in 85 galaxies by dynamical modeling of spatially resolved kinematics. The Hubble Space Telescope revolutionized BH research by advancing the subject from its proof-of-concept phase into quantitative studies of BH demographics. Most influential was the discovery of a tight correlation between BH mass [Formula: see text] and the velocity dispersion σ of the bulge component of the host galaxy. Together with similar correlations with bulge luminosity and mass, this led to the widespread belief that BHs and bulges coevolve by regulating each other's growth. Conclusions based on one set of correlations from [Formula: see text] in brightest cluster ellipticals to [Formula: see text] in the smallest galaxies dominated BH work for more than a decade. New results are now replacing this simple story with a richer and more plausible picture in which BHs correlate differently with different galaxy components. A reasonable aim is to use this progress to refine our understanding of BH-galaxy coevolution. BHs with masses of 105-106M⊙ are found in many bulgeless galaxies. Therefore, classical (elliptical-galaxy-like) bulges are not necessary for BH formation. On the other hand, although they live in galaxy disks, BHs do not correlate with galaxy disks. Also, any [Formula: see text] correlations with the properties of disk-grown pseudobulges and dark matter halos are weak enough to imply no close coevolution. The above and other correlations of host-galaxy parameters with each other and with [Formula: see text] suggest that there are four regimes of BH feedback. (1) Local, secular, episodic, and stochastic feeding of small BHs in largely bulgeless galaxies involves too little energy to result in coevolution. (2) Global feeding in major, wet galaxy mergers rapidly grows giant BHs in short-duration, quasar-like events whose energy feedback does affect galaxy evolution. The resulting hosts are classical bulges and coreless-rotating-disky ellipticals. (3) After these AGN phases and at the highest galaxy masses, maintenance-mode BH feedback into X-ray-emitting gas has the primarily negative effect of helping to keep baryons locked up in hot gas and thereby keeping galaxy formation from going to completion. This happens in giant, core-nonrotating-boxy ellipticals. Their properties, including their tight correlations between [Formula: see text] and core parameters, support the conclusion that core ellipticals form by dissipationless major mergers. They inherit coevolution effects from smaller progenitor galaxies. Also, (4) independent of any feedback physics, in BH growth modes 2 and 3, the averaging that results from successive mergers plays a major role in decreasing the scatter in [Formula: see text] correlations from the large values observed in bulgeless and pseudobulge galaxies to the small values observed in giant elliptical galaxies."}
{"bibcode": "1977ApJ...217..425M", "title": "The size distribution of interstellar grains.", "abstract": "The observed interstellar extinction over the wavelength range 0.11 μm &lt; λ &lt; 1 μm was fitted with a very general particle size distribution of uncoated graphite, enstatite, olivine, silicon carbide, iron, and magnetite. Combinations of these materials, up to three at a time, were considered. The cosmic abundances ofthe various constituents were taken into account as constraints on the possible distributions of particle sizes. <P />Excellent fits to the interstellar extinction, including the narrowness of the λ2160 feature, proved possible. Graphite was a necessary component of any good mixture, but it could be used with any of the other materials. The particle size distributions are roughly power law in nature, with an exponent of about -3.3 to -3.6. The size range for graphite is about 0.005 μm to about 1 μm. The size distribution for the other materials is also approximately power law in nature, with the same exponent, but there is a narrower range of sizes: about 0.025-0.25 μm, depending on the material. The number of large particles is not well determined, because they are gray. Similarly, the number of small particles is not well determined because they are in the Rayleigh limit. This power-law distribution is drastically different from an Oort-van de Hulst distribution, which is much more slowly varying for small particles but drops much faster for particles larger than average. <P />The extinction was also fitted with spherical graphite particles plus cylinders of each of the other materials. Linear and circular polarizations were then determined for the cylinders on the assumption of Davis-Greenstein alignment. The extinction was quite satisfactory, but the linear polarization reached a maximum in the ultraviolet (about 1600 Å). This is because the mixture contains many small particles. Ifthe small particles are not elongated or aligned, the wavelength dependence of the polarization can be fitted, but the larger particles which are aligned do not provide enough polarization per magnitude of extinction. However, a fit to polarization and extinction can be achieved if the material responsible for the polarization contributes only a small part of the extinction but consists of fairly large particles and is very well aligned. Dielectric particles with coatings could also provide the polarization. Subject headings: interstellar : matter «— polarization —", "database": ["astronomy"], "keywords": ["Cosmic Dust", "Interstellar Extinction", "Interstellar Matter", "Particle Size Distribution", "Enstatite", "Graphite", "Magnetite", "Olivine", "Polarization Characteristics", "Ultraviolet Astronomy", "Astrophysics"], "year": "1977", "doctype": "article", "citation_count": 3986, "domain_category": "astronomy", "abstract_clean": "The observed interstellar extinction over the wavelength range 0.11 μm < λ < 1 μm was fitted with a very general particle size distribution of uncoated graphite, enstatite, olivine, silicon carbide, iron, and magnetite. Combinations of these materials, up to three at a time, were considered. The cosmic abundances ofthe various constituents were taken into account as constraints on the possible distributions of particle sizes. Excellent fits to the interstellar extinction, including the narrowness of the λ2160 feature, proved possible. Graphite was a necessary component of any good mixture, but it could be used with any of the other materials. The particle size distributions are roughly power law in nature, with an exponent of about -3.3 to -3.6. The size range for graphite is about 0.005 μm to about 1 μm. The size distribution for the other materials is also approximately power law in nature, with the same exponent, but there is a narrower range of sizes: about 0.025-0.25 μm, depending on the material. The number of large particles is not well determined, because they are gray. Similarly, the number of small particles is not well determined because they are in the Rayleigh limit. This power-law distribution is drastically different from an Oort-van de Hulst distribution, which is much more slowly varying for small particles but drops much faster for particles larger than average. The extinction was also fitted with spherical graphite particles plus cylinders of each of the other materials. Linear and circular polarizations were then determined for the cylinders on the assumption of Davis-Greenstein alignment. The extinction was quite satisfactory, but the linear polarization reached a maximum in the ultraviolet (about 1600 Å). This is because the mixture contains many small particles. Ifthe small particles are not elongated or aligned, the wavelength dependence of the polarization can be fitted, but the larger particles which are aligned do not provide enough polarization per magnitude of extinction. However, a fit to polarization and extinction can be achieved if the material responsible for the polarization contributes only a small part of the extinction but consists of fairly large particles and is very well aligned. Dielectric particles with coatings could also provide the polarization. Subject headings: interstellar : matter «— polarization —"}
{"bibcode": "1982MNRAS.199..883B", "title": "Hydromagnetic flows from accretion disks and the production of radio jets.", "abstract": "The possibility is examined that angular momentum is removed magnetically from an accretion disk by field lines that leave the disk surface, and is eventually carried off in a jet moving perpendicular to the disk. The mechanism is illustrated by a self-similar MHD solution, with the gas being regarded as cold and starting from rest at the equatorial plane, with the disk itself in Keplerian orbit about a black hole. It is shown that a centrifugally driven outflow of matter from the disk is possible if the poloidal component of the magnetic field makes an angle of less than 60 deg with disk surface. At large distances the outflow forms a pair of collimated, antiparallel jets, while close to the disk it is probably driven by gas pressure in a hot, magnetically dominated corona.", "database": ["astronomy"], "keywords": ["Astrophysics", "Energy Transfer", "Magnetohydrodynamic Flow", "Momentum Transfer", "Radio Jets (Astronomy)", "Stellar Mass Accretion", "Angular Momentum", "Black Holes (Astronomy)", "Critical Point", "Disks (Shapes)", "Solar Corona", "Astrophysics", "Accretion Disks:Magnetohydrodynamics", "Accretion Disks:Radio Jets", "Radio Galaxies:Radio Jets"], "year": "1982", "doctype": "article", "citation_count": 3940, "domain_category": "astronomy", "abstract_clean": "The possibility is examined that angular momentum is removed magnetically from an accretion disk by field lines that leave the disk surface, and is eventually carried off in a jet moving perpendicular to the disk. The mechanism is illustrated by a self-similar MHD solution, with the gas being regarded as cold and starting from rest at the equatorial plane, with the disk itself in Keplerian orbit about a black hole. It is shown that a centrifugally driven outflow of matter from the disk is possible if the poloidal component of the magnetic field makes an angle of less than 60 deg with disk surface. At large distances the outflow forms a pair of collimated, antiparallel jets, while close to the disk it is probably driven by gas pressure in a hot, magnetically dominated corona."}
{"bibcode": "2010ForEM.259..660A", "title": "A global overview of drought and heat-induced tree mortality reveals emerging climate change risks for forests", "abstract": "Greenhouse gas emissions have significantly altered global climate, and will continue to do so in the future. Increases in the frequency, duration, and/or severity of drought and heat stress associated with climate change could fundamentally alter the composition, structure, and biogeography of forests in many regions. Of particular concern are potential increases in tree mortality associated with climate-induced physiological stress and interactions with other climate-mediated processes such as insect outbreaks and wildfire. Despite this risk, existing projections of tree mortality are based on models that lack functionally realistic mortality mechanisms, and there has been no attempt to track observations of climate-driven tree mortality globally. Here we present the first global assessment of recent tree mortality attributed to drought and heat stress. Although episodic mortality occurs in the absence of climate change, studies compiled here suggest that at least some of the world's forested ecosystems already may be responding to climate change and raise concern that forests may become increasingly vulnerable to higher background tree mortality rates and die-off in response to future warming and drought, even in environments that are not normally considered water-limited. This further suggests risks to ecosystem services, including the loss of sequestered forest carbon and associated atmospheric feedbacks. Our review also identifies key information gaps and scientific uncertainties that currently hinder our ability to predict tree mortality in response to climate change and emphasizes the need for a globally coordinated observation system. Overall, our review reveals the potential for amplified tree mortality due to drought and heat in forests worldwide.", "database": ["earth science"], "keywords": ["Climate change", "Drought effects", "Forest die-off", "Forest mortality", "Global patterns", "Tree mortality"], "year": "2010", "doctype": "article", "citation_count": 4125, "domain_category": "earth_science", "abstract_clean": "Greenhouse gas emissions have significantly altered global climate, and will continue to do so in the future. Increases in the frequency, duration, and/or severity of drought and heat stress associated with climate change could fundamentally alter the composition, structure, and biogeography of forests in many regions. Of particular concern are potential increases in tree mortality associated with climate-induced physiological stress and interactions with other climate-mediated processes such as insect outbreaks and wildfire. Despite this risk, existing projections of tree mortality are based on models that lack functionally realistic mortality mechanisms, and there has been no attempt to track observations of climate-driven tree mortality globally. Here we present the first global assessment of recent tree mortality attributed to drought and heat stress. Although episodic mortality occurs in the absence of climate change, studies compiled here suggest that at least some of the world's forested ecosystems already may be responding to climate change and raise concern that forests may become increasingly vulnerable to higher background tree mortality rates and die-off in response to future warming and drought, even in environments that are not normally considered water-limited. This further suggests risks to ecosystem services, including the loss of sequestered forest carbon and associated atmospheric feedbacks. Our review also identifies key information gaps and scientific uncertainties that currently hinder our ability to predict tree mortality in response to climate change and emphasizes the need for a globally coordinated observation system. Overall, our review reveals the potential for amplified tree mortality due to drought and heat in forests worldwide."}
{"bibcode": "2013JGRD..118.5380B", "title": "Bounding the role of black carbon in the climate system: A scientific assessment", "abstract": "carbon aerosol plays a unique and important role in Earth's climate system. Black carbon is a type of carbonaceous material with a unique combination of physical properties. This assessment provides an evaluation of black-carbon climate forcing that is comprehensive in its inclusion of all known and relevant processes and that is quantitative in providing best estimates and uncertainties of the main forcing terms: direct solar absorption; influence on liquid, mixed phase, and ice clouds; and deposition on snow and ice. These effects are calculated with climate models, but when possible, they are evaluated with both microphysical measurements and field observations. Predominant sources are combustion related, namely, fossil fuels for transportation, solid fuels for industrial and residential uses, and open burning of biomass. Total global emissions of black carbon using bottom-up inventory methods are 7500 Gg yr<SUP>-1</SUP> in the year 2000 with an uncertainty range of 2000 to 29000. However, global atmospheric absorption attributable to black carbon is too low in many models and should be increased by a factor of almost 3. After this scaling, the best estimate for the industrial-era (1750 to 2005) direct radiative forcing of atmospheric black carbon is +0.71 W m<SUP>-2</SUP> with 90% uncertainty bounds of (+0.08, +1.27) W m<SUP>-2</SUP>. Total direct forcing by all black carbon sources, without subtracting the preindustrial background, is estimated as +0.88 (+0.17, +1.48) W m<SUP>-2</SUP>. Direct radiative forcing alone does not capture important rapid adjustment mechanisms. A framework is described and used for quantifying climate forcings, including rapid adjustments. The best estimate of industrial-era climate forcing of black carbon through all forcing mechanisms, including clouds and cryosphere forcing, is +1.1 W m<SUP>-2</SUP> with 90% uncertainty bounds of +0.17 to +2.1 W m<SUP>-2</SUP>. Thus, there is a very high probability that black carbon emissions, independent of co-emitted species, have a positive forcing and warm the climate. We estimate that black carbon, with a total climate forcing of +1.1 W m<SUP>-2</SUP>, is the second most important human emission in terms of its climate forcing in the present-day atmosphere; only carbon dioxide is estimated to have a greater forcing. Sources that emit black carbon also emit other short-lived species that may either cool or warm climate. Climate forcings from co-emitted species are estimated and used in the framework described herein. When the principal effects of short-lived co-emissions, including cooling agents such as sulfur dioxide, are included in net forcing, energy-related sources (fossil fuel and biofuel) have an industrial-era climate forcing of +0.22 (-0.50 to +1.08) W m<SUP>-2</SUP> during the first year after emission. For a few of these sources, such as diesel engines and possibly residential biofuels, warming is strong enough that eliminating all short-lived emissions from these sources would reduce net climate forcing (i.e., produce cooling). When open burning emissions, which emit high levels of organic matter, are included in the total, the best estimate of net industrial-era climate forcing by all short-lived species from black-carbon-rich sources becomes slightly negative (-0.06 W m<SUP>-2</SUP> with 90% uncertainty bounds of -1.45 to +1.29 W m<SUP>-2</SUP>). The uncertainties in net climate forcing from black-carbon-rich sources are substantial, largely due to lack of knowledge about cloud interactions with both black carbon and co-emitted organic carbon. In prioritizing potential black-carbon mitigation actions, non-science factors, such as technical feasibility, costs, policy design, and implementation feasibility play important roles. The major sources of black carbon are presently in different stages with regard to the feasibility for near-term mitigation. This assessment, by evaluating the large number and complexity of the associated physical and radiative processes in black-carbon climate forcing, sets a baseline from which to improve future climate forcing estimates.", "database": ["physics", "earth science"], "keywords": ["black carbon", "climate forcing", "aerosol"], "year": "2013", "doctype": "article", "citation_count": 4045, "domain_category": "earth_science", "abstract_clean": "carbon aerosol plays a unique and important role in Earth's climate system. Black carbon is a type of carbonaceous material with a unique combination of physical properties. This assessment provides an evaluation of black-carbon climate forcing that is comprehensive in its inclusion of all known and relevant processes and that is quantitative in providing best estimates and uncertainties of the main forcing terms: direct solar absorption; influence on liquid, mixed phase, and ice clouds; and deposition on snow and ice. These effects are calculated with climate models, but when possible, they are evaluated with both microphysical measurements and field observations. Predominant sources are combustion related, namely, fossil fuels for transportation, solid fuels for industrial and residential uses, and open burning of biomass. Total global emissions of black carbon using bottom-up inventory methods are 7500 Gg yr-1 in the year 2000 with an uncertainty range of 2000 to 29000. However, global atmospheric absorption attributable to black carbon is too low in many models and should be increased by a factor of almost 3. After this scaling, the best estimate for the industrial-era (1750 to 2005) direct radiative forcing of atmospheric black carbon is +0.71 W m-2 with 90% uncertainty bounds of (+0.08, +1.27) W m-2. Total direct forcing by all black carbon sources, without subtracting the preindustrial background, is estimated as +0.88 (+0.17, +1.48) W m-2. Direct radiative forcing alone does not capture important rapid adjustment mechanisms. A framework is described and used for quantifying climate forcings, including rapid adjustments. The best estimate of industrial-era climate forcing of black carbon through all forcing mechanisms, including clouds and cryosphere forcing, is +1.1 W m-2 with 90% uncertainty bounds of +0.17 to +2.1 W m-2. Thus, there is a very high probability that black carbon emissions, independent of co-emitted species, have a positive forcing and warm the climate. We estimate that black carbon, with a total climate forcing of +1.1 W m-2, is the second most important human emission in terms of its climate forcing in the present-day atmosphere; only carbon dioxide is estimated to have a greater forcing. Sources that emit black carbon also emit other short-lived species that may either cool or warm climate. Climate forcings from co-emitted species are estimated and used in the framework described herein. When the principal effects of short-lived co-emissions, including cooling agents such as sulfur dioxide, are included in net forcing, energy-related sources (fossil fuel and biofuel) have an industrial-era climate forcing of +0.22 (-0.50 to +1.08) W m-2 during the first year after emission. For a few of these sources, such as diesel engines and possibly residential biofuels, warming is strong enough that eliminating all short-lived emissions from these sources would reduce net climate forcing (i.e., produce cooling). When open burning emissions, which emit high levels of organic matter, are included in the total, the best estimate of net industrial-era climate forcing by all short-lived species from black-carbon-rich sources becomes slightly negative (-0.06 W m-2 with 90% uncertainty bounds of -1.45 to +1.29 W m-2). The uncertainties in net climate forcing from black-carbon-rich sources are substantial, largely due to lack of knowledge about cloud interactions with both black carbon and co-emitted organic carbon. In prioritizing potential black-carbon mitigation actions, non-science factors, such as technical feasibility, costs, policy design, and implementation feasibility play important roles. The major sources of black carbon are presently in different stages with regard to the feasibility for near-term mitigation. This assessment, by evaluating the large number and complexity of the associated physical and radiative processes in black-carbon climate forcing, sets a baseline from which to improve future climate forcing estimates."}
{"bibcode": "2004BAMS...85..381R", "title": "The Global Land Data Assimilation System.", "abstract": "A Global Land Data Assimilation System (GLDAS) has been developed. Its purpose is to ingest satellite- and ground-based observational data products, using advanced land surface modeling and data assimilation techniques, in order to generate optimal fields of land surface states and fluxes. GLDAS is unique in that it is an uncoupled land surface modeling system that drives multiple models, integrates a huge quantity of observation-based data, runs globally at high resolution (0.25°), and produces results in near real time (typically within 48 h of the present). GLDAS is also a test bed for innovative modeling and assimilation capabilities. A vegetation-based “tiling” approach is used to simulate subgrid-scale variability, with a 1-km global vegetation dataset as its basis. Soil and elevation parameters are based on high-resolution global datasets. Observation-based precipitation and downward radiation and output fields from the best available global coupled atmospheric data assimilation systems are employed as forcing data. The high-quality, global land surface fields provided by GLDAS will be used to initialize weather and climate prediction models and will promote various hydrometeorological studies and applications. The ongoing GLDAS archive (started in 2001) of modeled and observed, global, surface meteorological data, parameter maps, and output is publicly available.", "database": ["physics", "earth science"], "keywords": [], "year": "2004", "doctype": "article", "citation_count": 4026, "domain_category": "earth_science", "abstract_clean": "A Global Land Data Assimilation System (GLDAS) has been developed. Its purpose is to ingest satellite- and ground-based observational data products, using advanced land surface modeling and data assimilation techniques, in order to generate optimal fields of land surface states and fluxes. GLDAS is unique in that it is an uncoupled land surface modeling system that drives multiple models, integrates a huge quantity of observation-based data, runs globally at high resolution (0.25°), and produces results in near real time (typically within 48 h of the present). GLDAS is also a test bed for innovative modeling and assimilation capabilities. A vegetation-based “tiling” approach is used to simulate subgrid-scale variability, with a 1-km global vegetation dataset as its basis. Soil and elevation parameters are based on high-resolution global datasets. Observation-based precipitation and downward radiation and output fields from the best available global coupled atmospheric data assimilation systems are employed as forcing data. The high-quality, global land surface fields provided by GLDAS will be used to initialize weather and climate prediction models and will promote various hydrometeorological studies and applications. The ongoing GLDAS archive (started in 2001) of modeled and observed, global, surface meteorological data, parameter maps, and output is publicly available."}
{"bibcode": "1986AREPS..14..493Z", "title": "Chemical Geodynamics", "abstract": "Consideration is given to the following three principal boundary conditions relating to the nature and development of chemical structure in the earth's mantle: (1) inferred scale lengths for mantle chemical heterogeneities, (2) interrelationships of the various isotopic tracers, and (3) the bulk composition of the earth. These boundary conditions are integrated with geophysical constraints in order to evaluate models for the development of the physical and chemical structure of the mantle. Data indicate that: (1) km-size heterogeneities can survive diffusive equilibrium for billions of years; (2) the mantle is chemically heterogeneous on both very small and very large scales; (3) isotopic heterogeneities in the mantle require the existence of four 'end-member' components (DMM, HIMU, EM I, and EM II) and are consistent with the existence of at least two additional components (BSE, PREMA); and (4) primitive undepleted mantle can made up no more than about 55 percent of the total mantle.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Earth Mantle", "Geochemistry", "Geodynamics", "Basalt", "Diffusion", "Isotopes", "Massifs", "Rocks"], "year": "1986", "doctype": "article", "citation_count": 3975, "domain_category": "earth_science", "abstract_clean": "Consideration is given to the following three principal boundary conditions relating to the nature and development of chemical structure in the earth's mantle: (1) inferred scale lengths for mantle chemical heterogeneities, (2) interrelationships of the various isotopic tracers, and (3) the bulk composition of the earth. These boundary conditions are integrated with geophysical constraints in order to evaluate models for the development of the physical and chemical structure of the mantle. Data indicate that: (1) km-size heterogeneities can survive diffusive equilibrium for billions of years; (2) the mantle is chemically heterogeneous on both very small and very large scales; (3) isotopic heterogeneities in the mantle require the existence of four 'end-member' components (DMM, HIMU, EM I, and EM II) and are consistent with the existence of at least two additional components (BSE, PREMA); and (4) primitive undepleted mantle can made up no more than about 55 percent of the total mantle."}
{"bibcode": "1996RSEnv..58..257G", "title": "NDWI-A normalized difference water index for remote sensing of vegetation liquid water from space", "abstract": "The normalized difference vegetation index (NDVI) has been widely used for remote sensing of vegetation for many years. This index uses radiances or reflectances from a red channel around 0.66 μm and a near-IR channel around 0.86 μm. The red channel is located in the strong chlorophyll absorption region, while the near-IR channel is located in the high reflectance plateau of vegetation canopies. The two channels sense very different depths through vegetation canopies. In this article, another index, namely, the normalized difference water index (NDWI), is proposed for remote sensing of vegetation liquid water from space. NDWI is defined as (ϱ(0.86 μm) - ϱ(1.24 μ m))/(ϱ(0.86 μ m) + ϱ(1.24 μ m)), where ϱ represents the radiance in reflectance units. Both the 0.86-μm and the 1.24-μm channels are located in the high reflectance plateau of vegetation canopies. They sense similar depths through vegetation canopies. Absorption by vegetation liquid water near 0.86 μm is negligible. Weak liquid absorption at 1.24 μm is present. Canopy scattering enhances the water absorption. As a result, NDWI is sensitive to changes in liquid water content of vegetation canopies. Atmospheric aerosol scattering effects in the 0.86-1.24 μm region are weak. NDWI is less sensitive to atmospheric effects than NDVI. NDWI does not remove completely the background soil reflectance effects, similar to NDVI. Because the information about vegetation canopies contained in the 1.24-μm channel is very different from that contained in the red channel near 0.66 μm, NDWI should be considered as an independent vegetation index. It is complementary to, not a substitute for NDVI. Laboratory-measured reflectance spectra of stacked green leaves, and spectral imaging data acquired with Airborne Visible Infrared Imaging Spectrometer (AVIRIS) over Jasper Ridge in California and the High Plains in northern Colorado, are used to demonstrate the usefulness of NDWI. Comparisons between NDWI and NDVI images are also given.", "database": ["physics", "earth science"], "keywords": [], "year": "1996", "doctype": "article", "citation_count": 3901, "domain_category": "earth_science", "abstract_clean": "The normalized difference vegetation index (NDVI) has been widely used for remote sensing of vegetation for many years. This index uses radiances or reflectances from a red channel around 0.66 μm and a near-IR channel around 0.86 μm. The red channel is located in the strong chlorophyll absorption region, while the near-IR channel is located in the high reflectance plateau of vegetation canopies. The two channels sense very different depths through vegetation canopies. In this article, another index, namely, the normalized difference water index (NDWI), is proposed for remote sensing of vegetation liquid water from space. NDWI is defined as (ϱ(0.86 μm) - ϱ(1.24 μ m))/(ϱ(0.86 μ m) + ϱ(1.24 μ m)), where ϱ represents the radiance in reflectance units. Both the 0.86-μm and the 1.24-μm channels are located in the high reflectance plateau of vegetation canopies. They sense similar depths through vegetation canopies. Absorption by vegetation liquid water near 0.86 μm is negligible. Weak liquid absorption at 1.24 μm is present. Canopy scattering enhances the water absorption. As a result, NDWI is sensitive to changes in liquid water content of vegetation canopies. Atmospheric aerosol scattering effects in the 0.86-1.24 μm region are weak. NDWI is less sensitive to atmospheric effects than NDVI. NDWI does not remove completely the background soil reflectance effects, similar to NDVI. Because the information about vegetation canopies contained in the 1.24-μm channel is very different from that contained in the red channel near 0.66 μm, NDWI should be considered as an independent vegetation index. It is complementary to, not a substitute for NDVI. Laboratory-measured reflectance spectra of stacked green leaves, and spectral imaging data acquired with Airborne Visible Infrared Imaging Spectrometer (AVIRIS) over Jasper Ridge in California and the High Plains in northern Colorado, are used to demonstrate the usefulness of NDWI. Comparisons between NDWI and NDVI images are also given."}
{"bibcode": "1995GeCoA..59.1217W", "title": "The composition of the continental crust", "abstract": "A new calculation of the crustal composition is based on the proportions of upper crust (UC) to felsic lower crust (FLC) to mafic lower crust (MLC) of about 1:0.6:0.4. These proportions are derived from a 3000 km long refraction seismic profile through western Europe (EGT) comprising 60% old shield and 40% younger fold belt area with about 40 km average Moho depth. A granodioritic bulk composition of the UC in major elements and thirty-two minor and trace elements was calculated from the Canadian Shield data (Shaw et al., 1967, 1976). The computed abundance of thirty-three additional trace elements in the UC is based on the following proportions of major rock units derived from mapping: 14% sedimentary rocks, 25% granites, 20% granodiorites, 5% tonalites, 6% gabbros, and 30% gneisses and mica schists. The composition of FLC and MLC in major and thirty-six minor and trace elements is calculated from data on felsic granulite terrains and mafic xenoliths, respectively, compiled by Rudnick and Presper (1990). More than thirty additional trace element abundances in FLC and MLC were computed or estimated from literature data. The bulk continental crust has a tonalitic and not a dioritic composition with distinctly higher concentrations of incompatible elements including the heat producing isotopes in our calculation. A dioritic bulk crust was suggested by Taylor and McLennan (1985). The amount of tonalite in the crust requires partial melting of mafic rocks with about 100 km thickness (compared with about 7 km in the present MLC) and water supply from dehydrated slabs and mafic intrusions. At the relatively low temperatures of old crustal segments MLC was partly converted into eclogite which could be recycled into the upper mantle under favourable tectonic conditions. The chemical fractionation of UC against FLC + MLC was caused by granitoidal partial melts and by mantle degassing which has controlled weathering and accumulation of volatile compounds close to the Earth's surface.", "database": ["astronomy", "earth science"], "keywords": [], "year": "1995", "doctype": "article", "citation_count": 3838, "domain_category": "earth_science", "abstract_clean": "A new calculation of the crustal composition is based on the proportions of upper crust (UC) to felsic lower crust (FLC) to mafic lower crust (MLC) of about 1:0.6:0.4. These proportions are derived from a 3000 km long refraction seismic profile through western Europe (EGT) comprising 60% old shield and 40% younger fold belt area with about 40 km average Moho depth. A granodioritic bulk composition of the UC in major elements and thirty-two minor and trace elements was calculated from the Canadian Shield data (Shaw et al., 1967, 1976). The computed abundance of thirty-three additional trace elements in the UC is based on the following proportions of major rock units derived from mapping: 14% sedimentary rocks, 25% granites, 20% granodiorites, 5% tonalites, 6% gabbros, and 30% gneisses and mica schists. The composition of FLC and MLC in major and thirty-six minor and trace elements is calculated from data on felsic granulite terrains and mafic xenoliths, respectively, compiled by Rudnick and Presper (1990). More than thirty additional trace element abundances in FLC and MLC were computed or estimated from literature data. The bulk continental crust has a tonalitic and not a dioritic composition with distinctly higher concentrations of incompatible elements including the heat producing isotopes in our calculation. A dioritic bulk crust was suggested by Taylor and McLennan (1985). The amount of tonalite in the crust requires partial melting of mafic rocks with about 100 km thickness (compared with about 7 km in the present MLC) and water supply from dehydrated slabs and mafic intrusions. At the relatively low temperatures of old crustal segments MLC was partly converted into eclogite which could be recycled into the upper mantle under favourable tectonic conditions. The chemical fractionation of UC against FLC + MLC was caused by granitoidal partial melts and by mantle degassing which has controlled weathering and accumulation of volatile compounds close to the Earth's surface."}
{"bibcode": "2009JHyd..377...80G", "title": "Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling", "abstract": "SummaryThe mean squared error (MSE) and the related normalization, the Nash-Sutcliffe efficiency (NSE), are the two criteria most widely used for calibration and evaluation of hydrological models with observed data. Here, we present a diagnostically interesting decomposition of NSE (and hence MSE), which facilitates analysis of the relative importance of its different components in the context of hydrological modelling, and show how model calibration problems can arise due to interactions among these components. The analysis is illustrated by calibrating a simple conceptual precipitation-runoff model to daily data for a number of Austrian basins having a broad range of hydro-meteorological characteristics. Evaluation of the results clearly demonstrates the problems that can be associated with any calibration based on the NSE (or MSE) criterion. While we propose and test an alternative criterion that can help to reduce model calibration problems, the primary purpose of this study is not to present an improved measure of model performance. Instead, we seek to show that there are systematic problems inherent with any optimization based on formulations related to the MSE. The analysis and results have implications to the manner in which we calibrate and evaluate environmental models; we discuss these and suggest possible ways forward that may move us towards an improved and diagnostically meaningful approach to model performance evaluation and identification.", "database": ["physics", "earth science"], "keywords": [], "year": "2009", "doctype": "article", "citation_count": 3817, "domain_category": "earth_science", "abstract_clean": "SummaryThe mean squared error (MSE) and the related normalization, the Nash-Sutcliffe efficiency (NSE), are the two criteria most widely used for calibration and evaluation of hydrological models with observed data. Here, we present a diagnostically interesting decomposition of NSE (and hence MSE), which facilitates analysis of the relative importance of its different components in the context of hydrological modelling, and show how model calibration problems can arise due to interactions among these components. The analysis is illustrated by calibrating a simple conceptual precipitation-runoff model to daily data for a number of Austrian basins having a broad range of hydro-meteorological characteristics. Evaluation of the results clearly demonstrates the problems that can be associated with any calibration based on the NSE (or MSE) criterion. While we propose and test an alternative criterion that can help to reduce model calibration problems, the primary purpose of this study is not to present an improved measure of model performance. Instead, we seek to show that there are systematic problems inherent with any optimization based on formulations related to the MSE. The analysis and results have implications to the manner in which we calibrate and evaluate environmental models; we discuss these and suggest possible ways forward that may move us towards an improved and diagnostically meaningful approach to model performance evaluation and identification."}
{"bibcode": "2005EcoM...75....3H", "title": "Effects of Biodiversity on Ecosystem Functioning: a Consensus of Current Knowledge", "abstract": "Humans are altering the composition of biological communities through a variety of activities that increase rates of species invasions and species extinctions, at all scales, from local to global. These changes in components of the Earth's biodiversity cause concern for ethical and aesthetic reasons, but they also have a strong potential to alter ecosystem properties and the goods and services they provide to humanity. Ecological experiments, observations, and theoretical developments show that ecosystem properties depend greatly on biodiversity in terms of the functional characteristics of organisms present in the ecosystem and the distribution and abundance of those organisms over space and time. Species effects act in concert with the effects of climate, resource availability, and disturbance regimes in influencing ecosystem properties. Human activities can modify all of the above factors; here we focus on modification of these biotic controls. The scientific community has come to a broad consensus on many aspects of the relationship between biodiversity and ecosystem functioning, including many points relevant to management of ecosystems. Further progress will require integration of knowledge about biotic and abiotic controls on ecosystem properties, how ecological communities are structured, and the forces driving species extinctions and invasions. To strengthen links to policy and management, we also need to integrate our ecological knowledge with understanding of the social and economic constraints of potential management practices. Understanding this complexity, while taking strong steps to minimize current losses of species, is necessary for responsible management of Earth's ecosystems and the diverse biota they contain. Based on our review of the scientific literature, we are certain of the following conclusions: 1) Species' functional characteristics strongly influence ecosystem properties. Functional characteristics operate in a variety of contexts, including effects of dominant species, keystone species, ecological engineers, and interactions among species (e.g., competition, facilitation, mutualism, disease, and predation). Relative abundance alone is not always a good predictor of the ecosystem-level importance of a species, as even relatively rare species (e.g., a keystone predator) can strongly influence pathways of energy and material flows. 2) Alteration of biota in ecosystems via species invasions and extinctions caused by human activities has altered ecosystem goods and services in many well-documented cases. Many of these changes are difficult, expensive, or impossible to reverse or fix with technological solutions. 3) The effects of species loss or changes in composition, and the mechanisms by which the effects manifest themselves, can differ among ecosystem properties, ecosystem types, and pathways of potential community change. 4) Some ecosystem properties are initially insensitive to species loss because (a) ecosystems may have multiple species that carry out similar functional roles, (b) some species may contribute relatively little to ecosystem properties, or (c) properties may be primarily controlled by abiotic environmental conditions. 5) More species are needed to insure a stable supply of ecosystem goods and services as spatial and temporal variability increases, which typically occurs as longer time periods and larger areas are considered. We have high confidence in the following conclusions: 1) Certain combinations of species are complementary in their patterns of resource use and can increase average rates of productivity and nutrient retention. At the same time, environmental conditions can influence the importance of complementarity in structuring communities. Identification of which and how many species act in a complementary way in complex communities is just beginning. 2) Susceptibility to invasion by exotic species is strongly influenced by species composition and, under similar environmental conditions, generally decreases with increasing species richness. However, several other factors, such as propagule pressure, disturbance regime, and resource availability also strongly influence invasion success and often override effects of species richness in comparisons across different sites or ecosystems. 3) Having a range of species that respond differently to different environmental perturbations can stabilize ecosystem process rates in response to disturbances and variation in abiotic conditions. Using practices that maintain a diversity of organisms of different functional effect and functional response types will help preserve a range of management options. Uncertainties remain and further research is necessary in the following areas: 1) Further resolution of the relationships among taxonomic diversity, functional diversity, and community structure is important for identifying mechanisms of biodiversity effects. 2) Multiple trophic levels are common to ecosystems but have been understudied in biodiversity/ecosystem functioning research. The response of ecosystem properties to varying composition and diversity of consumer organisms is much more complex than responses seen in experiments that vary only the diversity of primary producers. 3) Theoretical work on stability has outpaced experimental work, especially field research. We need long-term experiments to be able to assess temporal stability, as well as experimental perturbations to assess response to and recovery from a variety of disturbances. Design and analysis of such experiments must account for several factors that covary with species diversity. 4) Because biodiversity both responds to and influences ecosystem properties, understanding the feedbacks involved is necessary to integrate results from experimental communities with patterns seen at broader scales. Likely patterns of extinction and invasion need to be linked to different drivers of global change, the forces that structure communities, and controls on ecosystem properties for the development of effective management and conservation strategies. 5) This paper focuses primarily on terrestrial systems, with some coverage of freshwater systems, because that is where most empirical and theoretical study has focused. While the fundamental principles described here should apply to marine systems, further study of that realm is necessary. Despite some uncertainties about the mechanisms and circumstances under which diversity influences ecosystem properties, incorporating diversity effects into policy and management is essential, especially in making decisions involving large temporal and spatial scales. Sacrificing those aspects of ecosystems that are difficult or impossible to reconstruct, such as diversity, simply because we are not yet certain about the extent and mechanisms by which they affect ecosystem properties, will restrict future management options even further. It is incumbent upon ecologists to communicate this need, and the values that can derive from such a perspective, to those charged with economic and policy decision-making.", "database": ["earth science"], "keywords": ["biodiversity", "complementary resource use", "ecosystem goods and services", "ecosystem processes", "ecosystem properties", "functional characteristics", "functional diversity", "net primary production", "sampling effect", "species extinction", "species invasions", "species richness", "stability"], "year": "2005", "doctype": "article", "citation_count": 3786, "domain_category": "earth_science", "abstract_clean": "Humans are altering the composition of biological communities through a variety of activities that increase rates of species invasions and species extinctions, at all scales, from local to global. These changes in components of the Earth's biodiversity cause concern for ethical and aesthetic reasons, but they also have a strong potential to alter ecosystem properties and the goods and services they provide to humanity. Ecological experiments, observations, and theoretical developments show that ecosystem properties depend greatly on biodiversity in terms of the functional characteristics of organisms present in the ecosystem and the distribution and abundance of those organisms over space and time. Species effects act in concert with the effects of climate, resource availability, and disturbance regimes in influencing ecosystem properties. Human activities can modify all of the above factors; here we focus on modification of these biotic controls. The scientific community has come to a broad consensus on many aspects of the relationship between biodiversity and ecosystem functioning, including many points relevant to management of ecosystems. Further progress will require integration of knowledge about biotic and abiotic controls on ecosystem properties, how ecological communities are structured, and the forces driving species extinctions and invasions. To strengthen links to policy and management, we also need to integrate our ecological knowledge with understanding of the social and economic constraints of potential management practices. Understanding this complexity, while taking strong steps to minimize current losses of species, is necessary for responsible management of Earth's ecosystems and the diverse biota they contain. Based on our review of the scientific literature, we are certain of the following conclusions: 1) Species' functional characteristics strongly influence ecosystem properties. Functional characteristics operate in a variety of contexts, including effects of dominant species, keystone species, ecological engineers, and interactions among species (e.g., competition, facilitation, mutualism, disease, and predation). Relative abundance alone is not always a good predictor of the ecosystem-level importance of a species, as even relatively rare species (e.g., a keystone predator) can strongly influence pathways of energy and material flows. 2) Alteration of biota in ecosystems via species invasions and extinctions caused by human activities has altered ecosystem goods and services in many well-documented cases. Many of these changes are difficult, expensive, or impossible to reverse or fix with technological solutions. 3) The effects of species loss or changes in composition, and the mechanisms by which the effects manifest themselves, can differ among ecosystem properties, ecosystem types, and pathways of potential community change. 4) Some ecosystem properties are initially insensitive to species loss because (a) ecosystems may have multiple species that carry out similar functional roles, (b) some species may contribute relatively little to ecosystem properties, or (c) properties may be primarily controlled by abiotic environmental conditions. 5) More species are needed to insure a stable supply of ecosystem goods and services as spatial and temporal variability increases, which typically occurs as longer time periods and larger areas are considered. We have high confidence in the following conclusions: 1) Certain combinations of species are complementary in their patterns of resource use and can increase average rates of productivity and nutrient retention. At the same time, environmental conditions can influence the importance of complementarity in structuring communities. Identification of which and how many species act in a complementary way in complex communities is just beginning. 2) Susceptibility to invasion by exotic species is strongly influenced by species composition and, under similar environmental conditions, generally decreases with increasing species richness. However, several other factors, such as propagule pressure, disturbance regime, and resource availability also strongly influence invasion success and often override effects of species richness in comparisons across different sites or ecosystems. 3) Having a range of species that respond differently to different environmental perturbations can stabilize ecosystem process rates in response to disturbances and variation in abiotic conditions. Using practices that maintain a diversity of organisms of different functional effect and functional response types will help preserve a range of management options. Uncertainties remain and further research is necessary in the following areas: 1) Further resolution of the relationships among taxonomic diversity, functional diversity, and community structure is important for identifying mechanisms of biodiversity effects. 2) Multiple trophic levels are common to ecosystems but have been understudied in biodiversity/ecosystem functioning research. The response of ecosystem properties to varying composition and diversity of consumer organisms is much more complex than responses seen in experiments that vary only the diversity of primary producers. 3) Theoretical work on stability has outpaced experimental work, especially field research. We need long-term experiments to be able to assess temporal stability, as well as experimental perturbations to assess response to and recovery from a variety of disturbances. Design and analysis of such experiments must account for several factors that covary with species diversity. 4) Because biodiversity both responds to and influences ecosystem properties, understanding the feedbacks involved is necessary to integrate results from experimental communities with patterns seen at broader scales. Likely patterns of extinction and invasion need to be linked to different drivers of global change, the forces that structure communities, and controls on ecosystem properties for the development of effective management and conservation strategies. 5) This paper focuses primarily on terrestrial systems, with some coverage of freshwater systems, because that is where most empirical and theoretical study has focused. While the fundamental principles described here should apply to marine systems, further study of that realm is necessary. Despite some uncertainties about the mechanisms and circumstances under which diversity influences ecosystem properties, incorporating diversity effects into policy and management is essential, especially in making decisions involving large temporal and spatial scales. Sacrificing those aspects of ecosystems that are difficult or impossible to reconstruct, such as diversity, simply because we are not yet certain about the extent and mechanisms by which they affect ecosystem properties, will restrict future management options even further. It is incumbent upon ecologists to communicate this need, and the values that can derive from such a perspective, to those charged with economic and policy decision-making."}
{"bibcode": "2016JPRS..114...24B", "title": "Random forest in remote sensing: A review of applications and future directions", "abstract": "A random forest (RF) classifier is an ensemble classifier that produces multiple decision trees, using a randomly selected subset of training samples and variables. This classifier has become popular within the remote sensing community due to the accuracy of its classifications. The overall objective of this work was to review the utilization of RF classifier in remote sensing. This review has revealed that RF classifier can successfully handle high data dimensionality and multicolinearity, being both fast and insensitive to overfitting. It is, however, sensitive to the sampling design. The variable importance (VI) measurement provided by the RF classifier has been extensively exploited in different scenarios, for example to reduce the number of dimensions of hyperspectral data, to identify the most relevant multisource remote sensing and geographic data, and to select the most suitable season to classify particular target classes. Further investigations are required into less commonly exploited uses of this classifier, such as for sample proximity analysis to detect and remove outliers in the training samples.", "database": ["physics", "earth science"], "keywords": ["Random forest", "Supervised classifier", "Ensemble classifier", "Review", "Feature selection"], "year": "2016", "doctype": "article", "citation_count": 3751, "domain_category": "earth_science", "abstract_clean": "A random forest (RF) classifier is an ensemble classifier that produces multiple decision trees, using a randomly selected subset of training samples and variables. This classifier has become popular within the remote sensing community due to the accuracy of its classifications. The overall objective of this work was to review the utilization of RF classifier in remote sensing. This review has revealed that RF classifier can successfully handle high data dimensionality and multicolinearity, being both fast and insensitive to overfitting. It is, however, sensitive to the sampling design. The variable importance (VI) measurement provided by the RF classifier has been extensively exploited in different scenarios, for example to reduce the number of dimensions of hyperspectral data, to identify the most relevant multisource remote sensing and geographic data, and to select the most suitable season to classify particular target classes. Further investigations are required into less commonly exploited uses of this classifier, such as for sample proximity analysis to detect and remove outliers in the training samples."}
{"bibcode": "2010ESRv...99..125S", "title": "Investigating soil moisture-climate interactions in a changing climate: A review", "abstract": "Soil moisture is a key variable of the climate system. It constrains plant transpiration and photosynthesis in several regions of the world, with consequent impacts on the water, energy and biogeochemical cycles. Moreover it is a storage component for precipitation and radiation anomalies, inducing persistence in the climate system. Finally, it is involved in a number of feedbacks at the local, regional and global scales, and plays a major role in climate-change projections. In this review, we provide a synthesis of past research on the role of soil moisture for the climate system, based both on modelling and observational studies. We focus on soil moisture-temperature and soil moisture-precipitation feedbacks, and their possible modifications with climate change. We also highlight further impacts of soil moisture on climate, and the state of research regarding the validation of the relevant processes. There are promises for major advances in this research field in coming years thanks to the development of new validation datasets and multi-model initiatives. However, the availability of ground observations continues to be critical in limiting progress and should therefore strongly be fostered at the international level. Exchanges across disciplines will also be essential for bridging current knowledge gaps in this field. This is of key importance given the manifold impacts of soil moisture on climate, and their relevance for climate-change projections. A better understanding and quantification of the relevant processes would significantly help to reduce uncertainties in future-climate scenarios, in particular with regard to changes in climate variability and extreme events, as well as ecosystem and agricultural impacts.", "database": ["physics", "earth science"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 3727, "domain_category": "earth_science", "abstract_clean": "Soil moisture is a key variable of the climate system. It constrains plant transpiration and photosynthesis in several regions of the world, with consequent impacts on the water, energy and biogeochemical cycles. Moreover it is a storage component for precipitation and radiation anomalies, inducing persistence in the climate system. Finally, it is involved in a number of feedbacks at the local, regional and global scales, and plays a major role in climate-change projections. In this review, we provide a synthesis of past research on the role of soil moisture for the climate system, based both on modelling and observational studies. We focus on soil moisture-temperature and soil moisture-precipitation feedbacks, and their possible modifications with climate change. We also highlight further impacts of soil moisture on climate, and the state of research regarding the validation of the relevant processes. There are promises for major advances in this research field in coming years thanks to the development of new validation datasets and multi-model initiatives. However, the availability of ground observations continues to be critical in limiting progress and should therefore strongly be fostered at the international level. Exchanges across disciplines will also be essential for bridging current knowledge gaps in this field. This is of key importance given the manifold impacts of soil moisture on climate, and their relevance for climate-change projections. A better understanding and quantification of the relevant processes would significantly help to reduce uncertainties in future-climate scenarios, in particular with regard to changes in climate variability and extreme events, as well as ecosystem and agricultural impacts."}
{"bibcode": "2008JGRD..11313103I", "title": "Radiative forcing by long-lived greenhouse gases: Calculations with the AER radiative transfer models", "abstract": "A primary component of the observed recent climate change is the radiative forcing from increased concentrations of long-lived greenhouse gases (LLGHGs). Effective simulation of anthropogenic climate change by general circulation models (GCMs) is strongly dependent on the accurate representation of radiative processes associated with water vapor, ozone, and LLGHGs. In the context of the increasing application of the Atmospheric and Environmental Research, Inc. (AER), radiation models within the GCM community, their capability to calculate longwave and shortwave radiative forcing for clear sky scenarios previously examined by the radiative transfer model intercomparison project (RTMIP) is presented. Forcing calculations with the AER line-by-line (LBL) models are very consistent with the RTMIP line-by-line results in the longwave and shortwave. The AER broadband models, in all but one case, calculate longwave forcings within a range of -0.20 to 0.23 W m<SUP>-2</SUP> of LBL calculations and shortwave forcings within a range of -0.16 to 0.38 W m<SUP>-2</SUP> of LBL results. These models also perform well at the surface, which RTMIP identified as a level at which GCM radiation models have particular difficulty reproducing LBL fluxes. Heating profile perturbations calculated by the broadband models generally reproduce high-resolution calculations within a few hundredths K d<SUP>-1</SUP> in the troposphere and within 0.15 K d<SUP>-1</SUP> in the peak stratospheric heating near 1 hPa. In most cases, the AER broadband models provide radiative forcing results that are in closer agreement with high-resolution calculations than the GCM radiation codes examined by RTMIP, which supports the application of the AER models to climate change research.", "database": ["physics", "earth science"], "keywords": ["Atmospheric Processes: Radiative processes", "Atmospheric Composition and Structure: Radiation: transmission and scattering", "Atmospheric Processes: Climate change and variability (1616", "1635", "3309", "4215", "4513)", "greenhouse forcing", "radiative transfer", "climate change"], "year": "2008", "doctype": "article", "citation_count": 3676, "domain_category": "earth_science", "abstract_clean": "A primary component of the observed recent climate change is the radiative forcing from increased concentrations of long-lived greenhouse gases (LLGHGs). Effective simulation of anthropogenic climate change by general circulation models (GCMs) is strongly dependent on the accurate representation of radiative processes associated with water vapor, ozone, and LLGHGs. In the context of the increasing application of the Atmospheric and Environmental Research, Inc. (AER), radiation models within the GCM community, their capability to calculate longwave and shortwave radiative forcing for clear sky scenarios previously examined by the radiative transfer model intercomparison project (RTMIP) is presented. Forcing calculations with the AER line-by-line (LBL) models are very consistent with the RTMIP line-by-line results in the longwave and shortwave. The AER broadband models, in all but one case, calculate longwave forcings within a range of -0.20 to 0.23 W m-2 of LBL calculations and shortwave forcings within a range of -0.16 to 0.38 W m-2 of LBL results. These models also perform well at the surface, which RTMIP identified as a level at which GCM radiation models have particular difficulty reproducing LBL fluxes. Heating profile perturbations calculated by the broadband models generally reproduce high-resolution calculations within a few hundredths K d-1 in the troposphere and within 0.15 K d-1 in the peak stratospheric heating near 1 hPa. In most cases, the AER broadband models provide radiative forcing results that are in closer agreement with high-resolution calculations than the GCM radiation codes examined by RTMIP, which supports the application of the AER models to climate change research."}
{"bibcode": "2002JCli...15.1609R", "title": "An Improved In Situ and Satellite SST Analysis for Climate.", "abstract": "A weekly 1° spatial resolution optimum interpolation (OI) sea surface temperature (SST) analysis has been produced at the National Oceanic and Atmospheric Administration (NOAA) using both in situ and satellite data from November 1981 to the present. The weekly product has been available since 1993 and is widely used for weather and climate monitoring and forecasting. Errors in the satellite bias correction and the sea ice to SST conversion algorithm are discussed, and then an improved version of the OI analysis is developed. The changes result in a modest reduction in the satellite bias that leaves small global residual biases of roughly 0.03°C. The major improvement in the analysis occurs at high latitudes due to the new sea ice algorithm where local differences between the old and new analysis can exceed 1°C. Comparisons with other SST products are needed to determine the consistency of the OI. These comparisons show that the differences among products occur on large time- and space scales with monthly rms differences exceeding 0.5°C in some regions. These regions are primarily the mid- and high-latitude Southern Oceans and the Arctic where data are sparse, as well as high-gradient areas such as the Gulf Stream and Kuroshio where the gradients cannot be properly resolved on a 1° grid. In addition, globally averaged differences of roughly 0.05°C occur among the products on decadal scales. These differences primarily arise from the same regions where the rms differences are large. However, smaller unexplained differences also occur in other regions of the midlatitude Northern Hemisphere where in situ data should be adequate.", "database": ["physics", "earth science"], "keywords": [], "year": "2002", "doctype": "article", "citation_count": 3616, "domain_category": "earth_science", "abstract_clean": "A weekly 1° spatial resolution optimum interpolation (OI) sea surface temperature (SST) analysis has been produced at the National Oceanic and Atmospheric Administration (NOAA) using both in situ and satellite data from November 1981 to the present. The weekly product has been available since 1993 and is widely used for weather and climate monitoring and forecasting. Errors in the satellite bias correction and the sea ice to SST conversion algorithm are discussed, and then an improved version of the OI analysis is developed. The changes result in a modest reduction in the satellite bias that leaves small global residual biases of roughly 0.03°C. The major improvement in the analysis occurs at high latitudes due to the new sea ice algorithm where local differences between the old and new analysis can exceed 1°C. Comparisons with other SST products are needed to determine the consistency of the OI. These comparisons show that the differences among products occur on large time- and space scales with monthly rms differences exceeding 0.5°C in some regions. These regions are primarily the mid- and high-latitude Southern Oceans and the Arctic where data are sparse, as well as high-gradient areas such as the Gulf Stream and Kuroshio where the gradients cannot be properly resolved on a 1° grid. In addition, globally averaged differences of roughly 0.05°C occur among the products on decadal scales. These differences primarily arise from the same regions where the rms differences are large. However, smaller unexplained differences also occur in other regions of the midlatitude Northern Hemisphere where in situ data should be adequate."}
{"bibcode": "2005OcMod...9..347S", "title": "The regional oceanic modeling system (ROMS): a split-explicit, free-surface, topography-following-coordinate oceanic model", "abstract": "The purpose of this study is to find a combination of optimal numerical algorithms for time-stepping and mode-splitting suitable for a high-resolution, free-surface, terrain-following coordinate oceanic model. Due to mathematical feedback between the baroclinic momentum and tracer equations and, similarly, between the barotropic momentum and continuity equations, it is advantageous to treat both modes so that, after a time step for the momentum equation, the computed velocities participate immediately in the computation of tracers and continuity, and vice versa, rather than advancing all equations for one time step simultaneously. This leads to a new family of time-stepping algorithms that combine forward-backward feedback with the best known synchronous algorithms, allowing an increased time step due to the enhanced internal stability without sacrificing its accuracy. Based on these algorithms we design a split-explicit hydrodynamic kernel for a realistic oceanic model, which addresses multiple numerical issues associated with mode splitting. This kernel utilizes consistent temporal averaging of the barotropic mode via a specially designed filter function to guarantee both exact conservation and constancy preservation properties for tracers and yields more accurate (up to second-order), resolved barotropic processes, while preventing aliasing of unresolved barotropic signals into the slow baroclinic motions. It has a more accurate mode-splitting due to redefined barotropic pressure-gradient terms to account for the local variations in density field, while maintaining the computational efficiency of a split model. It is naturally compatible with a variety of centered and upstream-biased high-order advection algorithms, and helps to mitigate computational cost of expensive physical parameterization of mixing processes and submodels.", "database": ["physics", "earth science"], "keywords": [], "year": "2005", "doctype": "article", "citation_count": 3562, "domain_category": "earth_science", "abstract_clean": "The purpose of this study is to find a combination of optimal numerical algorithms for time-stepping and mode-splitting suitable for a high-resolution, free-surface, terrain-following coordinate oceanic model. Due to mathematical feedback between the baroclinic momentum and tracer equations and, similarly, between the barotropic momentum and continuity equations, it is advantageous to treat both modes so that, after a time step for the momentum equation, the computed velocities participate immediately in the computation of tracers and continuity, and vice versa, rather than advancing all equations for one time step simultaneously. This leads to a new family of time-stepping algorithms that combine forward-backward feedback with the best known synchronous algorithms, allowing an increased time step due to the enhanced internal stability without sacrificing its accuracy. Based on these algorithms we design a split-explicit hydrodynamic kernel for a realistic oceanic model, which addresses multiple numerical issues associated with mode splitting. This kernel utilizes consistent temporal averaging of the barotropic mode via a specially designed filter function to guarantee both exact conservation and constancy preservation properties for tracers and yields more accurate (up to second-order), resolved barotropic processes, while preventing aliasing of unresolved barotropic signals into the slow baroclinic motions. It has a more accurate mode-splitting due to redefined barotropic pressure-gradient terms to account for the local variations in density field, while maintaining the computational efficiency of a split model. It is naturally compatible with a variety of centered and upstream-biased high-order advection algorithms, and helps to mitigate computational cost of expensive physical parameterization of mixing processes and submodels."}
{"bibcode": "1980QJRMS.106..447G", "title": "Some simple solutions for heat-induced tropical circulation", "abstract": "A simple analytic model is constructed to elucidate some basic features of the response of the tropical atmosphere to diabatic heating. In particular, there is considerable east-west asymmetry which can be illustrated by solutions for heating concentrated in an area of finite extent. This is of more than academic interest because heating in practice tends to be concentrated in specific areas. For instance, a model with heating symmetric about the equator at Indonesian longitudes produces low-level easterly flow over the Pacific through propagation of Kelvin waves into the region. It also produces low-level westerly inflow over the Indian Ocean (but in a smaller region) because planetary waves propagate there. In the heating region itself the low-level flow is away from the equator as required by the vorticity equation. The return flow toward the equator is farther west because of planetary wave propagation, and so cyclonic flow is obtained around lows which form on the western margins of the heating zone. Another model solution with the heating displaced north of the equator provides a flow similar to the monsoon circulation of July and a simple model solution can also be found for heating concentrated along an inter-tropical convergence line.", "database": ["physics", "earth science"], "keywords": [], "year": "1980", "doctype": "article", "citation_count": 3549, "domain_category": "earth_science", "abstract_clean": "A simple analytic model is constructed to elucidate some basic features of the response of the tropical atmosphere to diabatic heating. In particular, there is considerable east-west asymmetry which can be illustrated by solutions for heating concentrated in an area of finite extent. This is of more than academic interest because heating in practice tends to be concentrated in specific areas. For instance, a model with heating symmetric about the equator at Indonesian longitudes produces low-level easterly flow over the Pacific through propagation of Kelvin waves into the region. It also produces low-level westerly inflow over the Indian Ocean (but in a smaller region) because planetary waves propagate there. In the heating region itself the low-level flow is away from the equator as required by the vorticity equation. The return flow toward the equator is farther west because of planetary wave propagation, and so cyclonic flow is obtained around lows which form on the western margins of the heating zone. Another model solution with the heating displaced north of the equator provides a flow similar to the monsoon circulation of July and a simple model solution can also be found for heating concentrated along an inter-tropical convergence line."}
{"bibcode": "2004NPGeo..11..561G", "title": "Application of the cross wavelet transform and wavelet coherence to geophysical time series", "abstract": "Many scientists have made use of the wavelet method in analyzing time series, often using popular free software. However, at present there are no similar easy to use wavelet packages for analyzing two time series together. We discuss the cross wavelet transform and wavelet coherence for examining relationships in time frequency space between two time series. We demonstrate how phase angle statistics can be used to gain confidence in causal relationships and test mechanistic models of physical relationships between the time series. As an example of typical data where such analyses have proven useful, we apply the methods to the Arctic Oscillation index and the Baltic maximum sea ice extent record. Monte Carlo methods are used to assess the statistical significance against red noise backgrounds. A software package has been developed that allows users to perform the cross wavelet transform and wavelet coherence (<A href=\"http://www.pol.ac.uk/home/research/waveletcoherence/\">www.pol.ac.uk/home/research/waveletcoherence/</A>).", "database": ["physics", "earth science"], "keywords": [], "year": "2004", "doctype": "article", "citation_count": 3446, "domain_category": "earth_science", "abstract_clean": "Many scientists have made use of the wavelet method in analyzing time series, often using popular free software. However, at present there are no similar easy to use wavelet packages for analyzing two time series together. We discuss the cross wavelet transform and wavelet coherence for examining relationships in time frequency space between two time series. We demonstrate how phase angle statistics can be used to gain confidence in causal relationships and test mechanistic models of physical relationships between the time series. As an example of typical data where such analyses have proven useful, we apply the methods to the Arctic Oscillation index and the Baltic maximum sea ice extent record. Monte Carlo methods are used to assess the statistical significance against red noise backgrounds. A software package has been developed that allows users to perform the cross wavelet transform and wavelet coherence (www.pol.ac.uk/home/research/waveletcoherence/)."}
{"bibcode": "2020Radcb..62..725R", "title": "The IntCal20 Northern Hemisphere Radiocarbon Age Calibration Curve (0-55 cal kBP)", "abstract": "Radiocarbon (14C) ages cannot provide absolutely dated chronologies for archaeological or paleoenvironmental studies directly but must be converted to calendar age equivalents using a calibration curve compensating for fluctuations in atmospheric14C concentration. Although calibration curves are constructed from independently dated archives, they invariably require revision as new data become available and our understanding of the Earth system improves. In this volume the international14C calibration curves for both the Northern and Southern Hemispheres, as well as for the ocean surface layer, have been updated to include a wealth of new data and extended to 55,000 cal BP. Based on tree rings, IntCal20 now extends as a fully atmospheric record to ca. 13,900 cal BP. For the older part of the timescale, IntCal20 comprises statistically integrated evidence from floating tree-ring chronologies, lacustrine and marine sediments, speleothems, and corals. We utilized improved evaluation of the timescales and location variable14C offsets from the atmosphere (reservoir age, dead carbon fraction) for each dataset. New statistical methods have refined the structure of the calibration curves while maintaining a robust treatment of uncertainties in the14C ages, the calendar ages and other corrections. The inclusion of modeled marine reservoir ages derived from a three-dimensional ocean circulation model has allowed us to apply more appropriate reservoir corrections to the marine14C data rather than the previous use of constant regional offsets from the atmosphere. Here we provide an overview of the new and revised datasets and the associated methods used for the construction of the IntCal20 curve and explore potential regional offsets for tree-ring data. We discuss the main differences with respect to the previous calibration curve, IntCal13, and some of the implications for archaeology and geosciences ranging from the recent past to the time of the extinction of the Neanderthals.", "database": ["earth science"], "keywords": [], "year": "2020", "doctype": "article", "citation_count": 3443, "domain_category": "earth_science", "abstract_clean": "Radiocarbon (14C) ages cannot provide absolutely dated chronologies for archaeological or paleoenvironmental studies directly but must be converted to calendar age equivalents using a calibration curve compensating for fluctuations in atmospheric14C concentration. Although calibration curves are constructed from independently dated archives, they invariably require revision as new data become available and our understanding of the Earth system improves. In this volume the international14C calibration curves for both the Northern and Southern Hemispheres, as well as for the ocean surface layer, have been updated to include a wealth of new data and extended to 55,000 cal BP. Based on tree rings, IntCal20 now extends as a fully atmospheric record to ca. 13,900 cal BP. For the older part of the timescale, IntCal20 comprises statistically integrated evidence from floating tree-ring chronologies, lacustrine and marine sediments, speleothems, and corals. We utilized improved evaluation of the timescales and location variable14C offsets from the atmosphere (reservoir age, dead carbon fraction) for each dataset. New statistical methods have refined the structure of the calibration curves while maintaining a robust treatment of uncertainties in the14C ages, the calendar ages and other corrections. The inclusion of modeled marine reservoir ages derived from a three-dimensional ocean circulation model has allowed us to apply more appropriate reservoir corrections to the marine14C data rather than the previous use of constant regional offsets from the atmosphere. Here we provide an overview of the new and revised datasets and the associated methods used for the construction of the IntCal20 curve and explore potential regional offsets for tree-ring data. We discuss the main differences with respect to the previous calibration curve, IntCal13, and some of the implications for archaeology and geosciences ranging from the recent past to the time of the extinction of the Neanderthals."}
{"bibcode": "1994RvGeo..32..363L", "title": "Oceanic vertical mixing: A review and a model with a nonlocal boundary layer parameterization", "abstract": "If model parameterizations of unresolved physics, such as the variety of upper ocean mixing processes, are to hold over the large range of time and space scales of importance to climate, they must be strongly physically based. Observations, theories, and models of oceanic vertical mixing are surveyed. Two distinct regimes are identified: ocean mixing in the boundary layer near the surface under a variety of surface forcing conditions (stabilizing, destabilizing, and wind driven), and mixing in the ocean interior due to internal waves, shear instability, and double diffusion (arising from the different molecular diffusion rates of heat and salt). Mixing schemes commonly applied to the upper ocean are shown not to contain some potentially important boundary layer physics. Therefore a new parameterization of oceanic boundary layer mixing is developed to accommodate some of this physics. It includes a scheme for determining the boundary layer depth h, where the turbulent contribution to the vertical shear of a bulk Richardson number is parameterized. Expressions for diffusivity and nonlocal transport throughout the boundary layer are given. The diffusivity is formulated to agree with similarity theory of turbulence in the surface layer and is subject to the conditions that both it and its vertical gradient match the interior values at h. This nonlocal \"K profile parameterization\" (KPP) is then verified and compared to alternatives, including its atmospheric counterparts. Its most important feature is shown to be the capability of the boundary layer to penetrate well into a stable thermocline in both convective and wind-driven situations. The diffusivities of the aforementioned three interior mixing processes are modeled as constants, functions of a gradient Richardson number (a measure of the relative importance of stratification to destabilizing shear), and functions of the double-diffusion density ratio, R<SUB>ρ</SUB>. Oceanic simulations of convective penetration, wind deepening, and diurnal cycling are used to determine appropriate values for various model parameters as weak functions of vertical resolution. Annual cycle simulations at ocean weather station Papa for 1961 and 1969-1974 are used to test the complete suite of parameterizations. Model and observed temperatures at all depths are shown to agree very well into September, after which systematic advective cooling in the ocean produces expected differences. It is argued that this cooling and a steady salt advection into the model are needed to balance the net annual surface heating and freshwater input. With these advections, good multiyear simulations of temperature and salinity can be achieved. These results and KPP simulations of the diurnal cycle at the Long-Term Upper Ocean Study (LOTUS) site are compared with the results of other models. It is demonstrated that the KPP model exchanges properties between the mixed layer and thermocline in a manner consistent with observations, and at least as well or better than alternatives.", "database": ["physics", "earth science"], "keywords": ["Oceanography: Physical: Turbulence", "diffusion", "and mixing processes", "Oceanography: Physical: Air/sea interactions"], "year": "1994", "doctype": "article", "citation_count": 3441, "domain_category": "earth_science", "abstract_clean": "If model parameterizations of unresolved physics, such as the variety of upper ocean mixing processes, are to hold over the large range of time and space scales of importance to climate, they must be strongly physically based. Observations, theories, and models of oceanic vertical mixing are surveyed. Two distinct regimes are identified: ocean mixing in the boundary layer near the surface under a variety of surface forcing conditions (stabilizing, destabilizing, and wind driven), and mixing in the ocean interior due to internal waves, shear instability, and double diffusion (arising from the different molecular diffusion rates of heat and salt). Mixing schemes commonly applied to the upper ocean are shown not to contain some potentially important boundary layer physics. Therefore a new parameterization of oceanic boundary layer mixing is developed to accommodate some of this physics. It includes a scheme for determining the boundary layer depth h, where the turbulent contribution to the vertical shear of a bulk Richardson number is parameterized. Expressions for diffusivity and nonlocal transport throughout the boundary layer are given. The diffusivity is formulated to agree with similarity theory of turbulence in the surface layer and is subject to the conditions that both it and its vertical gradient match the interior values at h. This nonlocal \"K profile parameterization\" (KPP) is then verified and compared to alternatives, including its atmospheric counterparts. Its most important feature is shown to be the capability of the boundary layer to penetrate well into a stable thermocline in both convective and wind-driven situations. The diffusivities of the aforementioned three interior mixing processes are modeled as constants, functions of a gradient Richardson number (a measure of the relative importance of stratification to destabilizing shear), and functions of the double-diffusion density ratio, Rρ. Oceanic simulations of convective penetration, wind deepening, and diurnal cycling are used to determine appropriate values for various model parameters as weak functions of vertical resolution. Annual cycle simulations at ocean weather station Papa for 1961 and 1969-1974 are used to test the complete suite of parameterizations. Model and observed temperatures at all depths are shown to agree very well into September, after which systematic advective cooling in the ocean produces expected differences. It is argued that this cooling and a steady salt advection into the model are needed to balance the net annual surface heating and freshwater input. With these advections, good multiyear simulations of temperature and salinity can be achieved. These results and KPP simulations of the diurnal cycle at the Long-Term Upper Ocean Study (LOTUS) site are compared with the results of other models. It is demonstrated that the KPP model exchanges properties between the mixed layer and thermocline in a manner consistent with observations, and at least as well or better than alternatives."}
{"bibcode": "2011MarPB..62.1596A", "title": "Microplastics in the marine environment", "abstract": "This review discusses the mechanisms of generation and potential impacts of microplastics in the ocean environment. Weathering degradation of plastics on the beaches results in their surface embrittlement and microcracking, yielding microparticles that are carried into water by wind or wave action. Unlike inorganic fines present in sea water, microplastics concentrate persistent organic pollutants (POPs) by partition. The relevant distribution coefficients for common POPs are several orders of magnitude in favour of the plastic medium. Consequently, the microparticles laden with high levels of POPs can be ingested by marine biota. Bioavailability and the efficiency of transfer of the ingested POPs across trophic levels are not known and the potential damage posed by these to the marine ecosystem has yet to be quantified and modelled. Given the increasing levels of plastic pollution of the oceans it is important to better understand the impact of microplastics in the ocean food web.", "database": ["earth science"], "keywords": ["Microplastics", "Nanoplastics", "POPs", "Plastics", "Food web"], "year": "2011", "doctype": "article", "citation_count": 3439, "domain_category": "earth_science", "abstract_clean": "This review discusses the mechanisms of generation and potential impacts of microplastics in the ocean environment. Weathering degradation of plastics on the beaches results in their surface embrittlement and microcracking, yielding microparticles that are carried into water by wind or wave action. Unlike inorganic fines present in sea water, microplastics concentrate persistent organic pollutants (POPs) by partition. The relevant distribution coefficients for common POPs are several orders of magnitude in favour of the plastic medium. Consequently, the microparticles laden with high levels of POPs can be ingested by marine biota. Bioavailability and the efficiency of transfer of the ingested POPs across trophic levels are not known and the potential damage posed by these to the marine ecosystem has yet to be quantified and modelled. Given the increasing levels of plastic pollution of the oceans it is important to better understand the impact of microplastics in the ocean food web."}
{"bibcode": "1980QJRMS.106...85W", "title": "Correction of flux measurements for density effects due to heat and water vapour transfer", "abstract": "When the atmospheric turbulent flux of a minor constituent such as CO2 (or of water vapour as a special case) is measured by either the eddy covariance or the mean gradient technique, account may need to be taken of variations of the constituent's density due to the presence of a flux of heat and/or water vapour. In this paper the basic relationships are discussed in the context of vertical transfer in the lower atmosphere, and the required corrections to the measured flux are derived. If the measurement involves sensing of the fluctuations or mean gradient of the constituent's mixing ratio relative to the dry air component, then no correction is required; while with sensing of the constituent's specific mass content relative to the total moist air, a correction arising from the water vapour flux only is required. Correspondingly, if in mean gradient measurements the constituent's density is measured in air from different heights which has been pre-dried and brought to a common temperature, then again no correction is required; while if the original (moist) air itself is brought to a common temperature, then only a correction arising from the water vapour flux is required. If the constituent's density fluctuations or mean gradients are measured directly in the air in situ, then corrections arising from both heat and water vapour fluxes are required. These corrections will often be very important. That due to the heat flux is about five times as great as that due to an equal latent heat (water vapour) flux. In CO2 flux measurements the magnitude of the correction will commonly exceed that of the flux itself. The correction to measurements of water vapour flux will often be only a few per cent but will sometimes exceed 10 per cent.", "database": ["physics", "earth science"], "keywords": [], "year": "1980", "doctype": "article", "citation_count": 3415, "domain_category": "earth_science", "abstract_clean": "When the atmospheric turbulent flux of a minor constituent such as CO2 (or of water vapour as a special case) is measured by either the eddy covariance or the mean gradient technique, account may need to be taken of variations of the constituent's density due to the presence of a flux of heat and/or water vapour. In this paper the basic relationships are discussed in the context of vertical transfer in the lower atmosphere, and the required corrections to the measured flux are derived. If the measurement involves sensing of the fluctuations or mean gradient of the constituent's mixing ratio relative to the dry air component, then no correction is required; while with sensing of the constituent's specific mass content relative to the total moist air, a correction arising from the water vapour flux only is required. Correspondingly, if in mean gradient measurements the constituent's density is measured in air from different heights which has been pre-dried and brought to a common temperature, then again no correction is required; while if the original (moist) air itself is brought to a common temperature, then only a correction arising from the water vapour flux is required. If the constituent's density fluctuations or mean gradients are measured directly in the air in situ, then corrections arising from both heat and water vapour fluxes are required. These corrections will often be very important. That due to the heat flux is about five times as great as that due to an equal latent heat (water vapour) flux. In CO2 flux measurements the magnitude of the correction will commonly exceed that of the flux itself. The correction to measurements of water vapour flux will often be only a few per cent but will sometimes exceed 10 per cent."}
{"bibcode": "1992JGR....97.7373W", "title": "Relationship Between Wind Speed and Gas Exchange Over the Ocean", "abstract": "Relationships between wind speed and gas transfer, combined with knowledge of the partial pressure difference of CO<SUB>2</SUB> across the air-sea interface are frequently used to determine the CO<SUB>2</SUB> flux between the ocean and the atmosphere. Little attention has been paid to the influence of variability in wind speed on the calculated gas transfer velocities and the possibility of chemical enhancement of CO<SUB>2</SUB> exchange at low wind speeds over the ocean. The effect of these parameters is illustrated using a quadratic dependence of gas exchange on wind speed which is fit through gas transfer velocities over the ocean determined by the natural-<SUP>14</SUP>C disequilibrium and the bomb-<SUP>14</SUP>C inventory methods. Some of the variability between different data sets can be accounted for by the suggested mechanisms, but much of the variation appears due to other causes. Possible causes for the large difference between two frequently used relationships between gas transfer and wind speed are discussed. To determine fluxes of gases other than CO<SUB>2</SUB> across the air-water interface, the relevant expressions for gas transfer, and the temperature and salinity dependence of the Schmidt number and solubility of several gases of environmental interest are included in an appendix.", "database": ["physics", "earth science"], "keywords": ["Oceanography: Physical: Air/sea interactions", "Oceanography: Biological and Chemical: General or miscellaneous"], "year": "1992", "doctype": "article", "citation_count": 3397, "domain_category": "earth_science", "abstract_clean": "Relationships between wind speed and gas transfer, combined with knowledge of the partial pressure difference of CO2 across the air-sea interface are frequently used to determine the CO2 flux between the ocean and the atmosphere. Little attention has been paid to the influence of variability in wind speed on the calculated gas transfer velocities and the possibility of chemical enhancement of CO2 exchange at low wind speeds over the ocean. The effect of these parameters is illustrated using a quadratic dependence of gas exchange on wind speed which is fit through gas transfer velocities over the ocean determined by the natural-14C disequilibrium and the bomb-14C inventory methods. Some of the variability between different data sets can be accounted for by the suggested mechanisms, but much of the variation appears due to other causes. Possible causes for the large difference between two frequently used relationships between gas transfer and wind speed are discussed. To determine fluxes of gases other than CO2 across the air-water interface, the relevant expressions for gas transfer, and the temperature and salinity dependence of the Schmidt number and solubility of several gases of environmental interest are included in an appendix."}
{"bibcode": "1997BAMS...78.2539X", "title": "Global Precipitation: A 17-Year Monthly Analysis Based on Gauge Observations, Satellite Estimates, and Numerical Model Outputs.", "abstract": "Gridded fields (analyses) of global monthly precipitation have been constructed on a 2.5° latitude-longitude grid for the 17-yr period from 1979 to 1995 by merging several kinds of information sources with different characteristics, including gauge observations, estimates inferred from a variety of satellite observations, and the NCEP-NCAR reanalysis. This new dataset, which the authors have named the CPC Merged Analysis of Precipitation (CMAP), contains precipitation distributions with full global coverage and improved quality compared to the individual data sources. Examinations showed no discontinuity during the 17-yr period, despite the different data sources used for the different subperiods. Comparisons of the CMAP with the merged analysis of Huffman et al. revealed remarkable agreements over the global land areas and over tropical and subtropical oceanic areas, with differences observed over extratropical oceanic areas. The 17-yr CMAP dataset is used to investigate the annual and interannual variability in large-scale precipitation. The mean distribution and the annual cycle in the 17-yr dataset exhibit reasonable agreement with existing long-term means except over the eastern tropical Pacific. The interannual variability associated with the El Niño-Southern Oscillation phenomenon resembles that found in previous studies, but with substantial additional details, particularly over the oceans. With complete global coverage, extended period and improved quality, the 17-yr dataset of the CMAP provides very useful information for climate analysis, numerical model validation, hydrological research, and many other applications. Further work is under way to improve the quality, extend the temporal coverage, and to refine the resolution of the merged analysis.", "database": ["physics", "earth science"], "keywords": [], "year": "1997", "doctype": "article", "citation_count": 3372, "domain_category": "earth_science", "abstract_clean": "Gridded fields (analyses) of global monthly precipitation have been constructed on a 2.5° latitude-longitude grid for the 17-yr period from 1979 to 1995 by merging several kinds of information sources with different characteristics, including gauge observations, estimates inferred from a variety of satellite observations, and the NCEP-NCAR reanalysis. This new dataset, which the authors have named the CPC Merged Analysis of Precipitation (CMAP), contains precipitation distributions with full global coverage and improved quality compared to the individual data sources. Examinations showed no discontinuity during the 17-yr period, despite the different data sources used for the different subperiods. Comparisons of the CMAP with the merged analysis of Huffman et al. revealed remarkable agreements over the global land areas and over tropical and subtropical oceanic areas, with differences observed over extratropical oceanic areas. The 17-yr CMAP dataset is used to investigate the annual and interannual variability in large-scale precipitation. The mean distribution and the annual cycle in the 17-yr dataset exhibit reasonable agreement with existing long-term means except over the eastern tropical Pacific. The interannual variability associated with the El Niño-Southern Oscillation phenomenon resembles that found in previous studies, but with substantial additional details, particularly over the oceans. With complete global coverage, extended period and improved quality, the 17-yr dataset of the CMAP provides very useful information for climate analysis, numerical model validation, hydrological research, and many other applications. Further work is under way to improve the quality, extend the temporal coverage, and to refine the resolution of the merged analysis."}
{"bibcode": "2010JHyd..391..202M", "title": "A review of drought concepts", "abstract": "SummaryOwing to the rise in water demand and looming climate change, recent years have witnessed much focus on global drought scenarios. As a natural hazard, drought is best characterized by multiple climatological and hydrological parameters. An understanding of the relationships between these two sets of parameters is necessary to develop measures for mitigating the impacts of droughts. Beginning with a discussion of drought definitions, this paper attempts to provide a review of fundamental concepts of drought, classification of droughts, drought indices, historical droughts using paleoclimatic studies, and the relation between droughts and large scale climate indices. Conclusions are drawn where gaps exist and more research needs to be focussed.", "database": ["physics", "earth science"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 3343, "domain_category": "earth_science", "abstract_clean": "SummaryOwing to the rise in water demand and looming climate change, recent years have witnessed much focus on global drought scenarios. As a natural hazard, drought is best characterized by multiple climatological and hydrological parameters. An understanding of the relationships between these two sets of parameters is necessary to develop measures for mitigating the impacts of droughts. Beginning with a discussion of drought definitions, this paper attempts to provide a review of fundamental concepts of drought, classification of droughts, drought indices, historical droughts using paleoclimatic studies, and the relation between droughts and large scale climate indices. Conclusions are drawn where gaps exist and more research needs to be focussed."}
{"bibcode": "2005EcolL...8..993G", "title": "Predicting species distribution: offering more than simple habitat models", "abstract": "In the last two decades, interest in species distribution models (SDMs) of plants and animals has grown dramatically. Recent advances in SDMs allow us to potentially forecast anthropogenic effects on patterns of biodiversity at different spatial scales. However, some limitations still preclude the use of SDMs in many theoretical and practical applications. Here, we provide an overview of recent advances in this field, discuss the ecological principles and assumptions underpinning SDMs, and highlight critical limitations and decisions inherent in the construction and evaluation of SDMs. Particular emphasis is given to the use of SDMs for the assessment of climate change impacts and conservation management issues. We suggest new avenues for incorporating species migration, population dynamics, biotic interactions and community ecology into SDMs at multiple spatial scales. Addressing all these issues requires a better integration of SDMs with ecological theory.", "database": ["earth science"], "keywords": ["Dispersal", "ecological niche theory", "future projections", "habitat suitability maps", "population dynamics", "prediction errors", "predictive biogeography", "spatial scales", "species distribution models"], "year": "2005", "doctype": "article", "citation_count": 3340, "domain_category": "earth_science", "abstract_clean": "In the last two decades, interest in species distribution models (SDMs) of plants and animals has grown dramatically. Recent advances in SDMs allow us to potentially forecast anthropogenic effects on patterns of biodiversity at different spatial scales. However, some limitations still preclude the use of SDMs in many theoretical and practical applications. Here, we provide an overview of recent advances in this field, discuss the ecological principles and assumptions underpinning SDMs, and highlight critical limitations and decisions inherent in the construction and evaluation of SDMs. Particular emphasis is given to the use of SDMs for the assessment of climate change impacts and conservation management issues. We suggest new avenues for incorporating species migration, population dynamics, biotic interactions and community ecology into SDMs at multiple spatial scales. Addressing all these issues requires a better integration of SDMs with ecological theory."}
{"bibcode": "2014Natur.514..218H", "title": "High secondary aerosol contribution to particulate pollution during haze events in China", "abstract": "Rapid industrialization and urbanization in developing countries has led to an increase in air pollution, along a similar trajectory to that previously experienced by the developed nations. In China, particulate pollution is a serious environmental problem that is influencing air quality, regional and global climates, and human health. In response to the extremely severe and persistent haze pollution experienced by about 800 million people during the first quarter of 2013 (refs 4, 5), the Chinese State Council announced its aim to reduce concentrations of PM<SUB>2.5</SUB> (particulate matter with an aerodynamic diameter less than 2.5 micrometres) by up to 25 per cent relative to 2012 levels by 2017 (ref. 6). Such efforts however require elucidation of the factors governing the abundance and composition of PM<SUB>2.5</SUB>, which remain poorly constrained in China. Here we combine a comprehensive set of novel and state-of-the-art offline analytical approaches and statistical techniques to investigate the chemical nature and sources of particulate matter at urban locations in Beijing, Shanghai, Guangzhou and Xi'an during January 2013. We find that the severe haze pollution event was driven to a large extent by secondary aerosol formation, which contributed 30-77 per cent and 44-71 per cent (average for all four cities) of PM<SUB>2.5</SUB> and of organic aerosol, respectively. On average, the contribution of secondary organic aerosol (SOA) and secondary inorganic aerosol (SIA) are found to be of similar importance (SOA/SIA ratios range from 0.6 to 1.4). Our results suggest that, in addition to mitigating primary particulate emissions, reducing the emissions of secondary aerosol precursors from, for example, fossil fuel combustion and biomass burning is likely to be important for controlling China's PM<SUB>2.5</SUB> levels and for reducing the environmental, economic and health impacts resulting from particulate pollution.", "database": ["physics", "earth science"], "keywords": [], "year": "2014", "doctype": "article", "citation_count": 3318, "domain_category": "earth_science", "abstract_clean": "Rapid industrialization and urbanization in developing countries has led to an increase in air pollution, along a similar trajectory to that previously experienced by the developed nations. In China, particulate pollution is a serious environmental problem that is influencing air quality, regional and global climates, and human health. In response to the extremely severe and persistent haze pollution experienced by about 800 million people during the first quarter of 2013 (refs 4, 5), the Chinese State Council announced its aim to reduce concentrations of PM2.5 (particulate matter with an aerodynamic diameter less than 2.5 micrometres) by up to 25 per cent relative to 2012 levels by 2017 (ref. 6). Such efforts however require elucidation of the factors governing the abundance and composition of PM2.5, which remain poorly constrained in China. Here we combine a comprehensive set of novel and state-of-the-art offline analytical approaches and statistical techniques to investigate the chemical nature and sources of particulate matter at urban locations in Beijing, Shanghai, Guangzhou and Xi'an during January 2013. We find that the severe haze pollution event was driven to a large extent by secondary aerosol formation, which contributed 30-77 per cent and 44-71 per cent (average for all four cities) of PM2.5 and of organic aerosol, respectively. On average, the contribution of secondary organic aerosol (SOA) and secondary inorganic aerosol (SIA) are found to be of similar importance (SOA/SIA ratios range from 0.6 to 1.4). Our results suggest that, in addition to mitigating primary particulate emissions, reducing the emissions of secondary aerosol precursors from, for example, fossil fuel combustion and biomass burning is likely to be important for controlling China's PM2.5 levels and for reducing the environmental, economic and health impacts resulting from particulate pollution."}
{"bibcode": "2016Natur.540..418P", "title": "High-resolution mapping of global surface water and its long-term changes", "abstract": "The location and persistence of surface water (inland and coastal) is both affected by climate and human activity and affects climate, biological diversity and human wellbeing. Global data sets documenting surface water location and seasonality have been produced from inventories and national descriptions, statistical extrapolation of regional data and satellite imagery, but measuring long-term changes at high resolution remains a challenge. Here, using three million Landsat satellite images, we quantify changes in global surface water over the past 32 years at 30-metre resolution. We record the months and years when water was present, where occurrence changed and what form changes took in terms of seasonality and persistence. Between 1984 and 2015 permanent surface water has disappeared from an area of almost 90,000 square kilometres, roughly equivalent to that of Lake Superior, though new permanent bodies of surface water covering 184,000 square kilometres have formed elsewhere. All continental regions show a net increase in permanent water, except Oceania, which has a fractional (one per cent) net loss. Much of the increase is from reservoir filling, although climate change is also implicated. Loss is more geographically concentrated than gain. Over 70 per cent of global net permanent water loss occurred in the Middle East and Central Asia, linked to drought and human actions including river diversion or damming and unregulated withdrawal. Losses in Australia and the USA linked to long-term droughts are also evident. This globally consistent, validated data set shows that impacts of climate change and climate oscillations on surface water occurrence can be measured and that evidence can be gathered to show how surface water is altered by human activities. We anticipate that this freely available data will improve the modelling of surface forcing, provide evidence of state and change in wetland ecotones (the transition areas between biomes), and inform water-management decision-making.", "database": ["physics", "earth science"], "keywords": [], "year": "2016", "doctype": "article", "citation_count": 3250, "domain_category": "earth_science", "abstract_clean": "The location and persistence of surface water (inland and coastal) is both affected by climate and human activity and affects climate, biological diversity and human wellbeing. Global data sets documenting surface water location and seasonality have been produced from inventories and national descriptions, statistical extrapolation of regional data and satellite imagery, but measuring long-term changes at high resolution remains a challenge. Here, using three million Landsat satellite images, we quantify changes in global surface water over the past 32 years at 30-metre resolution. We record the months and years when water was present, where occurrence changed and what form changes took in terms of seasonality and persistence. Between 1984 and 2015 permanent surface water has disappeared from an area of almost 90,000 square kilometres, roughly equivalent to that of Lake Superior, though new permanent bodies of surface water covering 184,000 square kilometres have formed elsewhere. All continental regions show a net increase in permanent water, except Oceania, which has a fractional (one per cent) net loss. Much of the increase is from reservoir filling, although climate change is also implicated. Loss is more geographically concentrated than gain. Over 70 per cent of global net permanent water loss occurred in the Middle East and Central Asia, linked to drought and human actions including river diversion or damming and unregulated withdrawal. Losses in Australia and the USA linked to long-term droughts are also evident. This globally consistent, validated data set shows that impacts of climate change and climate oscillations on surface water occurrence can be measured and that evidence can be gathered to show how surface water is altered by human activities. We anticipate that this freely available data will improve the modelling of surface forcing, provide evidence of state and change in wetland ecotones (the transition areas between biomes), and inform water-management decision-making."}
{"bibcode": "2016GMD.....9.3461O", "title": "The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6", "abstract": "Projections of future climate change play a fundamental role in improving understanding of the climate system as well as characterizing societal risks and response options. The Scenario Model Intercomparison Project (ScenarioMIP) is the primary activity within Phase 6 of the Coupled Model Intercomparison Project (CMIP6) that will provide multi-model climate projections based on alternative scenarios of future emissions and land use changes produced with integrated assessment models. In this paper, we describe ScenarioMIP's objectives, experimental design, and its relation to other activities within CMIP6. The ScenarioMIP design is one component of a larger scenario process that aims to facilitate a wide range of integrated studies across the climate science, integrated assessment modeling, and impacts, adaptation, and vulnerability communities, and will form an important part of the evidence base in the forthcoming Intergovernmental Panel on Climate Change (IPCC) assessments. At the same time, it will provide the basis for investigating a number of targeted science and policy questions that are especially relevant to scenario-based analysis, including the role of specific forcings such as land use and aerosols, the effect of a peak and decline in forcing, the consequences of scenarios that limit warming to below 2 °C, the relative contributions to uncertainty from scenarios, climate models, and internal variability, and long-term climate system outcomes beyond the 21st century. To serve this wide range of scientific communities and address these questions, a design has been identified consisting of eight alternative 21st century scenarios plus one large initial condition ensemble and a set of long-term extensions, divided into two tiers defined by relative priority. Some of these scenarios will also provide a basis for variants planned to be run in other CMIP6-Endorsed MIPs to investigate questions related to specific forcings. Harmonized, spatially explicit emissions and land use scenarios generated with integrated assessment models will be provided to participating climate modeling groups by late 2016, with the climate model simulations run within the 2017-2018 time frame, and output from the climate model projections made available and analyses performed over the 2018-2020 period.", "database": ["physics", "earth science"], "keywords": [], "year": "2016", "doctype": "article", "citation_count": 3236, "domain_category": "earth_science", "abstract_clean": "Projections of future climate change play a fundamental role in improving understanding of the climate system as well as characterizing societal risks and response options. The Scenario Model Intercomparison Project (ScenarioMIP) is the primary activity within Phase 6 of the Coupled Model Intercomparison Project (CMIP6) that will provide multi-model climate projections based on alternative scenarios of future emissions and land use changes produced with integrated assessment models. In this paper, we describe ScenarioMIP's objectives, experimental design, and its relation to other activities within CMIP6. The ScenarioMIP design is one component of a larger scenario process that aims to facilitate a wide range of integrated studies across the climate science, integrated assessment modeling, and impacts, adaptation, and vulnerability communities, and will form an important part of the evidence base in the forthcoming Intergovernmental Panel on Climate Change (IPCC) assessments. At the same time, it will provide the basis for investigating a number of targeted science and policy questions that are especially relevant to scenario-based analysis, including the role of specific forcings such as land use and aerosols, the effect of a peak and decline in forcing, the consequences of scenarios that limit warming to below 2 °C, the relative contributions to uncertainty from scenarios, climate models, and internal variability, and long-term climate system outcomes beyond the 21st century. To serve this wide range of scientific communities and address these questions, a design has been identified consisting of eight alternative 21st century scenarios plus one large initial condition ensemble and a set of long-term extensions, divided into two tiers defined by relative priority. Some of these scenarios will also provide a basis for variants planned to be run in other CMIP6-Endorsed MIPs to investigate questions related to specific forcings. Harmonized, spatially explicit emissions and land use scenarios generated with integrated assessment models will be provided to participating climate modeling groups by late 2016, with the climate model simulations run within the 2017-2018 time frame, and output from the climate model projections made available and analyses performed over the 2018-2020 period."}
{"bibcode": "1991QSRv...10..297B", "title": "Insolation values for the climate of the last 10 million years", "abstract": "New values for the astronomical parameters of the Earth's orbit and rotation (eccentricity, obliquity and precession) are proposed for paleoclimatic research related to the Late Miocene, the Pliocene and the Quaternary. They have been obtained from a numerical solution of the Lagrangian system of the planetary point masses and from an analytical solution of the Poisson equations of the Earth-Moon system. The analytical expansion developed in this paper allows the direct determination of the main frequencies with their phase and amplitude. Numerical and analytical comparisons with the former astronomical solution BER78 are performed so that the accuracy and the interval of time over which the new solution is valid can be estimated. The corresponding insolation values have also been computed and compared to the former ones. This analysis leads to the conclusion that the new values are expected to be reliable over the last 5 Ma in the time domain and at least over the last 10 Ma in the frequency domain.", "database": ["physics", "earth science"], "keywords": [], "year": "1991", "doctype": "article", "citation_count": 3228, "domain_category": "earth_science", "abstract_clean": "New values for the astronomical parameters of the Earth's orbit and rotation (eccentricity, obliquity and precession) are proposed for paleoclimatic research related to the Late Miocene, the Pliocene and the Quaternary. They have been obtained from a numerical solution of the Lagrangian system of the planetary point masses and from an analytical solution of the Poisson equations of the Earth-Moon system. The analytical expansion developed in this paper allows the direct determination of the main frequencies with their phase and amplitude. Numerical and analytical comparisons with the former astronomical solution BER78 are performed so that the accuracy and the interval of time over which the new solution is valid can be estimated. The corresponding insolation values have also been computed and compared to the former ones. This analysis leads to the conclusion that the new values are expected to be reliable over the last 5 Ma in the time domain and at least over the last 10 Ma in the frequency domain."}
{"bibcode": "2006ACP.....6.3181G", "title": "Estimates of global terrestrial isoprene emissions using MEGAN (Model of Emissions of Gases and Aerosols from Nature)", "abstract": "Reactive gases and aerosols are produced by terrestrial ecosystems, processed within plant canopies, and can then be emitted into the above-canopy atmosphere. Estimates of the above-canopy fluxes are needed for quantitative earth system studies and assessments of past, present and future air quality and climate. The Model of Emissions of Gases and Aerosols from Nature (MEGAN) is described and used to quantify net terrestrial biosphere emission of isoprene into the atmosphere. MEGAN is designed for both global and regional emission modeling and has global coverage with ~1 km<SUP>2</SUP> spatial resolution. Field and laboratory investigations of the processes controlling isoprene emission are described and data available for model development and evaluation are summarized. The factors controlling isoprene emissions include biological, physical and chemical driving variables. MEGAN driving variables are derived from models and satellite and ground observations. Tropical broadleaf trees contribute almost half of the estimated global annual isoprene emission due to their relatively high emission factors and because they are often exposed to conditions that are conducive for isoprene emission. The remaining flux is primarily from shrubs which have a widespread distribution. The annual global isoprene emission estimated with MEGAN ranges from about 500 to 750 Tg isoprene (440 to 660 Tg carbon) depending on the driving variables which include temperature, solar radiation, Leaf Area Index, and plant functional type. The global annual isoprene emission estimated using the standard driving variables is ~600 Tg isoprene. Differences in driving variables result in emission estimates that differ by more than a factor of three for specific times and locations. It is difficult to evaluate isoprene emission estimates using the concentration distributions simulated using chemistry and transport models, due to the substantial uncertainties in other model components, but at least some global models produce reasonable results when using isoprene emission distributions similar to MEGAN estimates. In addition, comparison with isoprene emissions estimated from satellite formaldehyde observations indicates reasonable agreement. The sensitivity of isoprene emissions to earth system changes (e.g., climate and land-use) demonstrates the potential for large future changes in emissions. Using temperature distributions simulated by global climate models for year 2100, MEGAN estimates that isoprene emissions increase by more than a factor of two. This is considerably greater than previous estimates and additional observations are needed to evaluate and improve the methods used to predict future isoprene emissions.", "database": ["physics", "earth science"], "keywords": [], "year": "2006", "doctype": "article", "citation_count": 3212, "domain_category": "earth_science", "abstract_clean": "Reactive gases and aerosols are produced by terrestrial ecosystems, processed within plant canopies, and can then be emitted into the above-canopy atmosphere. Estimates of the above-canopy fluxes are needed for quantitative earth system studies and assessments of past, present and future air quality and climate. The Model of Emissions of Gases and Aerosols from Nature (MEGAN) is described and used to quantify net terrestrial biosphere emission of isoprene into the atmosphere. MEGAN is designed for both global and regional emission modeling and has global coverage with ~1 km2 spatial resolution. Field and laboratory investigations of the processes controlling isoprene emission are described and data available for model development and evaluation are summarized. The factors controlling isoprene emissions include biological, physical and chemical driving variables. MEGAN driving variables are derived from models and satellite and ground observations. Tropical broadleaf trees contribute almost half of the estimated global annual isoprene emission due to their relatively high emission factors and because they are often exposed to conditions that are conducive for isoprene emission. The remaining flux is primarily from shrubs which have a widespread distribution. The annual global isoprene emission estimated with MEGAN ranges from about 500 to 750 Tg isoprene (440 to 660 Tg carbon) depending on the driving variables which include temperature, solar radiation, Leaf Area Index, and plant functional type. The global annual isoprene emission estimated using the standard driving variables is ~600 Tg isoprene. Differences in driving variables result in emission estimates that differ by more than a factor of three for specific times and locations. It is difficult to evaluate isoprene emission estimates using the concentration distributions simulated using chemistry and transport models, due to the substantial uncertainties in other model components, but at least some global models produce reasonable results when using isoprene emission distributions similar to MEGAN estimates. In addition, comparison with isoprene emissions estimated from satellite formaldehyde observations indicates reasonable agreement. The sensitivity of isoprene emissions to earth system changes (e.g., climate and land-use) demonstrates the potential for large future changes in emissions. Using temperature distributions simulated by global climate models for year 2100, MEGAN estimates that isoprene emissions increase by more than a factor of two. This is considerably greater than previous estimates and additional observations are needed to evaluate and improve the methods used to predict future isoprene emissions."}
{"bibcode": "2018NatSD...580214B", "title": "Present and future Köppen-Geiger climate classification maps at 1-km resolution", "abstract": "We present new global maps of the Köppen-Geiger climate classification at an unprecedented 1-km resolution for the present-day (1980-2016) and for projected future conditions (2071-2100) under climate change. The present-day map is derived from an ensemble of four high-resolution, topographically-corrected climatic maps. The future map is derived from an ensemble of 32 climate model projections (scenario RCP8.5), by superimposing the projected climate change anomaly on the baseline high-resolution climatic maps. For both time periods we calculate confidence levels from the ensemble spread, providing valuable indications of the reliability of the classifications. The new maps exhibit a higher classification accuracy and substantially more detail than previous maps, particularly in regions with sharp spatial or elevation gradients. We anticipate the new maps will be useful for numerous applications, including species and vegetation distribution modeling. The new maps including the associated confidence maps are freely available via www.gloh2o.org/koppen.", "database": ["physics", "earth science"], "keywords": [], "year": "2018", "doctype": "article", "citation_count": 3210, "domain_category": "earth_science", "abstract_clean": "We present new global maps of the Köppen-Geiger climate classification at an unprecedented 1-km resolution for the present-day (1980-2016) and for projected future conditions (2071-2100) under climate change. The present-day map is derived from an ensemble of four high-resolution, topographically-corrected climatic maps. The future map is derived from an ensemble of 32 climate model projections (scenario RCP8.5), by superimposing the projected climate change anomaly on the baseline high-resolution climatic maps. For both time periods we calculate confidence levels from the ensemble spread, providing valuable indications of the reliability of the classifications. The new maps exhibit a higher classification accuracy and substantially more detail than previous maps, particularly in regions with sharp spatial or elevation gradients. We anticipate the new maps will be useful for numerous applications, including species and vegetation distribution modeling. The new maps including the associated confidence maps are freely available via www.gloh2o.org/koppen."}
{"bibcode": "1970JGR....75.4997B", "title": "Tectonic stress and the spectra of seismic shear waves from earthquakes", "abstract": "An earthquake model is derived by considering the effective stress available to accelerate the sides of the fault. The model describes near- and far-field displacement-time functions and spectra and includes the effect of fractional stress drop. It successfully explains the near- and far-field spectra observed for earthquakes and indicates that effective stresses are of the order of 100 bars. For this stress, the estimated upper limit of near-fault particle velocity is 100 cm/sec, and the estimated upper limit for accelerations is approximately 2g at 10 Hz and proportionally lower for lower frequencies. The near field displacement u is approximately given by u(t) = (σ/μ) βr(1 - e<SUP>-t/r</SUP>) where. σ is the effective stress, μ is the rigidity, β is the shear wave velocity, and τ is of the order of the dimension of the fault divided by the shear-wave velocity. The corresponding spectrum isΩ(ω)=σβμ1ω(ω2+τ-2)1/2The rms average far-field spectrum is given by&lt;Ω(ω)&gt;=&lt;Rθϕ&gt;σβμrRF(ɛ)1ω2+α2where &lt;R<SUB>θϕ</SUB>&gt; is the rms average of the radiation pattern; r is the radius of an equivalent circular dislocation surface; R is the distance; F(ɛ) = {[2 - 2ɛ][1 - cos (1.21 ɛω/α)] +ɛ<SUP>2</SUP>}<SUP>1/2</SUP> ɛ is the fraction of stress drop; and α = 2.21 β/r. The rms spectrum falls off as (ω/α)<SUP>-2</SUP> at very high frequencies. For values of ω/α between 1 and 10 the rms spectrum falls off as (ω/α)<SUP>-1</SUP> for ɛ &lt; ∼0.1. At low frequencies the spectrum reduces to the spectrum for a double-couple point source of appropriate moment. Effective stress, stress drop and source dimensions may be estimated by comparing observed seismic spectra with the theoretical spectra.", "database": ["physics", "earth science"], "keywords": ["Seismology: Seismic sources (mechanisms", "magnitude", "frequency spectrum", "space and time distribution)", "Seismology: Strong motions and shock waves"], "year": "1970", "doctype": "article", "citation_count": 3184, "domain_category": "earth_science", "abstract_clean": "An earthquake model is derived by considering the effective stress available to accelerate the sides of the fault. The model describes near- and far-field displacement-time functions and spectra and includes the effect of fractional stress drop. It successfully explains the near- and far-field spectra observed for earthquakes and indicates that effective stresses are of the order of 100 bars. For this stress, the estimated upper limit of near-fault particle velocity is 100 cm/sec, and the estimated upper limit for accelerations is approximately 2g at 10 Hz and proportionally lower for lower frequencies. The near field displacement u is approximately given by u(t) = (σ/μ) βr(1 - e-t/r) where. σ is the effective stress, μ is the rigidity, β is the shear wave velocity, and τ is of the order of the dimension of the fault divided by the shear-wave velocity. The corresponding spectrum isΩ(ω)=σβμ1ω(ω2+τ-2)1/2The rms average far-field spectrum is given by<Ω(ω)>=<Rθϕ>σβμrRF(ɛ)1ω2+α2where <Rθϕ> is the rms average of the radiation pattern; r is the radius of an equivalent circular dislocation surface; R is the distance; F(ɛ) = {[2 - 2ɛ][1 - cos (1.21 ɛω/α)] +ɛ2}1/2 ɛ is the fraction of stress drop; and α = 2.21 β/r. The rms spectrum falls off as (ω/α)-2 at very high frequencies. For values of ω/α between 1 and 10 the rms spectrum falls off as (ω/α)-1 for ɛ < ∼0.1. At low frequencies the spectrum reduces to the spectrum for a double-couple point source of appropriate moment. Effective stress, stress drop and source dimensions may be estimated by comparing observed seismic spectra with the theoretical spectra."}
{"bibcode": "2015NatSD...250066F", "title": "The climate hazards infrared precipitation with stations—a new environmental record for monitoring extremes", "abstract": "The Climate Hazards group Infrared Precipitation with Stations (CHIRPS) dataset builds on previous approaches to ‘smart’ interpolation techniques and high resolution, long period of record precipitation estimates based on infrared Cold Cloud Duration (CCD) observations. The algorithm i) is built around a 0.05° climatology that incorporates satellite information to represent sparsely gauged locations, ii) incorporates daily, pentadal, and monthly 1981-present 0.05° CCD-based precipitation estimates, iii) blends station data to produce a preliminary information product with a latency of about 2 days and a final product with an average latency of about 3 weeks, and iv) uses a novel blending procedure incorporating the spatial correlation structure of CCD-estimates to assign interpolation weights. We present the CHIRPS algorithm, global and regional validation results, and show how CHIRPS can be used to quantify the hydrologic impacts of decreasing precipitation and rising air temperatures in the Greater Horn of Africa. Using the Variable Infiltration Capacity model, we show that CHIRPS can support effective hydrologic forecasts and trend analyses in southeastern Ethiopia.", "database": ["physics", "earth science"], "keywords": [], "year": "2015", "doctype": "article", "citation_count": 3181, "domain_category": "earth_science", "abstract_clean": "The Climate Hazards group Infrared Precipitation with Stations (CHIRPS) dataset builds on previous approaches to ‘smart’ interpolation techniques and high resolution, long period of record precipitation estimates based on infrared Cold Cloud Duration (CCD) observations. The algorithm i) is built around a 0.05° climatology that incorporates satellite information to represent sparsely gauged locations, ii) incorporates daily, pentadal, and monthly 1981-present 0.05° CCD-based precipitation estimates, iii) blends station data to produce a preliminary information product with a latency of about 2 days and a final product with an average latency of about 3 weeks, and iv) uses a novel blending procedure incorporating the spatial correlation structure of CCD-estimates to assign interpolation weights. We present the CHIRPS algorithm, global and regional validation results, and show how CHIRPS can be used to quantify the hydrologic impacts of decreasing precipitation and rising air temperatures in the Greater Horn of Africa. Using the Variable Infiltration Capacity model, we show that CHIRPS can support effective hydrologic forecasts and trend analyses in southeastern Ethiopia."}
{"bibcode": "2002ITGRS..40.2375B", "title": "A New Algorithm for Surface Deformation Monitoring Based on Small Baseline Differential SAR Interferograms", "abstract": "We present a new differential synthetic aperture radar (SAR) interferometry algorithm for monitoring the temporal evolution of surface deformations. The presented technique is based on an appropriate combination of differential interferograms produced by data pairs characterized by a small orbital separation (baseline) in order to limit the spatial decorrelation phenomena. The application of the singular value decomposition method allows us to easily \"link\" independent SAR acquisition datasets, separated by large baselines, thus increasing the observation temporal sampling rate. The availability of both spatial and temporal information in the processed data is used to identify and filter out atmospheric phase artifacts. We present results obtained on the data acquired from 1992 to 2000 by the European Remote Sensing satellites and relative to the Campi Flegrei caldera and to the city of Naples, Italy that demonstrate the capability of the proposed approach to follow the dynamics of the detected deformations.", "database": ["physics", "earth science"], "keywords": ["Ground deformations", "SAR interferometry", "synthetic aperture radar (SAR)"], "year": "2002", "doctype": "article", "citation_count": 3173, "domain_category": "earth_science", "abstract_clean": "We present a new differential synthetic aperture radar (SAR) interferometry algorithm for monitoring the temporal evolution of surface deformations. The presented technique is based on an appropriate combination of differential interferograms produced by data pairs characterized by a small orbital separation (baseline) in order to limit the spatial decorrelation phenomena. The application of the singular value decomposition method allows us to easily \"link\" independent SAR acquisition datasets, separated by large baselines, thus increasing the observation temporal sampling rate. The availability of both spatial and temporal information in the processed data is used to identify and filter out atmospheric phase artifacts. We present results obtained on the data acquired from 1992 to 2000 by the European Remote Sensing satellites and relative to the Campi Flegrei caldera and to the city of Naples, Italy that demonstrate the capability of the proposed approach to follow the dynamics of the detected deformations."}
{"bibcode": "2017GEC....42..153R", "title": "The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview", "abstract": "We present an overview of the Shared Socioeconomic Pathways (SSPs), which were developed as a community effort over the last years. The SSPs comprise five narratives and a set of driving forces. Our SSP scenarios quantify energy and land-use developments and associated uncertainties for greenhouse gas and air pollutant emissions. We conduct an SSP mitigation analysis, and estimate mitigation costs. We find that very low climate targets might be out of reach in SSPs featuring high challenges. The SSPs are now ready for use by the climate change research community.", "database": ["earth science"], "keywords": ["Shared Socioeconomic Pathways", "SSP", "Climate change", "RCP", "Community scenarios", "Mitigation", "Adaptation"], "year": "2017", "doctype": "article", "citation_count": 3095, "domain_category": "earth_science", "abstract_clean": "We present an overview of the Shared Socioeconomic Pathways (SSPs), which were developed as a community effort over the last years. The SSPs comprise five narratives and a set of driving forces. Our SSP scenarios quantify energy and land-use developments and associated uncertainties for greenhouse gas and air pollutant emissions. We conduct an SSP mitigation analysis, and estimate mitigation costs. We find that very low climate targets might be out of reach in SSPs featuring high challenges. The SSPs are now ready for use by the climate change research community."}
{"bibcode": "1998GeoRL..25.1297T", "title": "The Arctic oscillation signature in the wintertime geopotential height and temperature fields", "abstract": "The leading empirical orthogonal function of the wintertime sea-level pressure field is more strongly coupled to surface air temperature fluctuations over the Eurasian continent than the North Atlantic Oscillation (NAO). It resembles the NAO in many respects; but its primary center of action covers more of the Arctic, giving it a more zonally symmetric appearance. Coupled to strong fluctuations at the 50-hPa level on the intraseasonal, interannual, and interdecadal time scales, this “Arctic Oscillation” (AO) can be interpreted as the surface signature of modulations in the strength of the polar vortex aloft. It is proposed that the zonally asymmetric surface air temperature and mid-tropospheric circulation anomalies observed in association with the AO may be secondary baroclinic features induced by the land-sea contrasts. The same modal structure is mirrored in the pronounced trends in winter and springtime surface air temperature, sea-level pressure, and 50-hPa height over the past 30 years: parts of Eurasia have warmed by as much as several K, sea-level pressure over parts of the Arctic has fallen by 4 hPa, and the core of the lower stratospheric polar vortex has cooled by several K. These trends can be interpreted as the development of a systematic bias in one of the atmosphere's dominant, naturally occurring modes of variability.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Meteorology and Atmospheric Dynamics: Stratosphere/troposphere interactions", "Meteorology and Atmospheric Dynamics: General circulation", "Global Change: Climate dynamics (3309)", "Meteorology and Atmospheric Dynamics: Climatology (1620)"], "year": "1998", "doctype": "article", "citation_count": 3048, "domain_category": "earth_science", "abstract_clean": "The leading empirical orthogonal function of the wintertime sea-level pressure field is more strongly coupled to surface air temperature fluctuations over the Eurasian continent than the North Atlantic Oscillation (NAO). It resembles the NAO in many respects; but its primary center of action covers more of the Arctic, giving it a more zonally symmetric appearance. Coupled to strong fluctuations at the 50-hPa level on the intraseasonal, interannual, and interdecadal time scales, this “Arctic Oscillation” (AO) can be interpreted as the surface signature of modulations in the strength of the polar vortex aloft. It is proposed that the zonally asymmetric surface air temperature and mid-tropospheric circulation anomalies observed in association with the AO may be secondary baroclinic features induced by the land-sea contrasts. The same modal structure is mirrored in the pronounced trends in winter and springtime surface air temperature, sea-level pressure, and 50-hPa height over the past 30 years: parts of Eurasia have warmed by as much as several K, sea-level pressure over parts of the Arctic has fallen by 4 hPa, and the core of the lower stratospheric polar vortex has cooled by several K. These trends can be interpreted as the development of a systematic bias in one of the atmosphere's dominant, naturally occurring modes of variability."}
{"bibcode": "1992Ecol...73.1943L", "title": "The Problem of Pattern and Scale in Ecology: The Robert H. MacArthur Award Lecture", "abstract": "It is argued that the problem of pattern and scale is the central problem in ecology, unifying population biology and ecosystems science, and marrying basic and applied ecology. Applied challenges, such as the prediction of the ecological causes and consequences of global climate change, require the interfacing of phenomena that occur on very different scales of space, time, and ecological organization. Furthermore, there is no single natural scale at which ecological phenomena should be studied; systems generally show characteristic variability on a range of spatial, temporal, and organizational scales. The observer imposes a perceptual bias, a filter through which the system is viewed. This has fundamental evolutionary significance, since every organism is an \"observer\" of the environment, and life history adaptations such as dispersal and dormancy alter the perceptual scales of the species, and the observed variability. It likewise has fundamental significance for our own study of ecological systems, since the patterns that are unique to any range of scales will have unique causes and biological consequences. The key to prediction and understanding lies in the elucidation of mechanisms underlying observed patterns. Typically, these mechanisms operate at different scales than those on which the patterns are observed; in some cases, the patterns must be understood as emerging form the collective behaviors of large ensembles of smaller scale units. In other cases, the pattern is imposed by larger scale constraints. Examination of such phenomena requires the study of how pattern and variability change with the scale of description, and the development of laws for simplification, aggregation, and scaling. Examples are given from the marine and terrestrial literatures.", "database": ["earth science"], "keywords": [], "year": "1992", "doctype": "article", "citation_count": 3021, "domain_category": "earth_science", "abstract_clean": "It is argued that the problem of pattern and scale is the central problem in ecology, unifying population biology and ecosystems science, and marrying basic and applied ecology. Applied challenges, such as the prediction of the ecological causes and consequences of global climate change, require the interfacing of phenomena that occur on very different scales of space, time, and ecological organization. Furthermore, there is no single natural scale at which ecological phenomena should be studied; systems generally show characteristic variability on a range of spatial, temporal, and organizational scales. The observer imposes a perceptual bias, a filter through which the system is viewed. This has fundamental evolutionary significance, since every organism is an \"observer\" of the environment, and life history adaptations such as dispersal and dormancy alter the perceptual scales of the species, and the observed variability. It likewise has fundamental significance for our own study of ecological systems, since the patterns that are unique to any range of scales will have unique causes and biological consequences. The key to prediction and understanding lies in the elucidation of mechanisms underlying observed patterns. Typically, these mechanisms operate at different scales than those on which the patterns are observed; in some cases, the patterns must be understood as emerging form the collective behaviors of large ensembles of smaller scale units. In other cases, the pattern is imposed by larger scale constraints. Examination of such phenomena requires the study of how pattern and variability change with the scale of description, and the development of laws for simplification, aggregation, and scaling. Examples are given from the marine and terrestrial literatures."}
{"bibcode": "2001ITGRS..39....8F", "title": "Permanent Scatterers in SAR Interferometry", "abstract": "Temporal and geometrical decorrelation often prevents SAR interferometry from being an operational tool for surface deformation monitoring and topographic profile reconstruction. Moreover, atmospheric disturbances can strongly compromise the accuracy of the results. In this paper, we present a complete procedure for the identification and exploitation of stable natural reflectors or permanent scatterers (PSs) starting from long temporal series of interferometric SAR images. When, as it often happens, the dimension of the PS is smaller than the resolution cell, the coherence is good even for interferograms with baselines larger than the decorrelation one, and all the available images of the ESA ERS data set can be successfully exploited. On these pixels, submeter DEM accuracy and millimetric terrain motion detection can be achieved, since atmospheric phase screen (APS) contributions can be estimated and removed. Examples are then shown of small motion measurements, DEM refinement, and APS estimation and removal in the case of a sliding area in Ancona, Italy. ERS data have been used.", "database": ["physics", "earth science"], "keywords": ["Differential interferometry", "digital elevation model (DEM) reconstruction", "geodetic measurements", "radar data filtering", "synthetic aperture radar (SAR)", "Scattering", "Interferometry", "Decorrelation", "Motion estimation", "Surface reconstruction", "Surface topography", "Monitoring", "Image reconstruction", "Image resolution", "Motion detection"], "year": "2001", "doctype": "article", "citation_count": 2969, "domain_category": "earth_science", "abstract_clean": "Temporal and geometrical decorrelation often prevents SAR interferometry from being an operational tool for surface deformation monitoring and topographic profile reconstruction. Moreover, atmospheric disturbances can strongly compromise the accuracy of the results. In this paper, we present a complete procedure for the identification and exploitation of stable natural reflectors or permanent scatterers (PSs) starting from long temporal series of interferometric SAR images. When, as it often happens, the dimension of the PS is smaller than the resolution cell, the coherence is good even for interferograms with baselines larger than the decorrelation one, and all the available images of the ESA ERS data set can be successfully exploited. On these pixels, submeter DEM accuracy and millimetric terrain motion detection can be achieved, since atmospheric phase screen (APS) contributions can be estimated and removed. Examples are then shown of small motion measurements, DEM refinement, and APS estimation and removal in the case of a sliding area in Ancona, Italy. ERS data have been used."}
{"bibcode": "2000EcoAp..10..689M", "title": "Biotic Invasions: Causes, Epidemiology, Global Consequences, and Control", "abstract": "Biotic invaders are species that establish a new range in which they proliferate, spread, and persist to the detriment of the environment. They are the most important ecological outcomes from the unprecedented alterations in the distribution of the earth's biota brought about largely through human transport and commerce. In a world without borders, few if any areas remain sheltered from these immigrations. The fate of immigrants is decidedly mixed. Few survive the hazards of chronic and stochastic forces, and only a small fraction become naturalized. In turn, some naturalized species do become invasive. There are several potential reasons why some immigrant species prosper: some escape from the constraints of their native predators or parasites; others are aided by human-caused disturbance that disrupts native communities. Ironically, many biotic invasions are apparently facilitated by cultivation and husbandry, unintentional actions that foster immigrant populations until they are self-perpetuating and uncontrollable. Whatever the cause, biotic invaders can in many cases inflict enormous environmental damage: (1) Animal invaders can cause extinctions of vulnerable native species through predation, grazing, competition, and habitat alteration. (2) Plant invaders can completely alter the fire regime, nutrient cycling, hydrology, and energy budgets in a native ecosystem and can greatly diminish the abundance or survival of native species. (3) In agriculture, the principal pests of temperate crops are nonindigenous, and the combined expenses of pest control and crop losses constitute an onerous \"tax\" on food, fiber, and forage production. (4) The global cost of virulent plant and animal diseases caused by parasites transported to new ranges and presented with susceptible new hosts is currently incalculable. Identifying future invaders and taking effective steps to prevent their dispersal and establishment constitutes an enormous challenge to both conservation and international commerce. Detection and management when exclusion fails have proved daunting for varied reasons: (1) Efforts to identify general attributes of future invaders have often been inconclusive. (2) Predicting susceptible locales for future invasions seems even more problematic, given the enormous differences in the rates of arrival among potential invaders. (3) Eradication of an established invader is rare, and control efforts vary enormously in their efficacy. Successful control, however, depends more on commitment and continuing diligence than on the efficacy of specific tools themselves. (4) Control of biotic invasions is most effective when it employs a long-term, ecosystem-wide strategy rather than a tactical approach focused on battling individual invaders. (5) Prevention of invasions is much less costly than post-entry control. Revamping national and international quarantine laws by adopting a \"guilty until proven innocent\" approach would be a productive first step. Failure to address the issue of biotic invasions could effectively result in severe global consequences, including wholesale loss of agricultural, forestry, and fishery resources in some regions, disruption of the ecological processes that supply natural services on which human enterprise depends, and the creation of homogeneous, impoverished ecosystems composed of cosmopolitan species. Given their current scale, biotic invasions have taken their place alongside human-driven atmospheric and oceanic alterations as major agents of global change. Left unchecked, they will influence these other forces in profound but still unpredictable ways.", "database": ["earth science"], "keywords": ["alien species", "biological control", "biotic invaders", "eradication", "global change", "immigration", "invasion", "naturalization", "nonindigenous", "pests", "weeds"], "year": "2000", "doctype": "article", "citation_count": 2956, "domain_category": "earth_science", "abstract_clean": "Biotic invaders are species that establish a new range in which they proliferate, spread, and persist to the detriment of the environment. They are the most important ecological outcomes from the unprecedented alterations in the distribution of the earth's biota brought about largely through human transport and commerce. In a world without borders, few if any areas remain sheltered from these immigrations. The fate of immigrants is decidedly mixed. Few survive the hazards of chronic and stochastic forces, and only a small fraction become naturalized. In turn, some naturalized species do become invasive. There are several potential reasons why some immigrant species prosper: some escape from the constraints of their native predators or parasites; others are aided by human-caused disturbance that disrupts native communities. Ironically, many biotic invasions are apparently facilitated by cultivation and husbandry, unintentional actions that foster immigrant populations until they are self-perpetuating and uncontrollable. Whatever the cause, biotic invaders can in many cases inflict enormous environmental damage: (1) Animal invaders can cause extinctions of vulnerable native species through predation, grazing, competition, and habitat alteration. (2) Plant invaders can completely alter the fire regime, nutrient cycling, hydrology, and energy budgets in a native ecosystem and can greatly diminish the abundance or survival of native species. (3) In agriculture, the principal pests of temperate crops are nonindigenous, and the combined expenses of pest control and crop losses constitute an onerous \"tax\" on food, fiber, and forage production. (4) The global cost of virulent plant and animal diseases caused by parasites transported to new ranges and presented with susceptible new hosts is currently incalculable. Identifying future invaders and taking effective steps to prevent their dispersal and establishment constitutes an enormous challenge to both conservation and international commerce. Detection and management when exclusion fails have proved daunting for varied reasons: (1) Efforts to identify general attributes of future invaders have often been inconclusive. (2) Predicting susceptible locales for future invasions seems even more problematic, given the enormous differences in the rates of arrival among potential invaders. (3) Eradication of an established invader is rare, and control efforts vary enormously in their efficacy. Successful control, however, depends more on commitment and continuing diligence than on the efficacy of specific tools themselves. (4) Control of biotic invasions is most effective when it employs a long-term, ecosystem-wide strategy rather than a tactical approach focused on battling individual invaders. (5) Prevention of invasions is much less costly than post-entry control. Revamping national and international quarantine laws by adopting a \"guilty until proven innocent\" approach would be a productive first step. Failure to address the issue of biotic invasions could effectively result in severe global consequences, including wholesale loss of agricultural, forestry, and fishery resources in some regions, disruption of the ecological processes that supply natural services on which human enterprise depends, and the creation of homogeneous, impoverished ecosystems composed of cosmopolitan species. Given their current scale, biotic invasions have taken their place alongside human-driven atmospheric and oceanic alterations as major agents of global change. Left unchecked, they will influence these other forces in profound but still unpredictable ways."}
{"bibcode": "2009ACP.....9.5155H", "title": "The formation, properties and impact of secondary organic aerosol: current and emerging issues", "abstract": "Secondary organic aerosol (SOA) accounts for a significant fraction of ambient tropospheric aerosol and a detailed knowledge of the formation, properties and transformation of SOA is therefore required to evaluate its impact on atmospheric processes, climate and human health. The chemical and physical processes associated with SOA formation are complex and varied, and, despite considerable progress in recent years, a quantitative and predictive understanding of SOA formation does not exist and therefore represents a major research challenge in atmospheric science. This review begins with an update on the current state of knowledge on the global SOA budget and is followed by an overview of the atmospheric degradation mechanisms for SOA precursors, gas-particle partitioning theory and the analytical techniques used to determine the chemical composition of SOA. A survey of recent laboratory, field and modeling studies is also presented. The following topical and emerging issues are highlighted and discussed in detail: molecular characterization of biogenic SOA constituents, condensed phase reactions and oligomerization, the interaction of atmospheric organic components with sulfuric acid, the chemical and photochemical processing of organics in the atmospheric aqueous phase, aerosol formation from real plant emissions, interaction of atmospheric organic components with water, thermodynamics and mixtures in atmospheric models. Finally, the major challenges ahead in laboratory, field and modeling studies of SOA are discussed and recommendations for future research directions are proposed.", "database": ["physics", "earth science"], "keywords": [], "year": "2009", "doctype": "article", "citation_count": 2949, "domain_category": "earth_science", "abstract_clean": "Secondary organic aerosol (SOA) accounts for a significant fraction of ambient tropospheric aerosol and a detailed knowledge of the formation, properties and transformation of SOA is therefore required to evaluate its impact on atmospheric processes, climate and human health. The chemical and physical processes associated with SOA formation are complex and varied, and, despite considerable progress in recent years, a quantitative and predictive understanding of SOA formation does not exist and therefore represents a major research challenge in atmospheric science. This review begins with an update on the current state of knowledge on the global SOA budget and is followed by an overview of the atmospheric degradation mechanisms for SOA precursors, gas-particle partitioning theory and the analytical techniques used to determine the chemical composition of SOA. A survey of recent laboratory, field and modeling studies is also presented. The following topical and emerging issues are highlighted and discussed in detail: molecular characterization of biogenic SOA constituents, condensed phase reactions and oligomerization, the interaction of atmospheric organic components with sulfuric acid, the chemical and photochemical processing of organics in the atmospheric aqueous phase, aerosol formation from real plant emissions, interaction of atmospheric organic components with water, thermodynamics and mixtures in atmospheric models. Finally, the major challenges ahead in laboratory, field and modeling studies of SOA are discussed and recommendations for future research directions are proposed."}
{"bibcode": "1994JGR....9910143E", "title": "Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics", "abstract": "A new sequential data assimilation method is discussed. It is based on forecasting the error statistics using Monte Carlo methods, a better alternative than solving the traditional and computationally extremely demanding approximate error covariance equation used in the extended Kalman filter. The unbounded error growth found in the extended Kalman filter, which is caused by an overly simplified closure in the error covariance equation, is completely eliminated. Open boundaries can be handled as long as the ocean model is well posed. Well-known numerical instabilities associated with the error covariance equation are avoided because storage and evolution of the error covariance matrix itself are not needed. The results are also better than what is provided by the extended Kalman filter since there is no closure problem and the quality of the forecast error statistics therefore improves. The method should be feasible also for more sophisticated primitive equation models. The computational load for reasonable accuracy is only a fraction of what is required for the extended Kalman filter and is given by the storage of, say, 100 model states for an ensemble size of 100 and thus CPU requirements of the order of the cost of 100 model integrations. The proposed method can therefore be used with realistic nonlinear ocean models on large domains on existing computers, and it is also well suited for parallel computers and clusters of workstations where each processor integrates a few members of the ensemble.", "database": ["physics", "earth science"], "keywords": ["Oceanography: General: Ocean prediction", "Oceanography: General: Numerical modeling"], "year": "1994", "doctype": "article", "citation_count": 2902, "domain_category": "earth_science", "abstract_clean": "A new sequential data assimilation method is discussed. It is based on forecasting the error statistics using Monte Carlo methods, a better alternative than solving the traditional and computationally extremely demanding approximate error covariance equation used in the extended Kalman filter. The unbounded error growth found in the extended Kalman filter, which is caused by an overly simplified closure in the error covariance equation, is completely eliminated. Open boundaries can be handled as long as the ocean model is well posed. Well-known numerical instabilities associated with the error covariance equation are avoided because storage and evolution of the error covariance matrix itself are not needed. The results are also better than what is provided by the extended Kalman filter since there is no closure problem and the quality of the forecast error statistics therefore improves. The method should be feasible also for more sophisticated primitive equation models. The computational load for reasonable accuracy is only a fraction of what is required for the extended Kalman filter and is given by the storage of, say, 100 model states for an ensemble size of 100 and thus CPU requirements of the order of the cost of 100 model integrations. The proposed method can therefore be used with realistic nonlinear ocean models on large domains on existing computers, and it is also well suited for parallel computers and clusters of workstations where each processor integrates a few members of the ensemble."}
{"bibcode": "1999WRR....35..233L", "title": "Evaluating the use of ``goodness-of-fit'' measures in hydrologic and hydroclimatic model validation", "abstract": "Correlation and correlation-based measures (e.g., the coefficient of determination) have been widely used to evaluate the \"goodness-of-fit\" of hydrologic and hydroclimatic models. These measures are oversensitive to extreme values (outliers) and are insensitive to additive and proportional differences between model predictions and observations. Because of these limitations, correlation-based measures can indicate that a model is a good predictor, even when it is not. In this paper, useful alternative goodness-of-fit or relative error measures (including the coefficient of efficiency and the index of agreement) that overcome many of the limitations of correlation-based measures are discussed. Modifications to these statistics to aid in interpretation are presented. It is concluded that correlation and correlation-based measures should not be used to assess the goodness-of-fit of a hydrologic or hydroclimatic model and that additional evaluation measures (such as summary statistics and absolute error measures) should supplement model evaluation tools.", "database": ["physics", "earth science"], "keywords": ["Hydrology"], "year": "1999", "doctype": "article", "citation_count": 2901, "domain_category": "earth_science", "abstract_clean": "Correlation and correlation-based measures (e.g., the coefficient of determination) have been widely used to evaluate the \"goodness-of-fit\" of hydrologic and hydroclimatic models. These measures are oversensitive to extreme values (outliers) and are insensitive to additive and proportional differences between model predictions and observations. Because of these limitations, correlation-based measures can indicate that a model is a good predictor, even when it is not. In this paper, useful alternative goodness-of-fit or relative error measures (including the coefficient of efficiency and the index of agreement) that overcome many of the limitations of correlation-based measures are discussed. Modifications to these statistics to aid in interpretation are presented. It is concluded that correlation and correlation-based measures should not be used to assess the goodness-of-fit of a hydrologic or hydroclimatic model and that additional evaluation measures (such as summary statistics and absolute error measures) should supplement model evaluation tools."}
{"bibcode": "2011JEnvM..92..407F", "title": "Removal of heavy metal ions from wastewaters: A review", "abstract": "Heavy metal pollution has become one of the most serious environmental problems today. The treatment of heavy metals is of special concern due to their recalcitrance and persistence in the environment. In recent years, various methods for heavy metal removal from wastewater have been extensively studied. This paper reviews the current methods that have been used to treat heavy metal wastewater and evaluates these techniques. These technologies include chemical precipitation, ion-exchange, adsorption, membrane filtration, coagulation–flocculation, flotation and electrochemical methods. About 185 published studies (1988–2010) are reviewed in this paper. It is evident from the literature survey articles that ion-exchange, adsorption and membrane filtration are the most frequently studied for the treatment of heavy metal wastewater.", "database": ["earth science"], "keywords": ["Heavy metal wastewater", "Treatment technology", "Review"], "year": "2011", "doctype": "article", "citation_count": 2891, "domain_category": "earth_science", "abstract_clean": "Heavy metal pollution has become one of the most serious environmental problems today. The treatment of heavy metals is of special concern due to their recalcitrance and persistence in the environment. In recent years, various methods for heavy metal removal from wastewater have been extensively studied. This paper reviews the current methods that have been used to treat heavy metal wastewater and evaluates these techniques. These technologies include chemical precipitation, ion-exchange, adsorption, membrane filtration, coagulation–flocculation, flotation and electrochemical methods. About 185 published studies (1988–2010) are reviewed in this paper. It is evident from the literature survey articles that ion-exchange, adsorption and membrane filtration are the most frequently studied for the treatment of heavy metal wastewater."}
{"bibcode": "2005GCBio..11.1424R", "title": "On the separation of net ecosystem exchange into assimilation and ecosystem respiration: review and improved algorithm", "abstract": "This paper discusses the advantages and disadvantages of the different methods that separate net ecosystem exchange (NEE) into its major components, gross ecosystem carbon uptake (GEP) and ecosystem respiration (R<SUB>eco</SUB>). In particular, we analyse the effect of the extrapolation of night-time values of ecosystem respiration into the daytime; this is usually done with a temperature response function that is derived from long-term data sets. For this analysis, we used 16 one-year-long data sets of carbon dioxide exchange measurements from European and US-American eddy covariance networks. These sites span from the boreal to Mediterranean climates, and include deciduous and evergreen forest, scrubland and crop ecosystems. We show that the temperature sensitivity of R<SUB>eco</SUB>, derived from long-term (annual) data sets, does not reflect the short-term temperature sensitivity that is effective when extrapolating from night- to daytime. Specifically, in summer active ecosystems the long-term temperature sensitivity exceeds the short-term sensitivity. Thus, in those ecosystems, the application of a long-term temperature sensitivity to the extrapolation of respiration from night to day leads to a systematic overestimation of ecosystem respiration from half-hourly to annual time-scales, which can reach &gt;25% for an annual budget and which consequently affects estimates of GEP. Conversely, in summer passive (Mediterranean) ecosystems, the long-term temperature sensitivity is lower than the short-term temperature sensitivity resulting in underestimation of annual sums of respiration. We introduce a new generic algorithm that derives a short-term temperature sensitivity of R<SUB>eco</SUB> from eddy covariance data that applies this to the extrapolation from night- to daytime, and that further performs a filling of data gaps that exploits both, the covariance between fluxes and meteorological drivers and the temporal structure of the fluxes. While this algorithm should give less biased estimates of GEP and R<SUB>eco</SUB>, we discuss the remaining biases and recommend that eddy covariance measurements are still backed by ancillary flux measurements that can reduce the uncertainties inherent in the eddy covariance data.", "database": ["physics", "earth science"], "keywords": ["carbon balance", "computational methods", "ecosystem respiration", "eddy covariance", "gross carbon uptake", "temperature sensitivity of respiration"], "year": "2005", "doctype": "article", "citation_count": 2822, "domain_category": "earth_science", "abstract_clean": "This paper discusses the advantages and disadvantages of the different methods that separate net ecosystem exchange (NEE) into its major components, gross ecosystem carbon uptake (GEP) and ecosystem respiration (Reco). In particular, we analyse the effect of the extrapolation of night-time values of ecosystem respiration into the daytime; this is usually done with a temperature response function that is derived from long-term data sets. For this analysis, we used 16 one-year-long data sets of carbon dioxide exchange measurements from European and US-American eddy covariance networks. These sites span from the boreal to Mediterranean climates, and include deciduous and evergreen forest, scrubland and crop ecosystems. We show that the temperature sensitivity of Reco, derived from long-term (annual) data sets, does not reflect the short-term temperature sensitivity that is effective when extrapolating from night- to daytime. Specifically, in summer active ecosystems the long-term temperature sensitivity exceeds the short-term sensitivity. Thus, in those ecosystems, the application of a long-term temperature sensitivity to the extrapolation of respiration from night to day leads to a systematic overestimation of ecosystem respiration from half-hourly to annual time-scales, which can reach >25% for an annual budget and which consequently affects estimates of GEP. Conversely, in summer passive (Mediterranean) ecosystems, the long-term temperature sensitivity is lower than the short-term temperature sensitivity resulting in underestimation of annual sums of respiration. We introduce a new generic algorithm that derives a short-term temperature sensitivity of Reco from eddy covariance data that applies this to the extrapolation from night- to daytime, and that further performs a filling of data gaps that exploits both, the covariance between fluxes and meteorological drivers and the temporal structure of the fluxes. While this algorithm should give less biased estimates of GEP and Reco, we discuss the remaining biases and recommend that eddy covariance measurements are still backed by ancillary flux measurements that can reduce the uncertainties inherent in the eddy covariance data."}
{"bibcode": "1995JGR...100.8873G", "title": "A global model of natural volatile organic compound emissions", "abstract": "Numerical assessments of global air quality and potential changes in atmospheric chemical constituents require estimates of the surface fluxes of a variety of trace gas species. We have developed a global model to estimate emissions of volatile organic compounds from natural sources (NVOC). Methane is not considered here and has been reviewed in detail elsewhere. The model has a highly resolved spatial grid (0.5°×0.5° latitude/longitude) and generates hourly average emission estimates. Chemical species are grouped into four categories: isoprene, monoterpenes, other reactive VOC (ORVOC), and other VOC (OVOC). NVOC emissions from oceans are estimated as a function of geophysical variables from a general circulation model and ocean color satellite data. Emissions from plant foliage are estimated from ecosystem specific biomass and emission factors and algorithms describing light and temperature dependence of NVOC emissions. Foliar density estimates are based on climatic variables and satellite data. Temporal variations in the model are driven by monthly estimates of biomass and temperature and hourly light estimates. The annual global VOC flux is estimated to be 1150 Tg C, composed of 44% isoprene, 11% monoterpenes, 22.5% other reactive VOC, and 22.5% other VOC. Large uncertainties exist for each of these estimates and particularly for compounds other than isoprene and monoterpenes. Tropical woodlands (rain forest, seasonal, drought-deciduous, and savanna) contribute about half of all global natural VOC emissions. Croplands, shrublands and other woodlands contribute 10-20% apiece. Isoprene emissions calculated for temperate regions are as much as a factor of 5 higher than previous estimates.", "database": ["physics", "earth science"], "keywords": ["Atmospheric Composition and Structure: Biosphere/atmosphere interactions", "Global Change: Atmosphere", "Global Change: Biogeochemical processes", "Atmospheric Composition and Structure: Geochemical cycles"], "year": "1995", "doctype": "article", "citation_count": 2810, "domain_category": "earth_science", "abstract_clean": "Numerical assessments of global air quality and potential changes in atmospheric chemical constituents require estimates of the surface fluxes of a variety of trace gas species. We have developed a global model to estimate emissions of volatile organic compounds from natural sources (NVOC). Methane is not considered here and has been reviewed in detail elsewhere. The model has a highly resolved spatial grid (0.5°×0.5° latitude/longitude) and generates hourly average emission estimates. Chemical species are grouped into four categories: isoprene, monoterpenes, other reactive VOC (ORVOC), and other VOC (OVOC). NVOC emissions from oceans are estimated as a function of geophysical variables from a general circulation model and ocean color satellite data. Emissions from plant foliage are estimated from ecosystem specific biomass and emission factors and algorithms describing light and temperature dependence of NVOC emissions. Foliar density estimates are based on climatic variables and satellite data. Temporal variations in the model are driven by monthly estimates of biomass and temperature and hourly light estimates. The annual global VOC flux is estimated to be 1150 Tg C, composed of 44% isoprene, 11% monoterpenes, 22.5% other reactive VOC, and 22.5% other VOC. Large uncertainties exist for each of these estimates and particularly for compounds other than isoprene and monoterpenes. Tropical woodlands (rain forest, seasonal, drought-deciduous, and savanna) contribute about half of all global natural VOC emissions. Croplands, shrublands and other woodlands contribute 10-20% apiece. Isoprene emissions calculated for temperate regions are as much as a factor of 5 higher than previous estimates."}
{"bibcode": "2000EcMod.135..147G", "title": "Predictive habitat distribution models in ecology", "abstract": "With the rise of new powerful statistical techniques and GIS tools, the development of predictive habitat distribution models has rapidly increased in ecology. Such models are static and probabilistic in nature, since they statistically relate the geographical distribution of species or communities to their present environment. A wide array of models has been developed to cover aspects as diverse as biogeography, conservation biology, climate change research, and habitat or species management. In this paper, we present a review of predictive habitat distribution modeling. The variety of statistical techniques used is growing. Ordinary multiple regression and its generalized form (GLM) are very popular and are often used for modeling species distributions. Other methods include neural networks, ordination and classification methods, Bayesian models, locally weighted approaches (e.g. GAM), environmental envelopes or even combinations of these models. The selection of an appropriate method should not depend solely on statistical considerations. Some models are better suited to reflect theoretical findings on the shape and nature of the species' response (or realized niche). Conceptual considerations include e.g. the trade-off between optimizing accuracy versus optimizing generality. In the field of static distribution modeling, the latter is mostly related to selecting appropriate predictor variables and to designing an appropriate procedure for model selection. New methods, including threshold-independent measures (e.g. receiver operating characteristic (ROC)-plots) and resampling techniques (e.g. bootstrap, cross-validation) have been introduced in ecology for testing the accuracy of predictive models. The choice of an evaluation measure should be driven primarily by the goals of the study. This may possibly lead to the attribution of different weights to the various types of prediction errors (e.g. omission, commission or confusion). Testing the model in a wider range of situations (in space and time) will permit one to define the range of applications for which the model predictions are suitable. In turn, the qualification of the model depends primarily on the goals of the study that define the qualification criteria and on the usability of the model, rather than on statistics alone.", "database": ["earth science"], "keywords": ["Biogeography", "Plant ecology", "Vegetation models", "Species models", "Model formulation", "Model calibration", "Model predictions", "Model evaluation", "Model credibility", "Model applicability", "GIS", "Statistics", "CART", "classification and regression trees", "CC", "climate change", "CCA", "canonical correspondence analysis", "DEM", "digital elevation model", "ENFA", "ecological niche factor analysis", "GAM", "generalized additive model", "GLM", "generalized linear model", "LS", "least squares", "PCA", "principal component analysis", "RDA", "redundancy analysis"], "year": "2000", "doctype": "article", "citation_count": 2795, "domain_category": "earth_science", "abstract_clean": "With the rise of new powerful statistical techniques and GIS tools, the development of predictive habitat distribution models has rapidly increased in ecology. Such models are static and probabilistic in nature, since they statistically relate the geographical distribution of species or communities to their present environment. A wide array of models has been developed to cover aspects as diverse as biogeography, conservation biology, climate change research, and habitat or species management. In this paper, we present a review of predictive habitat distribution modeling. The variety of statistical techniques used is growing. Ordinary multiple regression and its generalized form (GLM) are very popular and are often used for modeling species distributions. Other methods include neural networks, ordination and classification methods, Bayesian models, locally weighted approaches (e.g. GAM), environmental envelopes or even combinations of these models. The selection of an appropriate method should not depend solely on statistical considerations. Some models are better suited to reflect theoretical findings on the shape and nature of the species' response (or realized niche). Conceptual considerations include e.g. the trade-off between optimizing accuracy versus optimizing generality. In the field of static distribution modeling, the latter is mostly related to selecting appropriate predictor variables and to designing an appropriate procedure for model selection. New methods, including threshold-independent measures (e.g. receiver operating characteristic (ROC)-plots) and resampling techniques (e.g. bootstrap, cross-validation) have been introduced in ecology for testing the accuracy of predictive models. The choice of an evaluation measure should be driven primarily by the goals of the study. This may possibly lead to the attribution of different weights to the various types of prediction errors (e.g. omission, commission or confusion). Testing the model in a wider range of situations (in space and time) will permit one to define the range of applications for which the model predictions are suitable. In turn, the qualification of the model depends primarily on the goals of the study that define the qualification criteria and on the usability of the model, rather than on statistics alone."}
{"bibcode": "1997E&PSL.148..243B", "title": "The Lu-Hf isotope geochemistry of chondrites and the evolution of the mantle-crust system", "abstract": "We report analyses of the <SUP>176</SUP>Hf/ <SUP>177</SUP>Hf ratio for 25 chondrites from different classes of meteorites (C, O, and E) and the <SUP>176</SUP>Lu/ <SUP>177</SUP>Hf ratio for 23 of these as measured by plasma source mass spectrometry. We have obtained a new set of present-day mean values in chondrites of <SUP>176</SUP>Hf/ <SUP>177</SUP>Hf= 0.282772 ± 29 and <SUP>176</SUP>Lu/ <SUP>177</SUP>Hf= 0.0332 ± 2. The <SUP>176</SUP>Hf/ <SUP>177</SUP>Hf ratio of the Solar System material 4.56 Ga ago was 0.279742 ± 29. Because the mantle array lies above the Bulk Silicate Earth in a <SUP>143</SUP>Nd/ <SUP>144</SUP>Nd versus <SUP>176</SUP>Hf/ <SUP>177</SUP>Hf plot, no terrestrial basalt seems to have formed from a primitive undifferentiated mantle, thereby casting doubt on the significance of high <SUP>3</SUP>He/ <SUP>4</SUP>He ratios. Comparison of observedHf/Nd ratios with those inferred from isotopic plots indicates that, in addition to the two most prominent components at the surface of the Earth, the depleted mantle and the continental crust, at least one more reservoir, which is not a significant component in the source of oceanic basalts, is needed to account for the Bulk Silicate Earth Hf-Nd geochemistry. This unaccounted for component probably consists of subducted basalts, representing ancient oceanic crust and plateaus. The lower continental crust and subducted pelagic sediments are found to be unsuitable candidates. Although it would explain the Lu-Hf systematics of oceanic basalts, perovskite fractionation from an early magma ocean does not account for the associated Nd isotopic signature. Most basalts forming the mantle array tap a mantle source which corresponds to residues left by ancient melting events with garnet at the liquidus.", "database": ["astronomy", "physics", "earth science"], "keywords": [], "year": "1997", "doctype": "article", "citation_count": 2793, "domain_category": "earth_science", "abstract_clean": "We report analyses of the 176Hf/ 177Hf ratio for 25 chondrites from different classes of meteorites (C, O, and E) and the 176Lu/ 177Hf ratio for 23 of these as measured by plasma source mass spectrometry. We have obtained a new set of present-day mean values in chondrites of 176Hf/ 177Hf= 0.282772 ± 29 and 176Lu/ 177Hf= 0.0332 ± 2. The 176Hf/ 177Hf ratio of the Solar System material 4.56 Ga ago was 0.279742 ± 29. Because the mantle array lies above the Bulk Silicate Earth in a 143Nd/ 144Nd versus 176Hf/ 177Hf plot, no terrestrial basalt seems to have formed from a primitive undifferentiated mantle, thereby casting doubt on the significance of high 3He/ 4He ratios. Comparison of observedHf/Nd ratios with those inferred from isotopic plots indicates that, in addition to the two most prominent components at the surface of the Earth, the depleted mantle and the continental crust, at least one more reservoir, which is not a significant component in the source of oceanic basalts, is needed to account for the Bulk Silicate Earth Hf-Nd geochemistry. This unaccounted for component probably consists of subducted basalts, representing ancient oceanic crust and plateaus. The lower continental crust and subducted pelagic sediments are found to be unsuitable candidates. Although it would explain the Lu-Hf systematics of oceanic basalts, perovskite fractionation from an early magma ocean does not account for the associated Nd isotopic signature. Most basalts forming the mantle array tap a mantle source which corresponds to residues left by ancient melting events with garnet at the liquidus."}
{"bibcode": "1992HyPr....6..279B", "title": "The future of distributed models: Model calibration and uncertainty prediction", "abstract": "This paper describes a methodology for calibration and uncertainty estimation of distributed models based on generalized likelihood measures. the GLUE procedure works with multiple sets of parameter values and allows that, within the limitations of a given model structure and errors in boundary conditions and field observations, different sets of values May, be equally likely as simulators of a catchment. Procedures for incorporating different types of observations into the calibration; Bayesian updating of likelihood values and evaluating the value of additional observations to the calibration process are described. the procedure is computationally intensive but has been implemented on a local parallel processing computer. the methodology is illustrated by an application of the Institute of Hydrology Distributed Model to data from the Gwy experimental catchment at Plynlimon, mid-Wales.", "database": ["physics", "earth science"], "keywords": ["Distributed models", "Calibration uncertainty", "Likelihood"], "year": "1992", "doctype": "article", "citation_count": 2771, "domain_category": "earth_science", "abstract_clean": "This paper describes a methodology for calibration and uncertainty estimation of distributed models based on generalized likelihood measures. the GLUE procedure works with multiple sets of parameter values and allows that, within the limitations of a given model structure and errors in boundary conditions and field observations, different sets of values May, be equally likely as simulators of a catchment. Procedures for incorporating different types of observations into the calibration; Bayesian updating of likelihood values and evaluating the value of additional observations to the calibration process are described. the procedure is computationally intensive but has been implemented on a local parallel processing computer. the methodology is illustrated by an application of the Institute of Hydrology Distributed Model to data from the Gwy experimental catchment at Plynlimon, mid-Wales."}
{"bibcode": "2010Sci...328.1382I", "title": "Climate Change Will Affect the Asian Water Towers", "abstract": "More than 1.4 billion people depend on water from the Indus, Ganges, Brahmaputra, Yangtze, and Yellow rivers. Upstream snow and ice reserves of these basins, important in sustaining seasonal water availability, are likely to be affected substantially by climate change, but to what extent is yet unclear. Here, we show that meltwater is extremely important in the Indus basin and important for the Brahmaputra basin, but plays only a modest role for the Ganges, Yangtze, and Yellow rivers. A huge difference also exists between basins in the extent to which climate change is predicted to affect water availability and food security. The Brahmaputra and Indus basins are most susceptible to reductions of flow, threatening the food security of an estimated 60 million people.", "database": ["physics", "earth science"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 2763, "domain_category": "earth_science", "abstract_clean": "More than 1.4 billion people depend on water from the Indus, Ganges, Brahmaputra, Yangtze, and Yellow rivers. Upstream snow and ice reserves of these basins, important in sustaining seasonal water availability, are likely to be affected substantially by climate change, but to what extent is yet unclear. Here, we show that meltwater is extremely important in the Indus basin and important for the Brahmaputra basin, but plays only a modest role for the Ganges, Yangtze, and Yellow rivers. A huge difference also exists between basins in the extent to which climate change is predicted to affect water availability and food security. The Brahmaputra and Indus basins are most susceptible to reductions of flow, threatening the food security of an estimated 60 million people."}
{"bibcode": "1998ChGeo.145..325P", "title": "The chemical composition of subducting sediment and its consequences for the crust and mantle", "abstract": "Subducted sediments play an important role in arc magmatism and crust-mantle recycling. Models of continental growth, continental composition, convergent margin magmatism and mantle heterogeneity all require a better understanding of the mass and chemical fluxes associated with subducting sediments. We have evaluated subducting sediments on a global basis in order to better define their chemical systematics and to determine both regional and global average compositions. We then use these compositions to assess the importance of sediments to arc volcanism and crust-mantle recycling, and to re-evaluate the chemical composition of the continental crust. The large variations in the chemical composition of marine sediments are for the most part linked to the main lithological constituents. The alkali elements (K, Rb and Cs) and high field strength elements (Ti, Nb, Hf, Zr) are closely linked to the detrital phase in marine sediments; Th is largely detrital but may be enriched in the hydrogenous Fe-Mn component of sediments; REE patterns are largely continental, but abundances are closely linked to fish debris phosphate; U is mostly detrital, but also dependent on the supply and burial rate of organic matter; Ba is linked to both biogenic barite and hydrothermal components; Sr is linked to carbonate phases. Thus, the important geochemical tracers follow the lithology of the sediments. Sediment lithologies are controlled in turn by a small number of factors: proximity of detrital sources (volcanic and continental); biological productivity and preservation of carbonate and opal; and sedimentation rate. Because of the link with lithology and the wealth of lithological data routinely collected for ODP and DSDP drill cores, bulk geochemical averages can be calculated to better than 30% for most elements from fewer than ten chemical analyses for a typical drill core (100-1000 m). Combining the geochemical systematics with convergence rate and other parameters permits calculation of regional compositional fluxes for subducting sediment. These regional fluxes can be compared to the compositions of arc volcanics to asses the importance of sediment subduction to arc volcanism. For the 70% of the trenches worldwide where estimates can be made, the regional fluxes also provide the basis for a global subducting sediment (GLOSS) composition and flux. GLOSS is dominated by terrigenous material (76 wt% terrigenous, 7 wt% calcium carbonate, 10 wt% opal, 7 wt% mineral-bound H <SUB>2</SUB>O <SUP>+</SUP>), and therefore similar to upper continental crust (UCC) in composition. Exceptions include enrichment in Ba, Mn and the middle and heavy REE, and depletions in detrital elements diluted by biogenic material (alkalis, Th, Zr, Hf). Sr and Pb are identical in GLOSS and UCC as a result of a balance between dilution and enrichment by marine phases. GLOSS and the systematics of marine sediments provide an independent approach to the composition of the upper continental crust for detrital elements. Significant discrepancies of up to a factor of two exist between the marine sediment data and current upper crustal estimates for Cs, Nb, Ta and Ti. Suggested revisions to UCC include Cs (7.3 ppm), Nb (13.7 ppm), Ta (0.96 ppm) and TiO <SUB>2</SUB> (0.76 wt%). These revisions affect recent bulk continental crust estimates for La/Nb and U/Nb, and lead to an even greater contrast between the continents and mantle for these important trace element ratios. GLOSS and the regional sediment data also provide new insights into the mantle sources of oceanic basalts. The classical geochemical distinction between `pelagic' and `terrigenous' sediment sources is not valid and needs to be replaced by a more comprehensive understanding of the compositional variations in complete sedimentary columns. In addition, isotopic arguments based on surface sediments alone can lead to erroneous conclusions. Specifically, the Nd/Hf ratio of GLOSS relaxes considerably the severe constraints on the amount of sediment recycling into the mantle based on earlier estimates from surface sediment compositions.", "database": ["physics", "earth science"], "keywords": [], "year": "1998", "doctype": "article", "citation_count": 2756, "domain_category": "earth_science", "abstract_clean": "Subducted sediments play an important role in arc magmatism and crust-mantle recycling. Models of continental growth, continental composition, convergent margin magmatism and mantle heterogeneity all require a better understanding of the mass and chemical fluxes associated with subducting sediments. We have evaluated subducting sediments on a global basis in order to better define their chemical systematics and to determine both regional and global average compositions. We then use these compositions to assess the importance of sediments to arc volcanism and crust-mantle recycling, and to re-evaluate the chemical composition of the continental crust. The large variations in the chemical composition of marine sediments are for the most part linked to the main lithological constituents. The alkali elements (K, Rb and Cs) and high field strength elements (Ti, Nb, Hf, Zr) are closely linked to the detrital phase in marine sediments; Th is largely detrital but may be enriched in the hydrogenous Fe-Mn component of sediments; REE patterns are largely continental, but abundances are closely linked to fish debris phosphate; U is mostly detrital, but also dependent on the supply and burial rate of organic matter; Ba is linked to both biogenic barite and hydrothermal components; Sr is linked to carbonate phases. Thus, the important geochemical tracers follow the lithology of the sediments. Sediment lithologies are controlled in turn by a small number of factors: proximity of detrital sources (volcanic and continental); biological productivity and preservation of carbonate and opal; and sedimentation rate. Because of the link with lithology and the wealth of lithological data routinely collected for ODP and DSDP drill cores, bulk geochemical averages can be calculated to better than 30% for most elements from fewer than ten chemical analyses for a typical drill core (100-1000 m). Combining the geochemical systematics with convergence rate and other parameters permits calculation of regional compositional fluxes for subducting sediment. These regional fluxes can be compared to the compositions of arc volcanics to asses the importance of sediment subduction to arc volcanism. For the 70% of the trenches worldwide where estimates can be made, the regional fluxes also provide the basis for a global subducting sediment (GLOSS) composition and flux. GLOSS is dominated by terrigenous material (76 wt% terrigenous, 7 wt% calcium carbonate, 10 wt% opal, 7 wt% mineral-bound H 2O +), and therefore similar to upper continental crust (UCC) in composition. Exceptions include enrichment in Ba, Mn and the middle and heavy REE, and depletions in detrital elements diluted by biogenic material (alkalis, Th, Zr, Hf). Sr and Pb are identical in GLOSS and UCC as a result of a balance between dilution and enrichment by marine phases. GLOSS and the systematics of marine sediments provide an independent approach to the composition of the upper continental crust for detrital elements. Significant discrepancies of up to a factor of two exist between the marine sediment data and current upper crustal estimates for Cs, Nb, Ta and Ti. Suggested revisions to UCC include Cs (7.3 ppm), Nb (13.7 ppm), Ta (0.96 ppm) and TiO 2 (0.76 wt%). These revisions affect recent bulk continental crust estimates for La/Nb and U/Nb, and lead to an even greater contrast between the continents and mantle for these important trace element ratios. GLOSS and the regional sediment data also provide new insights into the mantle sources of oceanic basalts. The classical geochemical distinction between `pelagic' and `terrigenous' sediment sources is not valid and needs to be replaced by a more comprehensive understanding of the compositional variations in complete sedimentary columns. In addition, isotopic arguments based on surface sediments alone can lead to erroneous conclusions. Specifically, the Nd/Hf ratio of GLOSS relaxes considerably the severe constraints on the amount of sediment recycling into the mantle based on earlier estimates from surface sediment compositions."}
{"bibcode": "1984GeCoA..48.1135M", "title": "Stepwise enrichment of <SUP>15</SUP>N along food chains: Further evidence and the relation between δ <SUP>15</SUP>N and animal age", "abstract": "The isotopic composition of nitrogen was measured in marine and fresh-water animals from the East China Sea, The Bering Sea, Lake Ashinoko and Usujiri intertidal zone. Primary producers, showed average δ<SUP>15</SUP>Nversus atmospheric nitrogen of +5.0%. (+3.4 to +7.5) in the Bering Sea and Lake Ashinoko, and +6.8%. (+6.0 to +7.6) in Usujiri intertidal zone. Blue green algae from the East China Sea show an average -0.55%. (-0.8 to +1.2). All consumers, Zooplankton, fish and bird exhibited Stepwise enrichment of <SUP>15</SUP>N with increasing trophic level. The <SUP>15</SUP>N enrichment at a single feeding process ranged from +1.3 to +5.3 averaging +3.4 ± 1.1%.. This isotopic fractionation seems to be independent of habitat. The effect of age in animals was obtained by analyzing two marine mussels. The soft tissue nitrogen showed +2.0%. enrichment relative to that of primary producers, and the magnitude was almost constant with shell ages ranging from 0 to 8 years. A similar <SUP>15</SUP>N enrichment occurs in all Molluscs, Crustaceans, Insecta, Amphibia, Fish, Ave and Mammal species regardless of the difference in the form of excreted nitrogen and in laboratory cultured fish, brine shrimp and mice (+2.9 to +4.9%.). The excreted ammonia from guppy was sufficiently light to balance the concentration of <SUP>15</SUP>N to animal body.", "database": ["astronomy", "earth science"], "keywords": [], "year": "1984", "doctype": "article", "citation_count": 2753, "domain_category": "earth_science", "abstract_clean": "The isotopic composition of nitrogen was measured in marine and fresh-water animals from the East China Sea, The Bering Sea, Lake Ashinoko and Usujiri intertidal zone. Primary producers, showed average δ15Nversus atmospheric nitrogen of +5.0%. (+3.4 to +7.5) in the Bering Sea and Lake Ashinoko, and +6.8%. (+6.0 to +7.6) in Usujiri intertidal zone. Blue green algae from the East China Sea show an average -0.55%. (-0.8 to +1.2). All consumers, Zooplankton, fish and bird exhibited Stepwise enrichment of 15N with increasing trophic level. The 15N enrichment at a single feeding process ranged from +1.3 to +5.3 averaging +3.4 ± 1.1%.. This isotopic fractionation seems to be independent of habitat. The effect of age in animals was obtained by analyzing two marine mussels. The soft tissue nitrogen showed +2.0%. enrichment relative to that of primary producers, and the magnitude was almost constant with shell ages ranging from 0 to 8 years. A similar 15N enrichment occurs in all Molluscs, Crustaceans, Insecta, Amphibia, Fish, Ave and Mammal species regardless of the difference in the form of excreted nitrogen and in laboratory cultured fish, brine shrimp and mice (+2.9 to +4.9%.). The excreted ammonia from guppy was sufficiently light to balance the concentration of 15N to animal body."}
{"bibcode": "2001BAMS...82.2415B", "title": "FLUXNET: A New Tool to Study the Temporal and Spatial Variability of Ecosystem-Scale Carbon Dioxide, Water Vapor, and Energy Flux Densities.", "abstract": "FLUXNET is a global network of micrometeorological flux measurement sites that measure the exchanges of carbon dioxide, water vapor, and energy between the biosphere and atmosphere. At present over 140 sites are operating on a long-term and continuous basis. Vegetation under study includes temperate conifer and broadleaved (deciduous and evergreen) forests, tropical and boreal forests, crops, grasslands, chaparral, wetlands, and tundra. Sites exist on five continents and their latitudinal distribution ranges from 70°N to 30°S. FLUXNET has several primary functions. First, it provides infrastructure for compiling, archiving, and distributing carbon, water, and energy flux measurement, and meteorological, plant, and soil data to the science community. (Data and site information are available online at the FLUXNET <A href=\"http://www-eosdis.ornl.gov/FLUXNET/\">www-eosdis.ornl.gov/FLUXNET/</A><A href=\"http://\"></A>.) Second, the project supports calibration and flux intercomparison activities. This activity ensures that data from the regional networks are intercomparable. And third, FLUXNET supports the synthesis, discussion, and communication of ideas and data by supporting project scientists, workshops, and visiting scientists. The overarching goal is to provide information for validating computations of net primary productivity, evaporation, and energy absorption that are being generated by sensors mounted on the NASA Terra satellite. Data being compiled by FLUXNET are being used to quantify and compare magnitudes and dynamics of annual ecosystem carbon and water balances, to quantify the response of stand-scale carbon dioxide and water vapor flux densities to controlling biotic and abiotic factors, and to validate a hierarchy of soil-plant-atmosphere trace gas exchange models. Findings so far include 1) net CO<SUB>2</SUB> exchange of temperate broadleaved forests increases by about 5.7 g C m<SUP>-2</SUP> day<SUP>-1</SUP> for each additional day that the growing season is extended; 2) the sensitivity of net ecosystem CO<SUB>2</SUB> exchange to sunlight doubles if the sky is cloudy rather than clear; 3) the spectrum of CO<SUB>2</SUB> flux density exhibits peaks at timescales of days, weeks, and years, and a spectral gap exists at the month timescale; 4) the optimal temperature of net CO<SUB>2</SUB> exchange varies with mean summer temperature; and 5) stand age affects carbon dioxide and water vapor flux densities.", "database": ["physics", "earth science"], "keywords": [], "year": "2001", "doctype": "article", "citation_count": 2738, "domain_category": "earth_science", "abstract_clean": "FLUXNET is a global network of micrometeorological flux measurement sites that measure the exchanges of carbon dioxide, water vapor, and energy between the biosphere and atmosphere. At present over 140 sites are operating on a long-term and continuous basis. Vegetation under study includes temperate conifer and broadleaved (deciduous and evergreen) forests, tropical and boreal forests, crops, grasslands, chaparral, wetlands, and tundra. Sites exist on five continents and their latitudinal distribution ranges from 70°N to 30°S. FLUXNET has several primary functions. First, it provides infrastructure for compiling, archiving, and distributing carbon, water, and energy flux measurement, and meteorological, plant, and soil data to the science community. (Data and site information are available online at the FLUXNET www-eosdis.ornl.gov/FLUXNET/.) Second, the project supports calibration and flux intercomparison activities. This activity ensures that data from the regional networks are intercomparable. And third, FLUXNET supports the synthesis, discussion, and communication of ideas and data by supporting project scientists, workshops, and visiting scientists. The overarching goal is to provide information for validating computations of net primary productivity, evaporation, and energy absorption that are being generated by sensors mounted on the NASA Terra satellite. Data being compiled by FLUXNET are being used to quantify and compare magnitudes and dynamics of annual ecosystem carbon and water balances, to quantify the response of stand-scale carbon dioxide and water vapor flux densities to controlling biotic and abiotic factors, and to validate a hierarchy of soil-plant-atmosphere trace gas exchange models. Findings so far include 1) net CO2 exchange of temperate broadleaved forests increases by about 5.7 g C m-2 day-1 for each additional day that the growing season is extended; 2) the sensitivity of net ecosystem CO2 exchange to sunlight doubles if the sky is cloudy rather than clear; 3) the spectrum of CO2 flux density exhibits peaks at timescales of days, weeks, and years, and a spectral gap exists at the month timescale; 4) the optimal temperature of net CO2 exchange varies with mean summer temperature; and 5) stand age affects carbon dioxide and water vapor flux densities."}
{"bibcode": "2000JCli...13.1000T", "title": "Annular Modes in the Extratropical Circulation. Part I: Month-to-Month Variability*.", "abstract": "The leading modes of variability of the extratropical circulation in both hemispheres are characterized by deep, zonally symmetric or `annular' structures, with geopotential height perturbations of opposing signs in the polar cap region and in the surrounding zonal ring centered near 45° latitude. The structure and dynamics of the Southern Hemisphere (SH) annular mode have been extensively documented, whereas the existence of a Northern Hemisphere (NH) mode, herein referred to as the Arctic Oscillation (AO), has only recently been recognized. Like the SH mode, the AO can be defined as the leading empirical orthogonal function of the sea level pressure field or of the zonally symmetric geopotential height or zonal wind fields. In this paper the structure and seasonality of the NH and SH modes are compared based on data from the National Centers for Environmental Prediction-National Center for Atmospheric Research reanalysis and supplementary datasets.The structures of the NH and SH annular modes are shown to be remarkably similar, not only in the zonally averaged geopotential height and zonal wind fields, but in the mean meridional circulations as well. Both exist year-round in the troposphere, but they amplify with height upward into the stratosphere during those seasons in which the strength of the zonal flow is conducive to strong planetary wave-mean flow interaction: midwinter in the NH and late spring in the SH. During these `active seasons,' the annular modes modulate the strength of the Lagrangian mean circulation in the lower stratosphere, total column ozone and tropopause height over mid- and high latitudes, and the strength of the trade winds of their respective hemispheres. The NH mode also contains an embedded planetary wave signature with expressions in surface air temperature, precipitation, total column ozone, and tropopause height. It is argued that the horizontal temperature advection by the perturbed zonal-mean zonal wind field in the lower troposphere is instrumental in forcing this pattern.A companion paper documents the striking resemblance between the structure of the annular modes and observed climate trends over the past few decades.", "database": ["astronomy", "physics", "earth science"], "keywords": [], "year": "2000", "doctype": "article", "citation_count": 2396, "domain_category": "planetary_science", "abstract_clean": "The leading modes of variability of the extratropical circulation in both hemispheres are characterized by deep, zonally symmetric or `annular' structures, with geopotential height perturbations of opposing signs in the polar cap region and in the surrounding zonal ring centered near 45° latitude. The structure and dynamics of the Southern Hemisphere (SH) annular mode have been extensively documented, whereas the existence of a Northern Hemisphere (NH) mode, herein referred to as the Arctic Oscillation (AO), has only recently been recognized. Like the SH mode, the AO can be defined as the leading empirical orthogonal function of the sea level pressure field or of the zonally symmetric geopotential height or zonal wind fields. In this paper the structure and seasonality of the NH and SH modes are compared based on data from the National Centers for Environmental Prediction-National Center for Atmospheric Research reanalysis and supplementary datasets.The structures of the NH and SH annular modes are shown to be remarkably similar, not only in the zonally averaged geopotential height and zonal wind fields, but in the mean meridional circulations as well. Both exist year-round in the troposphere, but they amplify with height upward into the stratosphere during those seasons in which the strength of the zonal flow is conducive to strong planetary wave-mean flow interaction: midwinter in the NH and late spring in the SH. During these `active seasons,' the annular modes modulate the strength of the Lagrangian mean circulation in the lower stratosphere, total column ozone and tropopause height over mid- and high latitudes, and the strength of the trade winds of their respective hemispheres. The NH mode also contains an embedded planetary wave signature with expressions in surface air temperature, precipitation, total column ozone, and tropopause height. It is argued that the horizontal temperature advection by the perturbed zonal-mean zonal wind field in the lower troposphere is instrumental in forcing this pattern.A companion paper documents the striking resemblance between the structure of the annular modes and observed climate trends over the past few decades."}
{"bibcode": "1962AJ.....67..591K", "title": "Secular perturbations of asteroids with high inclination and eccentricity", "abstract": "Secular perturbations of asteroids with high inclination and eccentricity moving under the attraction of the sun and Jupiter are studied on the assumption that Jupiter's orbit is circular. After short-periodic terms in the Hamiltonian are eliminated, the degree of freedom for the canonical equations of motion can be reduced to 1. Since there is an energy integral, the equations can be solved by quadrature. When the ratio of the semi-major axes of the asteroid and Jupiter takes a very small value, the solutions are expressed by elliptic functions. When the z component of the angular momentum (that is, Delaunay's H) of the asteroid is smaller than a certain limiting value, there are both a stationary solution and solutions corresponding to libration cases. The limiting value of H increases as the ratio of the semimajor axes increases, i.e., the corresponding limiting inclination drops from 39.2° to 1.8° as the ratio of the axes increases from 0.0 to 0.95.", "database": ["astronomy"], "keywords": [], "year": "1962", "doctype": "article", "citation_count": 2291, "domain_category": "planetary_science", "abstract_clean": "Secular perturbations of asteroids with high inclination and eccentricity moving under the attraction of the sun and Jupiter are studied on the assumption that Jupiter's orbit is circular. After short-periodic terms in the Hamiltonian are eliminated, the degree of freedom for the canonical equations of motion can be reduced to 1. Since there is an energy integral, the equations can be solved by quadrature. When the ratio of the semi-major axes of the asteroid and Jupiter takes a very small value, the solutions are expressed by elliptic functions. When the z component of the angular momentum (that is, Delaunay's H) of the asteroid is smaller than a certain limiting value, there are both a stationary solution and solutions corresponding to libration cases. The limiting value of H increases as the ratio of the semimajor axes increases, i.e., the corresponding limiting inclination drops from 39.2° to 1.8° as the ratio of the axes increases from 0.0 to 0.95."}
{"bibcode": "2011PhRvD..84b4020H", "title": "f(R,T) gravity", "abstract": "We consider f(R,T) modified theories of gravity, where the gravitational Lagrangian is given by an arbitrary function of the Ricci scalar R and of the trace of the stress-energy tensor T. We obtain the gravitational field equations in the metric formalism, as well as the equations of motion for test particles, which follow from the covariant divergence of the stress-energy tensor. Generally, the gravitational field equations depend on the nature of the matter source. The field equations of several particular models, corresponding to some explicit forms of the function f(R,T), are also presented. An important case, which is analyzed in detail, is represented by scalar field models. We write down the action and briefly consider the cosmological implications of the f(R,T<SUP>ϕ</SUP>) models, where T<SUP>ϕ</SUP> is the trace of the stress-energy tensor of a self-interacting scalar field. The equations of motion of the test particles are also obtained from a variational principle. The motion of massive test particles is nongeodesic, and takes place in the presence of an extra-force orthogonal to the four velocity. The Newtonian limit of the equation of motion is further analyzed. Finally, we provide a constraint on the magnitude of the extra acceleration by analyzing the perihelion precession of the planet Mercury in the framework of the present model.", "database": ["astronomy", "physics"], "keywords": ["04.20.Cv", "04.50.Kd", "98.80.Jk", "98.80.Bp", "Fundamental problems and general formalism", "Modified theories of gravity", "Mathematical and relativistic aspects of cosmology", "Origin and formation of the Universe", "General Relativity and Quantum Cosmology", "Astrophysics - Cosmology and Extragalactic Astrophysics", "High Energy Physics - Theory"], "year": "2011", "doctype": "article", "citation_count": 2254, "domain_category": "planetary_science", "abstract_clean": "We consider f(R,T) modified theories of gravity, where the gravitational Lagrangian is given by an arbitrary function of the Ricci scalar R and of the trace of the stress-energy tensor T. We obtain the gravitational field equations in the metric formalism, as well as the equations of motion for test particles, which follow from the covariant divergence of the stress-energy tensor. Generally, the gravitational field equations depend on the nature of the matter source. The field equations of several particular models, corresponding to some explicit forms of the function f(R,T), are also presented. An important case, which is analyzed in detail, is represented by scalar field models. We write down the action and briefly consider the cosmological implications of the f(R,Tϕ) models, where Tϕ is the trace of the stress-energy tensor of a self-interacting scalar field. The equations of motion of the test particles are also obtained from a variational principle. The motion of massive test particles is nongeodesic, and takes place in the presence of an extra-force orthogonal to the four velocity. The Newtonian limit of the equation of motion is further analyzed. Finally, we provide a constraint on the magnitude of the extra acceleration by analyzing the perihelion precession of the planet Mercury in the framework of the present model."}
{"bibcode": "1980Sci...208.1095A", "title": "Extraterrestrial Cause for the Cretaceous-Tertiary Extinction", "abstract": "Platinum metals are depleted in the earth's crust relative to their cosmic abundance; concentrations of these elements in deep-sea sediments may thus indicate influxes of extraterrestrial material. Deep-sea limestones exposed in Italy, Denmark, and New Zealand show iridium increases of about 30, 160, and 20 times, respectively, above the background level at precisely the time of the Cretaceous-Tertiary extinctions, 65 million years ago. Reasons are given to indicate that this iridium is of extraterrestrial origin, but did not come from a nearby supernova. A hypothesis is suggested which accounts for the extinctions and the iridium observations. Impact of a large earth-crossing asteroid would inject about 60 times the object's mass into the atmosphere as pulverized rock; a fraction of this dust would stay in the stratosphere for several years and be distributed worldwide. The resulting darkness would suppress photosynthesis, and the expected biological consequences match quite closely the extinctions observed in the paleontological record. One prediction of this hypothesis has been verified: the chemical composition of the boundary clay, which is thought to come from the stratospheric dust, is markedly different from that of clay mixed with the Cretaceous and Tertiary limestones, which are chemically similar to each other. Four different independent estimates of the diameter of the asteroid give values that lie in the range 10 ± 4 kilometers.", "database": ["astronomy", "general"], "keywords": [], "year": "1980", "doctype": "article", "citation_count": 2180, "domain_category": "planetary_science", "abstract_clean": "Platinum metals are depleted in the earth's crust relative to their cosmic abundance; concentrations of these elements in deep-sea sediments may thus indicate influxes of extraterrestrial material. Deep-sea limestones exposed in Italy, Denmark, and New Zealand show iridium increases of about 30, 160, and 20 times, respectively, above the background level at precisely the time of the Cretaceous-Tertiary extinctions, 65 million years ago. Reasons are given to indicate that this iridium is of extraterrestrial origin, but did not come from a nearby supernova. A hypothesis is suggested which accounts for the extinctions and the iridium observations. Impact of a large earth-crossing asteroid would inject about 60 times the object's mass into the atmosphere as pulverized rock; a fraction of this dust would stay in the stratosphere for several years and be distributed worldwide. The resulting darkness would suppress photosynthesis, and the expected biological consequences match quite closely the extinctions observed in the paleontological record. One prediction of this hypothesis has been verified: the chemical composition of the boundary clay, which is thought to come from the stratospheric dust, is markedly different from that of clay mixed with the Cretaceous and Tertiary limestones, which are chemically similar to each other. Four different independent estimates of the diameter of the asteroid give values that lie in the range 10 ± 4 kilometers."}
{"bibcode": "2008Natur.451..293G", "title": "An Earth-system perspective of the global nitrogen cycle", "abstract": "With humans having an increasing impact on the planet, the interactions between the nitrogen cycle, the carbon cycle and climate are expected to become an increasingly important determinant of the Earth system.", "database": ["astronomy", "general"], "keywords": [], "year": "2008", "doctype": "article", "citation_count": 2174, "domain_category": "planetary_science", "abstract_clean": "With humans having an increasing impact on the planet, the interactions between the nitrogen cycle, the carbon cycle and climate are expected to become an increasingly important determinant of the Earth system."}
{"bibcode": "2001Sci...293..683S", "title": "Calibration of the Lutetium-Hafnium Clock", "abstract": "Well-defined constants of radioactive decay are the cornerstone of geochronology and the use of radiogenic isotopes to constrain the time scales and mechanisms of planetary differentiation. Four new determinations of the lutetium-176 decay constant (λ<SUP>176</SUP>Lu) made by calibration against the uranium-lead decay schemes yield a mean value of 1.865 +/- 0.015 × 10<SUP>-11</SUP> year<SUP>-1</SUP>, in agreement with the two most recent decay-counting experiments. Lutetium-hafnium ages that are based on the previously used λ<SUP>176</SUP>Lu of 1.93 × 10<SUP>-11</SUP> to 1.94 × 10<SUP>-11</SUP> year<SUP>-1</SUP> are thus ~4% too young, and the initial hafnium isotope compositions of some of Earth's oldest minerals and rocks become less radiogenic relative to bulk undifferentiated Earth when calculated using the new decay constant. The existence of strongly unradiogenic hafnium in Early Archean and Hadean zircons implies that enriched crustal reservoirs existed on Earth by 4.3 billion years ago and persisted for 200 million years or more. Hence, current models of early terrestrial differentiation need revision.", "database": ["astronomy", "physics"], "keywords": ["GEOCHEM PHYS"], "year": "2001", "doctype": "article", "citation_count": 2153, "domain_category": "planetary_science", "abstract_clean": "Well-defined constants of radioactive decay are the cornerstone of geochronology and the use of radiogenic isotopes to constrain the time scales and mechanisms of planetary differentiation. Four new determinations of the lutetium-176 decay constant (λ176Lu) made by calibration against the uranium-lead decay schemes yield a mean value of 1.865 +/- 0.015 × 10-11 year-1, in agreement with the two most recent decay-counting experiments. Lutetium-hafnium ages that are based on the previously used λ176Lu of 1.93 × 10-11 to 1.94 × 10-11 year-1 are thus ~4% too young, and the initial hafnium isotope compositions of some of Earth's oldest minerals and rocks become less radiogenic relative to bulk undifferentiated Earth when calculated using the new decay constant. The existence of strongly unradiogenic hafnium in Early Archean and Hadean zircons implies that enriched crustal reservoirs existed on Earth by 4.3 billion years ago and persisted for 200 million years or more. Hence, current models of early terrestrial differentiation need revision."}
{"bibcode": "1986Natur.324..446B", "title": "A hierarchical O(N log N) force-calculation algorithm", "abstract": "Until recently the gravitational N-body problem has been modelled numerically either by direct integration, in which the computation needed increases as N<SUP>2</SUP>, or by an iterative potential method in which the number of operations grows as N log N. Here we describe a novel method of directly calculating the force on N bodies that grows only as N log N. The technique uses a tree-structured hierarchical subdivision of space into cubic cells, each of which is recursively divided into eight subcells whenever more than one particle is found to occupy the same cell. This tree is constructed anew at every time step, avoiding ambiguity and tangling. Advantages over potential-solving codes are: accurate local interactions; freedom from geometrical assumptions and restrictions; and applicability to a wide class of systems, including (proto-)planetary, stellar, galactic and cosmological ones. Advantages over previous hierarchical tree-codes include simplicity and the possibility of rigorous analysis of error. Although we concentrate here on stellar dynamical applications, our techniques of efficiently handling a large number of long-range interactions and concentrating computational effort where most needed have potential applications in other areas of astrophysics as well.", "database": ["astronomy", "physics", "general"], "keywords": ["Computational Astrophysics", "Many Body Problem", "Numerical Integration", "Stellar Motions", "Algorithms", "Hierarchies", "Physics (General)"], "year": "1986", "doctype": "article", "citation_count": 2089, "domain_category": "planetary_science", "abstract_clean": "Until recently the gravitational N-body problem has been modelled numerically either by direct integration, in which the computation needed increases as N2, or by an iterative potential method in which the number of operations grows as N log N. Here we describe a novel method of directly calculating the force on N bodies that grows only as N log N. The technique uses a tree-structured hierarchical subdivision of space into cubic cells, each of which is recursively divided into eight subcells whenever more than one particle is found to occupy the same cell. This tree is constructed anew at every time step, avoiding ambiguity and tangling. Advantages over potential-solving codes are: accurate local interactions; freedom from geometrical assumptions and restrictions; and applicability to a wide class of systems, including (proto-)planetary, stellar, galactic and cosmological ones. Advantages over previous hierarchical tree-codes include simplicity and the possibility of rigorous analysis of error. Although we concentrate here on stellar dynamical applications, our techniques of efficiently handling a large number of long-range interactions and concentrating computational effort where most needed have potential applications in other areas of astrophysics as well."}
{"bibcode": "2011AJ....142...72E", "title": "SDSS-III: Massive Spectroscopic Surveys of the Distant Universe, the Milky Way, and Extra-Solar Planetary Systems", "abstract": "Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II), SDSS-III is a program of four spectroscopic surveys on three scientific themes: dark energy and cosmological parameters, the history and structure of the Milky Way, and the population of giant planets around other stars. In keeping with SDSS tradition, SDSS-III will provide regular public releases of all its data, beginning with SDSS Data Release 8 (DR8), which was made public in 2011 January and includes SDSS-I and SDSS-II images and spectra reprocessed with the latest pipelines and calibrations produced for the SDSS-III investigations. This paper presents an overview of the four surveys that comprise SDSS-III. The Baryon Oscillation Spectroscopic Survey will measure redshifts of 1.5 million massive galaxies and Lyα forest spectra of 150,000 quasars, using the baryon acoustic oscillation feature of large-scale structure to obtain percent-level determinations of the distance scale and Hubble expansion rate at z &lt; 0.7 and at z ≈ 2.5. SEGUE-2, an already completed SDSS-III survey that is the continuation of the SDSS-II Sloan Extension for Galactic Understanding and Exploration (SEGUE), measured medium-resolution (R = λ/Δλ ≈ 1800) optical spectra of 118,000 stars in a variety of target categories, probing chemical evolution, stellar kinematics and substructure, and the mass profile of the dark matter halo from the solar neighborhood to distances of 100 kpc. APOGEE, the Apache Point Observatory Galactic Evolution Experiment, will obtain high-resolution (R ≈ 30,000), high signal-to-noise ratio (S/N &gt;= 100 per resolution element), H-band (1.51 μm &lt; λ &lt; 1.70 μm) spectra of 10<SUP>5</SUP> evolved, late-type stars, measuring separate abundances for ~15 elements per star and creating the first high-precision spectroscopic survey of all Galactic stellar populations (bulge, bar, disks, halo) with a uniform set of stellar tracers and spectral diagnostics. The Multi-object APO Radial Velocity Exoplanet Large-area Survey (MARVELS) will monitor radial velocities of more than 8000 FGK stars with the sensitivity and cadence (10-40 m s<SUP>-1</SUP>, ~24 visits per star) needed to detect giant planets with periods up to two years, providing an unprecedented data set for understanding the formation and dynamical evolution of giant planet systems. As of 2011 January, SDSS-III has obtained spectra of more than 240,000 galaxies, 29,000 z &gt;= 2.2 quasars, and 140,000 stars, including 74,000 velocity measurements of 2580 stars for MARVELS.", "database": ["astronomy"], "keywords": ["cosmology: observations", "Galaxy: evolution", "planets and satellites: detection", "surveys", "Astrophysics - Instrumentation and Methods for Astrophysics"], "year": "2011", "doctype": "article", "citation_count": 2028, "domain_category": "planetary_science", "abstract_clean": "Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II), SDSS-III is a program of four spectroscopic surveys on three scientific themes: dark energy and cosmological parameters, the history and structure of the Milky Way, and the population of giant planets around other stars. In keeping with SDSS tradition, SDSS-III will provide regular public releases of all its data, beginning with SDSS Data Release 8 (DR8), which was made public in 2011 January and includes SDSS-I and SDSS-II images and spectra reprocessed with the latest pipelines and calibrations produced for the SDSS-III investigations. This paper presents an overview of the four surveys that comprise SDSS-III. The Baryon Oscillation Spectroscopic Survey will measure redshifts of 1.5 million massive galaxies and Lyα forest spectra of 150,000 quasars, using the baryon acoustic oscillation feature of large-scale structure to obtain percent-level determinations of the distance scale and Hubble expansion rate at z < 0.7 and at z ≈ 2.5. SEGUE-2, an already completed SDSS-III survey that is the continuation of the SDSS-II Sloan Extension for Galactic Understanding and Exploration (SEGUE), measured medium-resolution (R = λ/Δλ ≈ 1800) optical spectra of 118,000 stars in a variety of target categories, probing chemical evolution, stellar kinematics and substructure, and the mass profile of the dark matter halo from the solar neighborhood to distances of 100 kpc. APOGEE, the Apache Point Observatory Galactic Evolution Experiment, will obtain high-resolution (R ≈ 30,000), high signal-to-noise ratio (S/N >= 100 per resolution element), H-band (1.51 μm < λ < 1.70 μm) spectra of 105 evolved, late-type stars, measuring separate abundances for ~15 elements per star and creating the first high-precision spectroscopic survey of all Galactic stellar populations (bulge, bar, disks, halo) with a uniform set of stellar tracers and spectral diagnostics. The Multi-object APO Radial Velocity Exoplanet Large-area Survey (MARVELS) will monitor radial velocities of more than 8000 FGK stars with the sensitivity and cadence (10-40 m s-1, ~24 visits per star) needed to detect giant planets with periods up to two years, providing an unprecedented data set for understanding the formation and dynamical evolution of giant planet systems. As of 2011 January, SDSS-III has obtained spectra of more than 240,000 galaxies, 29,000 z >= 2.2 quasars, and 140,000 stars, including 74,000 velocity measurements of 2580 stars for MARVELS."}
{"bibcode": "2010GeoJI.181....1D", "title": "Geologically current plate motions", "abstract": "We describe best-fitting angular velocities and MORVEL, a new closure-enforced set of angular velocities for the geologically current motions of 25 tectonic plates that collectively occupy 97 per cent of Earth's surface. Seafloor spreading rates and fault azimuths are used to determine the motions of 19 plates bordered by mid-ocean ridges, including all the major plates. Six smaller plates with little or no connection to the mid-ocean ridges are linked to MORVEL with GPS station velocities and azimuthal data. By design, almost no kinematic information is exchanged between the geologically determined and geodetically constrained subsets of the global circuit-MORVEL thus averages motion over geological intervals for all the major plates. Plate geometry changes relative to NUVEL-1A include the incorporation of Nubia, Lwandle and Somalia plates for the former Africa plate, Capricorn, Australia and Macquarie plates for the former Australia plate, and Sur and South America plates for the former South America plate. MORVEL also includes Amur, Philippine Sea, Sundaland and Yangtze plates, making it more useful than NUVEL-1A for studies of deformation in Asia and the western Pacific. Seafloor spreading rates are estimated over the past 0.78 Myr for intermediate and fast spreading centres and since 3.16 Ma for slow and ultraslow spreading centres. Rates are adjusted downward by 0.6-2.6mmyr<SUP>-1</SUP> to compensate for the several kilometre width of magnetic reversal zones. Nearly all the NUVEL-1A angular velocities differ significantly from the MORVEL angular velocities. The many new data, revised plate geometries, and correction for outward displacement thus significantly modify our knowledge of geologically current plate motions. MORVEL indicates significantly slower 0.78-Myr-average motion across the Nazca-Antarctic and Nazca-Pacific boundaries than does NUVEL-1A, consistent with a progressive slowdown in the eastward component of Nazca plate motion since 3.16 Ma. It also indicates that motions across the Caribbean-North America and Caribbean-South America plate boundaries are twice as fast as given by NUVEL-1A. Summed, least-squares differences between angular velocities estimated from GPS and those for MORVEL, NUVEL-1 and NUVEL-1A are, respectively, 260 per cent larger for NUVEL-1 and 50 per cent larger for NUVEL-1A than for MORVEL, suggesting that MORVEL more accurately describes historically current plate motions. Significant differences between geological and GPS estimates of Nazca plate motion and Arabia-Eurasia and India-Eurasia motion are reduced but not eliminated when using MORVEL instead of NUVEL-1A, possibly indicating that changes have occurred in those plate motions since 3.16 Ma. The MORVEL and GPS estimates of Pacific-North America plate motion in western North America differ by only 2.6 +/- 1.7mmyr<SUP>-1</SUP>, ~25 per cent smaller than for NUVEL-1A. The remaining difference for this plate pair, assuming there are no unrecognized systematic errors and no measurable change in Pacific-North America motion over the past 1-3 Myr, indicates deformation of one or more plates in the global circuit. Tests for closure of six three-plate circuits indicate that two, Pacific-Cocos-Nazca and Sur-Nubia-Antarctic, fail closure, with respective linear velocities of non-closure of 14 +/- 5 and 3 +/- 1mmyr<SUP>-1</SUP> (95 per cent confidence limits) at their triple junctions. We conclude that the rigid plate approximation continues to be tremendously useful, but-absent any unrecognized systematic errors-the plates deform measurably, possibly by thermal contraction and wide plate boundaries with deformation rates near or beneath the level of noise in plate kinematic data.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Plate motions; Planetary tectonics"], "year": "2010", "doctype": "article", "citation_count": 1998, "domain_category": "planetary_science", "abstract_clean": "We describe best-fitting angular velocities and MORVEL, a new closure-enforced set of angular velocities for the geologically current motions of 25 tectonic plates that collectively occupy 97 per cent of Earth's surface. Seafloor spreading rates and fault azimuths are used to determine the motions of 19 plates bordered by mid-ocean ridges, including all the major plates. Six smaller plates with little or no connection to the mid-ocean ridges are linked to MORVEL with GPS station velocities and azimuthal data. By design, almost no kinematic information is exchanged between the geologically determined and geodetically constrained subsets of the global circuit-MORVEL thus averages motion over geological intervals for all the major plates. Plate geometry changes relative to NUVEL-1A include the incorporation of Nubia, Lwandle and Somalia plates for the former Africa plate, Capricorn, Australia and Macquarie plates for the former Australia plate, and Sur and South America plates for the former South America plate. MORVEL also includes Amur, Philippine Sea, Sundaland and Yangtze plates, making it more useful than NUVEL-1A for studies of deformation in Asia and the western Pacific. Seafloor spreading rates are estimated over the past 0.78 Myr for intermediate and fast spreading centres and since 3.16 Ma for slow and ultraslow spreading centres. Rates are adjusted downward by 0.6-2.6mmyr-1 to compensate for the several kilometre width of magnetic reversal zones. Nearly all the NUVEL-1A angular velocities differ significantly from the MORVEL angular velocities. The many new data, revised plate geometries, and correction for outward displacement thus significantly modify our knowledge of geologically current plate motions. MORVEL indicates significantly slower 0.78-Myr-average motion across the Nazca-Antarctic and Nazca-Pacific boundaries than does NUVEL-1A, consistent with a progressive slowdown in the eastward component of Nazca plate motion since 3.16 Ma. It also indicates that motions across the Caribbean-North America and Caribbean-South America plate boundaries are twice as fast as given by NUVEL-1A. Summed, least-squares differences between angular velocities estimated from GPS and those for MORVEL, NUVEL-1 and NUVEL-1A are, respectively, 260 per cent larger for NUVEL-1 and 50 per cent larger for NUVEL-1A than for MORVEL, suggesting that MORVEL more accurately describes historically current plate motions. Significant differences between geological and GPS estimates of Nazca plate motion and Arabia-Eurasia and India-Eurasia motion are reduced but not eliminated when using MORVEL instead of NUVEL-1A, possibly indicating that changes have occurred in those plate motions since 3.16 Ma. The MORVEL and GPS estimates of Pacific-North America plate motion in western North America differ by only 2.6 +/- 1.7mmyr-1, ~25 per cent smaller than for NUVEL-1A. The remaining difference for this plate pair, assuming there are no unrecognized systematic errors and no measurable change in Pacific-North America motion over the past 1-3 Myr, indicates deformation of one or more plates in the global circuit. Tests for closure of six three-plate circuits indicate that two, Pacific-Cocos-Nazca and Sur-Nubia-Antarctic, fail closure, with respective linear velocities of non-closure of 14 +/- 5 and 3 +/- 1mmyr-1 (95 per cent confidence limits) at their triple junctions. We conclude that the rigid plate approximation continues to be tremendously useful, but-absent any unrecognized systematic errors-the plates deform measurably, possibly by thermal contraction and wide plate boundaries with deformation rates near or beneath the level of noise in plate kinematic data."}
{"bibcode": "1993ApJ...405..538B", "title": "Spectral Evolution of Stellar Populations Using Isochrone Synthesis", "abstract": "We combine the photometric model of isochrone synthesis recently published by Chariot and Bruzual (1991) with an updated library of stellar spectra to predict the spectral evolution of stellar populations with solar metallicity. The library of spectra assembled here supersedes other existing libraries by its spectral range, its complete coverage of the color-magnitude diagram, and its inclusion of observed near-infrared spectra out to 2.56 micron. Also, the spectra are distributed on the stellar evolutionary tracks using optical/near-infrared color calibrations, as an improvement over models that used a single color of the effective temperature of the stars alone. The spectroscopic results obtained here confirm and extend the previous photometric predictions of the isochrone synthesis models while including the recent revision of evolutionary tracks for stars between 1.3 and 2.5 solar masses by their authors.", "database": ["astronomy"], "keywords": ["Astronomical Models", "Astronomical Spectroscopy", "Stellar Evolution", "Stellar Spectra", "Astronomical Photometry", "Color-Magnitude Diagram", "Far Ultraviolet Radiation", "Metallicity", "Near Infrared Radiation", "Planetary Nebulae", "Red Shift", "Sun", "White Dwarf Stars", "Astrophysics", "GALAXIES: LUMINOSITY FUNCTION", "MASS FUNCTION", "GALAXIES: STELLAR CONTENT", "STARS: EVOLUTION", "ULTRAVIOLET: GALAXIES"], "year": "1993", "doctype": "article", "citation_count": 1982, "domain_category": "planetary_science", "abstract_clean": "We combine the photometric model of isochrone synthesis recently published by Chariot and Bruzual (1991) with an updated library of stellar spectra to predict the spectral evolution of stellar populations with solar metallicity. The library of spectra assembled here supersedes other existing libraries by its spectral range, its complete coverage of the color-magnitude diagram, and its inclusion of observed near-infrared spectra out to 2.56 micron. Also, the spectra are distributed on the stellar evolutionary tracks using optical/near-infrared color calibrations, as an improvement over models that used a single color of the effective temperature of the stars alone. The spectroscopic results obtained here confirm and extend the previous photometric predictions of the isochrone synthesis models while including the recent revision of evolutionary tracks for stars between 1.3 and 2.5 solar masses by their authors."}
{"bibcode": "2004AREPS..32..111P", "title": "Global Glacial Isostasy and the Surface of the Ice-Age Earth: The ICE-5G (VM2) Model and GRACE", "abstract": "The 100 kyr quasiperiodic variation of continental ice cover, which has been a persistent feature of climate system evolution throughout the most recent 900 kyr of Earth history, has occurred as a consequence of changes in the seasonal insolation regime forced by the influence of gravitational n-body effects in the Solar System on the geometry of Earth's orbit around the Sun. The impacts of the changing surface ice load upon both Earth's shape and gravitational field, as well as upon sea-level history, have come to be measurable using a variety of geological and geophysical techniques. These observations are invertible to obtain useful information on both the internal viscoelastic structure of the solid Earth and on the detailed spatiotemporal characteristics of glaciation history. This review focuses upon the most recent advances that have been achieved in each of these areas, advances that have proven to be central to the construction of the refined model of the global process of glacial isostatic adjustment, denoted ICE-5G (VM2). A significant test of this new global model will be provided by the global measurement of the time dependence of the gravity field of the planet that will be delivered by the GRACE satellite system that is now in space.", "database": ["astronomy", "earth science"], "keywords": [], "year": "2004", "doctype": "article", "citation_count": 1973, "domain_category": "planetary_science", "abstract_clean": "The 100 kyr quasiperiodic variation of continental ice cover, which has been a persistent feature of climate system evolution throughout the most recent 900 kyr of Earth history, has occurred as a consequence of changes in the seasonal insolation regime forced by the influence of gravitational n-body effects in the Solar System on the geometry of Earth's orbit around the Sun. The impacts of the changing surface ice load upon both Earth's shape and gravitational field, as well as upon sea-level history, have come to be measurable using a variety of geological and geophysical techniques. These observations are invertible to obtain useful information on both the internal viscoelastic structure of the solid Earth and on the detailed spatiotemporal characteristics of glaciation history. This review focuses upon the most recent advances that have been achieved in each of these areas, advances that have proven to be central to the construction of the refined model of the global process of glacial isostatic adjustment, denoted ICE-5G (VM2). A significant test of this new global model will be provided by the global measurement of the time dependence of the gravity field of the planet that will be delivered by the GRACE satellite system that is now in space."}
{"bibcode": "1974SSRv...16..527H", "title": "Light scattering in planetary atmospheres", "abstract": "This paper reviews scattering theory required for analysis of light reflected by planetary atmospheres. Section 1 defines the radiative quantities which are observed. Section 2 demonstrates the dependence of single-scattered radiation on the physical properties of the scatterers. Section 3 describes several methods to compute the effects of multiple scattering on the reflected light.", "database": ["astronomy"], "keywords": ["Atmospheric Optics", "Atmospheric Scattering", "Light Scattering", "Optical Reflection", "Planetary Atmospheres", "Albedo", "Angular Distribution", "Integral Equations", "Invariant Imbeddings", "Mie Scattering", "Monte Carlo Method", "Particle Size Distribution", "Polarization Characteristics", "Rayleigh Scattering", "Refractivity", "Geophysics"], "year": "1974", "doctype": "article", "citation_count": 1968, "domain_category": "planetary_science", "abstract_clean": "This paper reviews scattering theory required for analysis of light reflected by planetary atmospheres. Section 1 defines the radiative quantities which are observed. Section 2 demonstrates the dependence of single-scattered radiation on the physical properties of the scatterers. Section 3 describes several methods to compute the effects of multiple scattering on the reflected light."}
{"bibcode": "1979ApJS...41..513M", "title": "The Initial Mass Function and Stellar Birthrate in the Solar Neighborhood", "abstract": "The paper examines the initial mass function (IMF) and time history of the stellar birthrate in the solar neighborhood under the assumption of a time-independent IMF over the lifetime of the galactic disk. All relevant observational quantities and the associated uncertainties are discussed. Also discussed are IMFs derived for a range of birthrate histories consistent with the continuity constraint. Finally, the study summarizes observational and theoretical arguments concerning the basic assumptions that the IMF is time independent and a continuous function of mass.", "database": ["astronomy"], "keywords": ["Cosmology", "Solar System", "Stellar Evolution", "Stellar Mass", "Gas Density", "Main Sequence Stars", "Milky Way Galaxy", "Nuclear Fusion", "Planetary Nebulae", "Stellar Luminosity", "Stellar Mass Accretion", "Stellar Mass Ejection", "Supernovae", "White Dwarf Stars", "Astrophysics"], "year": "1979", "doctype": "article", "citation_count": 1960, "domain_category": "planetary_science", "abstract_clean": "The paper examines the initial mass function (IMF) and time history of the stellar birthrate in the solar neighborhood under the assumption of a time-independent IMF over the lifetime of the galactic disk. All relevant observational quantities and the associated uncertainties are discussed. Also discussed are IMFs derived for a range of birthrate histories consistent with the continuity constraint. Finally, the study summarizes observational and theoretical arguments concerning the basic assumptions that the IMF is time independent and a continuous function of mass."}
{"bibcode": "2002ApJ...580L.171M", "title": "Analytic Light Curves for Planetary Transit Searches", "abstract": "We present exact analytic formulae for the eclipse of a star described by quadratic or nonlinear limb darkening. In the limit that the planet radius is less than a tenth of the stellar radius, we show that the exact light curve can be well approximated by assuming the region of the star blocked by the planet has constant surface brightness. We apply these results to the Hubble Space Telescope observations of HD 209458, showing that the ratio of the planetary to stellar radii is 0.1207+/-0.0003. These formulae give a fast and accurate means of computing light curves using limb-darkening coefficients from model atmospheres that should aid in the detection, simulation, and parameter fitting of planetary transits.", "database": ["astronomy"], "keywords": ["Stars: Binaries: Eclipsing", "Eclipses", "Occultations", "Stars: Planetary Systems", "Astrophysics"], "year": "2002", "doctype": "article", "citation_count": 1939, "domain_category": "planetary_science", "abstract_clean": "We present exact analytic formulae for the eclipse of a star described by quadratic or nonlinear limb darkening. In the limit that the planet radius is less than a tenth of the stellar radius, we show that the exact light curve can be well approximated by assuming the region of the star blocked by the planet has constant surface brightness. We apply these results to the Hubble Space Telescope observations of HD 209458, showing that the ratio of the planetary to stellar radii is 0.1207+/-0.0003. These formulae give a fast and accurate means of computing light curves using limb-darkening coefficients from model atmospheres that should aid in the detection, simulation, and parameter fitting of planetary transits."}
{"bibcode": "1993Icar..101..108K", "title": "Habitable Zones around Main Sequence Stars", "abstract": "A one-dimensional climate model is used to estimate the width of the habitable zone (HZ) around our Sun and around other main sequence stars. Our basic premise is that we are dealing with Earth-like planets with CO<SUB>2</SUB>/H<SUB>2</SUB>O/N<SUB>2</SUB> atmospheres and that habitability requires the presence of liquid water on the planet's surface. The inner edge of the HZ is determined in our model by loss of water via photolysis and hydrogen escape. The outer edge of the HZ is determined by the formation of CO<SUB>2</SUB> clouds, which cool a planet's surface by increasing its albedo and by lowering the convective lapse rate. Conservative estimates for these distances in our own Solar System are 0.95 and 1.37 AU, respectively; the actual width of the present HZ could be much greater. Between these two limits, climate stability is ensured by a feedback mechanism in which atmospheric CO<SUB>2</SUB> concentrations vary inversely with planetary surface temperature. The width of the HZ is slightly greater for planets that are larger than Earth and for planets which have higher N<SUB>2</SUB> partial pressures. The HZ evolves outward in time because the Sun increases in luminosity as it ages. A conservative estimate for the width of the 4.6-Gyr continuously habitable zone (CHZ) is 0.95 to 1.15 AU. <P />Stars later than F0 have main sequence lifetimes exceeding 2 Gyr and, so, are also potential candidates for harboring habitable planets. The HZ around an F star is larger and occurs farther out than for our Sun; the HZ around K and M stars is smaller and occurs farther in. Nevertheless, the widths of all of these HZs are approximately the same if distance is expressed on a logarithmic scale. A log distance scale is probably the appropriate scale for this problem because the planets in our own Solar System are spaced logarithmically and because the distance at which another star would be expected to form planets should be related to the star's mass. The width of the CHZ around other stars depends on the time that a planet is required to remain habitable and on whether a planet that is initially frozen can be thawed by modest increases in stellar luminosity. For a specified period of habitability, CHZs around K and M stars are wider (in log distance) than for our Sun because these stars evolve more slowly. Planets orbiting late K stars and M stars may not be habitable, however, because they can become trapped in synchronous rotation as a consequence of tidal damping. F stars have narrower (log distance) CHZ's than our Sun because they evolve more rapidly. Our results suggest that mid-to-early K stars should be considered along with G stars as optimal candidates in the search for extraterrestrial life.", "database": ["astronomy", "earth science"], "keywords": [], "year": "1993", "doctype": "article", "citation_count": 1906, "domain_category": "planetary_science", "abstract_clean": "A one-dimensional climate model is used to estimate the width of the habitable zone (HZ) around our Sun and around other main sequence stars. Our basic premise is that we are dealing with Earth-like planets with CO2/H2O/N2 atmospheres and that habitability requires the presence of liquid water on the planet's surface. The inner edge of the HZ is determined in our model by loss of water via photolysis and hydrogen escape. The outer edge of the HZ is determined by the formation of CO2 clouds, which cool a planet's surface by increasing its albedo and by lowering the convective lapse rate. Conservative estimates for these distances in our own Solar System are 0.95 and 1.37 AU, respectively; the actual width of the present HZ could be much greater. Between these two limits, climate stability is ensured by a feedback mechanism in which atmospheric CO2 concentrations vary inversely with planetary surface temperature. The width of the HZ is slightly greater for planets that are larger than Earth and for planets which have higher N2 partial pressures. The HZ evolves outward in time because the Sun increases in luminosity as it ages. A conservative estimate for the width of the 4.6-Gyr continuously habitable zone (CHZ) is 0.95 to 1.15 AU. Stars later than F0 have main sequence lifetimes exceeding 2 Gyr and, so, are also potential candidates for harboring habitable planets. The HZ around an F star is larger and occurs farther out than for our Sun; the HZ around K and M stars is smaller and occurs farther in. Nevertheless, the widths of all of these HZs are approximately the same if distance is expressed on a logarithmic scale. A log distance scale is probably the appropriate scale for this problem because the planets in our own Solar System are spaced logarithmically and because the distance at which another star would be expected to form planets should be related to the star's mass. The width of the CHZ around other stars depends on the time that a planet is required to remain habitable and on whether a planet that is initially frozen can be thawed by modest increases in stellar luminosity. For a specified period of habitability, CHZs around K and M stars are wider (in log distance) than for our Sun because these stars evolve more slowly. Planets orbiting late K stars and M stars may not be habitable, however, because they can become trapped in synchronous rotation as a consequence of tidal damping. F stars have narrower (log distance) CHZ's than our Sun because they evolve more rapidly. Our results suggest that mid-to-early K stars should be considered along with G stars as optimal candidates in the search for extraterrestrial life."}
{"bibcode": "1971JGR....76.3534B", "title": "Large-amplitude Alfvén waves in the interplanetary medium, 2", "abstract": "An extensive study of the dynamic nonshock properties of the microscale fluctuations (scale lengths of 0.01 AU and less) in the interplanetary medium was made by using plasma and magnetic field data from Mariner 5 (Venus 1967). The observational results of the study are: (1) Large-amplitude, nonsinusoidal Alfvén waves propagating outward from the sun with a broad wavelength range from 10<SUP>3</SUP> to 5×10<SUP>6</SUP> km dominate the microscale structure at least 50% of the time; the waves frequently have energy densities comparable both to the unperturbed magnetic field energy density and to the thermal energy density. (2) The purest examples of these outwardly propagating Alfvén waves occur in high-velocity solar wind streams and on their trailing edges (where the velocity decreases slowly with time). In low-velocity regions Alfvén waves are also outwardly propagating but usually have smaller amplitudes than in the fast streams and tend to be less pure in the sense that they are more strongly intermixed with structures of a non-Alfvénic and possibly static nature. (3) The largest amplitude Alfvénic fluctuations are found in the compression regions at the leading edges of high-velocity streams where the velocity increases rapidly with time; these regions may contain significant amounts of inwardly propagating or non-Alfvénic wave modes. (4) Power spectra of the interplanetary magnetic field in the frequency range from 1/(107 min) to 1/(25.2 sec) have frequency dependencies of ƒ<SUP>-1.5</SUP> to ƒ<SUP>-2.2</SUP> the spectra with slower falloffs tend to be associated with higher temperature regions. (5) The microscale magnetic field fluctuations have on the average a 5∶4∶1 power anisotropy in an orthogonal coodinate system whose axes are (e<SUB>B</SUB>×e<SUB>R</SUB>, e<SUB>B</SUB>× (e<SUB>B</SUB> ×e<SUB>R</SUB>), e<SUB>B</SUB>), where e<SUB>B</SUB> is a unit vector in the average field direction and e<SUB>R</SUB> is a unit vector radially away from the sun; this anisotropy tends to be strongest (6∶3∶1) in the compression regions at the leading edges of high-velocity streams. (6) Presumably magnetoacoustic wave modes occur, but they have not been identified and, if present, have a small average power of the order of 10% or less of that in the Alfvén mode.These observations are organized on the basis of a model of the solar wind velocity structure. Most Alfvén waves in the interplanetary medium are likely the undamped remnants of waves generated at or near the sun. The high level of wave activity in high-velocity high-temperature streams can be interpreted as evidence for the extensive heating of these streams by wave damping near the sun. The highest level of Alfvénic wave activity in the compression regions at the leading edges of high-velocity streams may be due to either the amplification of ambient Alfvén waves in high-velocity streams as they are swept into the compression regions or the fresh generation of waves in these regions by the stream-stream interactions. The observed absence of the magnetoacoustic modes is evidence for their strong damping. The e<SUB>B</SUB>×e<SUB>R</SUB> anisotropy is viewed as due to the partial conversion of the Alfvén waves to the damped magnetoacoustic modes as they are convected away from the sun; this process continually transfers energy from the microscale field fluctuations to the thermalized solar wind plasma.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Particles and Fields in Interplanetary Space: Solar-wind magnetic fields", "Particles and Fields in Interplanetary Space: Solar-wind plasma"], "year": "1971", "doctype": "article", "citation_count": 1771, "domain_category": "planetary_science", "abstract_clean": "An extensive study of the dynamic nonshock properties of the microscale fluctuations (scale lengths of 0.01 AU and less) in the interplanetary medium was made by using plasma and magnetic field data from Mariner 5 (Venus 1967). The observational results of the study are: (1) Large-amplitude, nonsinusoidal Alfvén waves propagating outward from the sun with a broad wavelength range from 103 to 5×106 km dominate the microscale structure at least 50% of the time; the waves frequently have energy densities comparable both to the unperturbed magnetic field energy density and to the thermal energy density. (2) The purest examples of these outwardly propagating Alfvén waves occur in high-velocity solar wind streams and on their trailing edges (where the velocity decreases slowly with time). In low-velocity regions Alfvén waves are also outwardly propagating but usually have smaller amplitudes than in the fast streams and tend to be less pure in the sense that they are more strongly intermixed with structures of a non-Alfvénic and possibly static nature. (3) The largest amplitude Alfvénic fluctuations are found in the compression regions at the leading edges of high-velocity streams where the velocity increases rapidly with time; these regions may contain significant amounts of inwardly propagating or non-Alfvénic wave modes. (4) Power spectra of the interplanetary magnetic field in the frequency range from 1/(107 min) to 1/(25.2 sec) have frequency dependencies of ƒ-1.5 to ƒ-2.2 the spectra with slower falloffs tend to be associated with higher temperature regions. (5) The microscale magnetic field fluctuations have on the average a 5∶4∶1 power anisotropy in an orthogonal coodinate system whose axes are (eB×eR, eB× (eB ×eR), eB), where eB is a unit vector in the average field direction and eR is a unit vector radially away from the sun; this anisotropy tends to be strongest (6∶3∶1) in the compression regions at the leading edges of high-velocity streams. (6) Presumably magnetoacoustic wave modes occur, but they have not been identified and, if present, have a small average power of the order of 10% or less of that in the Alfvén mode.These observations are organized on the basis of a model of the solar wind velocity structure. Most Alfvén waves in the interplanetary medium are likely the undamped remnants of waves generated at or near the sun. The high level of wave activity in high-velocity high-temperature streams can be interpreted as evidence for the extensive heating of these streams by wave damping near the sun. The highest level of Alfvénic wave activity in the compression regions at the leading edges of high-velocity streams may be due to either the amplification of ambient Alfvén waves in high-velocity streams as they are swept into the compression regions or the fresh generation of waves in these regions by the stream-stream interactions. The observed absence of the magnetoacoustic modes is evidence for their strong damping. The eB×eR anisotropy is viewed as due to the partial conversion of the Alfvén waves to the damped magnetoacoustic modes as they are convected away from the sun; this process continually transfers energy from the microscale field fluctuations to the thermalized solar wind plasma."}
{"bibcode": "1984ApJS...54..335I", "title": "Supernovae of type I as end products of the evolution of binaries with components of moderate initial mass.", "abstract": "Formation frequencies of binary systems which may become Type I supernovae are estimated. Presupernova systems consist of a CO or He degenerate dwarf and a (potential) mass donor (main-sequence star = MS; low-mass red giant = RG; asymptotic giant branch star = AGB; CO or He degenerate dwarf = CODD or HeDD). Mass transfer is driven by nuclear evolution (E), capture from wind (W), a magnetic stellar wind (MSW), or gravitational wave radiation (GWR). For several scenarios, the composition of accretor, nature of donor, driving mechanism, and formation frequency (in 10<SUP>-3</SUP> yr<SUP>-1</SUP> per 10<SUP>10</SUP> L_sun; in the B band), respectively, are the following: (1) CO, RG, E, 10<SUP>-2</SUP>- 10<SUP>-3</SUP>; (2) CO, AGB, W, 4; (3) CO, MS, MSW, 2; (4) He, MS, MSW, 2; (5) CO or He, near-MS, E+MSW, 3; (6) CO, CODD, GWR, 8; (7) CO, HeDD, GWR, 1; (8) He, HeDD, GWR, 5. The galactic Type I supernova frequency is 10.", "database": ["astronomy"], "keywords": ["Binary Stars", "Stellar Evolution", "Stellar Mass", "Supernovae", "Dwarf Stars", "Planetary Nebulae", "Red Giant Stars", "Stellar Cores", "Stellar Envelopes", "Stellar Mass Accretion", "Stellar Winds", "X Ray Sources", "Astrophysics"], "year": "1984", "doctype": "article", "citation_count": 1753, "domain_category": "planetary_science", "abstract_clean": "Formation frequencies of binary systems which may become Type I supernovae are estimated. Presupernova systems consist of a CO or He degenerate dwarf and a (potential) mass donor (main-sequence star = MS; low-mass red giant = RG; asymptotic giant branch star = AGB; CO or He degenerate dwarf = CODD or HeDD). Mass transfer is driven by nuclear evolution (E), capture from wind (W), a magnetic stellar wind (MSW), or gravitational wave radiation (GWR). For several scenarios, the composition of accretor, nature of donor, driving mechanism, and formation frequency (in 10-3 yr-1 per 1010 L_sun; in the B band), respectively, are the following: (1) CO, RG, E, 10-2- 10-3; (2) CO, AGB, W, 4; (3) CO, MS, MSW, 2; (4) He, MS, MSW, 2; (5) CO or He, near-MS, E+MSW, 3; (6) CO, CODD, GWR, 8; (7) CO, HeDD, GWR, 1; (8) He, HeDD, GWR, 5. The galactic Type I supernova frequency is 10."}
{"bibcode": "2010ApJS..190....1R", "title": "A Survey of Stellar Families: Multiplicity of Solar-type Stars", "abstract": "We present the results of a comprehensive assessment of companions to solar-type stars. A sample of 454 stars, including the Sun, was selected from the Hipparcos catalog with π&gt;40 mas, σ<SUB>π</SUB>/π &lt; 0.05, 0.5 &lt;= B - V &lt;= 1.0 (~F6-K3), and constrained by absolute magnitude and color to exclude evolved stars. These criteria are equivalent to selecting all dwarf and subdwarf stars within 25 pc with V-band flux between 0.1 and 10 times that of the Sun, giving us a physical basis for the term \"solar-type.\" New observational aspects of this work include surveys for (1) very close companions with long-baseline interferometry at the Center for High Angular Resolution Astronomy Array, (2) close companions with speckle interferometry, and (3) wide proper-motion companions identified by blinking multi-epoch archival images. In addition, we include the results from extensive radial-velocity monitoring programs and evaluate companion information from various catalogs covering many different techniques. The results presented here include four new common proper-motion companions discovered by blinking archival images. Additionally, the spectroscopic data searched reveal five new stellar companions. Our synthesis of results from many methods and sources results in a thorough evaluation of stellar and brown dwarf companions to nearby Sun-like stars. The overall observed fractions of single, double, triple, and higher-order systems are 56% ± 2%, 33% ± 2%, 8% ± 1%, and 3% ± 1%, respectively, counting all confirmed stellar and brown dwarf companions. If all candidate, i.e., unconfirmed, companions identified are found to be real, the percentages would change to 54% ± 2%, 34% ± 2%, 9% ± 2%, and 3% ± 1%, respectively. Our completeness analysis indicates that only a few undiscovered companions remain in this well-studied sample, implying that the majority (54% ± 2%) of solar-type stars are single, in contrast to the results of prior multiplicity studies. Our sample is large enough to enable a check of the multiplicity dependence on various physical parameters by analyzing appropriate subsamples. Bluer, more massive stars are seen as more likely to have companions than redder, less massive ones, consistent with the trend seen over the entire spectral range. Systems with larger interaction cross sections, i.e., those with more than two components or long orbital periods, are preferentially younger, suggesting that companions may be stripped over time by dynamical interactions. We confirm the planet-metallicity correlation (i.e., higher metallicity stars are more likely to host planets), but are unable to check it for brown dwarfs due to the paucity of such companions, implying that the brown dwarf desert extends over all separation regimes. We find no correlation between stellar companions and metallicity for B - V &lt; 0.625, but among the redder subset, metal-poor stars ([Fe/H] &lt;-0.3) are more likely to have companions with a 2.4σ significance. The orbital-period distribution of companions is unimodal and roughly log normal with a peak and median of about 300 years. The period-eccentricity relation shows the expected circularization for periods below 12 days, caused by tidal forces over the age of the Galaxy, followed by a roughly flat distribution. The mass-ratio distribution shows a preference for like-mass pairs, which occur more frequently in relatively close pairs. The fraction of planet hosts among single, binary, and multiple systems are statistically indistinguishable, suggesting that planets are as likely to form around single stars as they are around components of binary or multiple systems with sufficiently wide separations. This, along with the preference of long orbital periods among stellar systems, increases the space around stars conducive for planet formation, and perhaps life.", "database": ["astronomy"], "keywords": ["binaries: general", "planetary systems", "stars: solar-type", "stars: statistics", "surveys", "Astrophysics - Solar and Stellar Astrophysics"], "year": "2010", "doctype": "article", "citation_count": 1750, "domain_category": "planetary_science", "abstract_clean": "We present the results of a comprehensive assessment of companions to solar-type stars. A sample of 454 stars, including the Sun, was selected from the Hipparcos catalog with π>40 mas, σπ/π < 0.05, 0.5 <= B - V <= 1.0 (~F6-K3), and constrained by absolute magnitude and color to exclude evolved stars. These criteria are equivalent to selecting all dwarf and subdwarf stars within 25 pc with V-band flux between 0.1 and 10 times that of the Sun, giving us a physical basis for the term \"solar-type.\" New observational aspects of this work include surveys for (1) very close companions with long-baseline interferometry at the Center for High Angular Resolution Astronomy Array, (2) close companions with speckle interferometry, and (3) wide proper-motion companions identified by blinking multi-epoch archival images. In addition, we include the results from extensive radial-velocity monitoring programs and evaluate companion information from various catalogs covering many different techniques. The results presented here include four new common proper-motion companions discovered by blinking archival images. Additionally, the spectroscopic data searched reveal five new stellar companions. Our synthesis of results from many methods and sources results in a thorough evaluation of stellar and brown dwarf companions to nearby Sun-like stars. The overall observed fractions of single, double, triple, and higher-order systems are 56% ± 2%, 33% ± 2%, 8% ± 1%, and 3% ± 1%, respectively, counting all confirmed stellar and brown dwarf companions. If all candidate, i.e., unconfirmed, companions identified are found to be real, the percentages would change to 54% ± 2%, 34% ± 2%, 9% ± 2%, and 3% ± 1%, respectively. Our completeness analysis indicates that only a few undiscovered companions remain in this well-studied sample, implying that the majority (54% ± 2%) of solar-type stars are single, in contrast to the results of prior multiplicity studies. Our sample is large enough to enable a check of the multiplicity dependence on various physical parameters by analyzing appropriate subsamples. Bluer, more massive stars are seen as more likely to have companions than redder, less massive ones, consistent with the trend seen over the entire spectral range. Systems with larger interaction cross sections, i.e., those with more than two components or long orbital periods, are preferentially younger, suggesting that companions may be stripped over time by dynamical interactions. We confirm the planet-metallicity correlation (i.e., higher metallicity stars are more likely to host planets), but are unable to check it for brown dwarfs due to the paucity of such companions, implying that the brown dwarf desert extends over all separation regimes. We find no correlation between stellar companions and metallicity for B - V < 0.625, but among the redder subset, metal-poor stars ([Fe/H] <-0.3) are more likely to have companions with a 2.4σ significance. The orbital-period distribution of companions is unimodal and roughly log normal with a peak and median of about 300 years. The period-eccentricity relation shows the expected circularization for periods below 12 days, caused by tidal forces over the age of the Galaxy, followed by a roughly flat distribution. The mass-ratio distribution shows a preference for like-mass pairs, which occur more frequently in relatively close pairs. The fraction of planet hosts among single, binary, and multiple systems are statistically indistinguishable, suggesting that planets are as likely to form around single stars as they are around components of binary or multiple systems with sufficiently wide separations. This, along with the preference of long orbital periods among stellar systems, increases the space around stars conducive for planet formation, and perhaps life."}
{"bibcode": "1998JQSRT..60..883P", "title": "Submillimeter, millimeter and microwave spectral line catalog.", "abstract": "This paper describes a computer-accessible catalog of submillimeter, millimeter, and microwave spectral lines in the frequency range between 0 and 10,000 GHz (i.e. wavelengths longer than 30 μm). The catalog can be used as a planning guide or as an aid in the identification and analysis of observed spectral lines in the interstellar medium, the Earth's atmosphere, and the atmospheres of other planets. The information listed for each spectral line includes the frequency and its estimated error, the intensity, the lower state energy, and the quantum number assignment. The catalog is continuously updated and at present has information on 331 atomic and molecular species and includes a total of 1,845,866 lines. The catalog has been constructed by using theoretical least-squares fits of published spectral lines to accepted molecular models. The associated predictions and their estimated errors are based upon the resultant fitted parameters and their covariance. Future versions of this catalog will add more atoms and molecules and update the present listings as new data appear. The catalog is available on-line via anonymous FTP at spec.jpl.nasa.gov and on the world wide web at http://spec.jpl.nasa.gov.", "database": ["astronomy", "physics"], "keywords": ["Laboratory Spectra: Catalogues", "Laboratory Spectra: MM Spectra", "Laboratory Spectra: Sub-MM Spectra", "Laboratory Spectra: Microwave Spectra", "Atomic Spectra: Catalogues", "Atomic Spectra: MM Spectra", "Atomic Spectra: Sub-MM Spectra", "Atomic Spectra: Microwave Spectra", "Molecular Spectra: Catalogues", "Molecular Spectra: MM Spectra", "Molecular Spectra: Sub-MM Spectra", "Molecular Spectra: Microwave Spectra"], "year": "1998", "doctype": "article", "citation_count": 1731, "domain_category": "planetary_science", "abstract_clean": "This paper describes a computer-accessible catalog of submillimeter, millimeter, and microwave spectral lines in the frequency range between 0 and 10,000 GHz (i.e. wavelengths longer than 30 μm). The catalog can be used as a planning guide or as an aid in the identification and analysis of observed spectral lines in the interstellar medium, the Earth's atmosphere, and the atmospheres of other planets. The information listed for each spectral line includes the frequency and its estimated error, the intensity, the lower state energy, and the quantum number assignment. The catalog is continuously updated and at present has information on 331 atomic and molecular species and includes a total of 1,845,866 lines. The catalog has been constructed by using theoretical least-squares fits of published spectral lines to accepted molecular models. The associated predictions and their estimated errors are based upon the resultant fitted parameters and their covariance. Future versions of this catalog will add more atoms and molecules and update the present listings as new data appear. The catalog is available on-line via anonymous FTP at spec.jpl.nasa.gov and on the world wide web at http://spec.jpl.nasa.gov."}
{"bibcode": "2005JMoSt.742..215M", "title": "The Cologne Database for Molecular Spectroscopy, CDMS: a useful tool for astronomers and spectroscopists", "abstract": "The general features of the internet browser-accessible Cologne Database for Molecular Spectroscopy (CDMS) and recent developments in the CDMS are described in the present article. The database consists of several parts; among them is a catalog of transition frequencies from the radio-frequency to the far-infrared region covering atomic and molecular species that (may) occur in the interstellar or circumstellar medium or in planetary atmospheres. As of December 2004, 280 species are present in this catalog. The transition frequencies were predicted from fits of experimental data to established Hamiltonian models. We present some examples to demonstrate how the combination of various input data or a compact representation of the Hamiltonian can be beneficial for the prediction of the line frequencies.", "database": ["astronomy", "physics"], "keywords": [], "year": "2005", "doctype": "article", "citation_count": 1716, "domain_category": "planetary_science", "abstract_clean": "The general features of the internet browser-accessible Cologne Database for Molecular Spectroscopy (CDMS) and recent developments in the CDMS are described in the present article. The database consists of several parts; among them is a catalog of transition frequencies from the radio-frequency to the far-infrared region covering atomic and molecular species that (may) occur in the interstellar or circumstellar medium or in planetary atmospheres. As of December 2004, 280 species are present in this catalog. The transition frequencies were predicted from fits of experimental data to established Hamiltonian models. We present some examples to demonstrate how the combination of various input data or a compact representation of the Hamiltonian can be beneficial for the prediction of the line frequencies."}
{"bibcode": "2006SSRv..123..485G", "title": "The James Webb Space Telescope", "abstract": "The James Webb Space Telescope (JWST) is a large (6.6 m), cold (&lt;50 K), infrared (IR)-optimized space observatory that will be launched early in the next decade into orbit around the second Earth Sun Lagrange point. The observatory will have four instruments: a near-IR camera, a near-IR multiobject spectrograph, and a tunable filter imager will cover the wavelength range, 0.6 &lt; ; &lt; 5.0 μ m, while the mid-IR instrument will do both imaging and spectroscopy from 5.0 &lt; ; &lt; 29 μ m. The JWST science goals are divided into four themes. The key objective of The End of the Dark Ages: First Light and Reionization theme is to identify the first luminous sources to form and to determine the ionization history of the early universe. The key objective of The Assembly of Galaxies theme is to determine how galaxies and the dark matter, gas, stars, metals, morphological structures, and active nuclei within them evolved from the epoch of reionization to the present day. The key objective of The Birth of Stars and Protoplanetary Systems theme is to unravel the birth and early evolution of stars, from infall on to dust-enshrouded protostars to the genesis of planetary systems. The key objective of the Planetary Systems and the Origins of Life theme is to determine the physical and chemical properties of planetary systems including our own, and investigate the potential for the origins of life in those systems. Within these themes and objectives, we have derived representative astronomical observations. To enable these observations, JWST consists of a telescope, an instrument package, a spacecraft, and a sunshield. The telescope consists of 18 beryllium segments, some of which are deployed. The segments will be brought into optical alignment on-orbit through a process of periodic wavefront sensing and control. The instrument package contains the four science instruments and a fine guidance sensor. The spacecraft provides pointing, orbit maintenance, and communications. The sunshield provides passive thermal control. The JWST operations plan is based on that used for previous space observatories, and the majority of JWST observing time will be allocated to the international astronomical community through annual peer-reviewed proposal opportunities.", "database": ["astronomy"], "keywords": ["galaxies: formation", "infrared: general", "planetary systems", "space vehicles: instruments", "stars: formation", "Astrophysics"], "year": "2006", "doctype": "article", "citation_count": 1715, "domain_category": "planetary_science", "abstract_clean": "The James Webb Space Telescope (JWST) is a large (6.6 m), cold (<50 K), infrared (IR)-optimized space observatory that will be launched early in the next decade into orbit around the second Earth Sun Lagrange point. The observatory will have four instruments: a near-IR camera, a near-IR multiobject spectrograph, and a tunable filter imager will cover the wavelength range, 0.6 < ; < 5.0 μ m, while the mid-IR instrument will do both imaging and spectroscopy from 5.0 < ; < 29 μ m. The JWST science goals are divided into four themes. The key objective of The End of the Dark Ages: First Light and Reionization theme is to identify the first luminous sources to form and to determine the ionization history of the early universe. The key objective of The Assembly of Galaxies theme is to determine how galaxies and the dark matter, gas, stars, metals, morphological structures, and active nuclei within them evolved from the epoch of reionization to the present day. The key objective of The Birth of Stars and Protoplanetary Systems theme is to unravel the birth and early evolution of stars, from infall on to dust-enshrouded protostars to the genesis of planetary systems. The key objective of the Planetary Systems and the Origins of Life theme is to determine the physical and chemical properties of planetary systems including our own, and investigate the potential for the origins of life in those systems. Within these themes and objectives, we have derived representative astronomical observations. To enable these observations, JWST consists of a telescope, an instrument package, a spacecraft, and a sunshield. The telescope consists of 18 beryllium segments, some of which are deployed. The segments will be brought into optical alignment on-orbit through a process of periodic wavefront sensing and control. The instrument package contains the four science instruments and a fine guidance sensor. The spacecraft provides pointing, orbit maintenance, and communications. The sunshield provides passive thermal control. The JWST operations plan is based on that used for previous space observatories, and the majority of JWST observing time will be allocated to the international astronomical community through annual peer-reviewed proposal opportunities."}
{"bibcode": "2014PASP..126..398H", "title": "The K2 Mission: Characterization and Early Results", "abstract": "The K2 mission will make use of the Kepler spacecraft and its assets to expand upon Kepler's groundbreaking discoveries in the fields of exoplanets and astrophysics through new and exciting observations. K2 will use an innovative way of operating the spacecraft to observe target fields along the ecliptic for the next 2-3 years. Early science commissioning observations have shown an estimated photometric precision near 400 ppm in a single 30 minute observation, and a 6-hour photometric precision of 80 ppm (both at V=12). The K2 mission offers long-term, simultaneous optical observation of thousands of objects at a precision far better than is achievable from ground-based telescopes. Ecliptic fields will be observed for approximately 75-days enabling a unique exoplanet survey which fills the gaps in duration and sensitivity between the Kepler and TESS missions, and offers pre-launch exoplanet target identification for JWST transit spectroscopy. Astrophysics observations with K2 will include studies of young open clusters, bright stars, galaxies, supernovae, and asteroseismology.", "database": ["astronomy"], "keywords": ["Astrophysics - Instrumentation and Methods for Astrophysics", "Astrophysics - Earth and Planetary Astrophysics"], "year": "2014", "doctype": "article", "citation_count": 1714, "domain_category": "planetary_science", "abstract_clean": "The K2 mission will make use of the Kepler spacecraft and its assets to expand upon Kepler's groundbreaking discoveries in the fields of exoplanets and astrophysics through new and exciting observations. K2 will use an innovative way of operating the spacecraft to observe target fields along the ecliptic for the next 2-3 years. Early science commissioning observations have shown an estimated photometric precision near 400 ppm in a single 30 minute observation, and a 6-hour photometric precision of 80 ppm (both at V=12). The K2 mission offers long-term, simultaneous optical observation of thousands of objects at a precision far better than is achievable from ground-based telescopes. Ecliptic fields will be observed for approximately 75-days enabling a unique exoplanet survey which fills the gaps in duration and sensitivity between the Kepler and TESS missions, and offers pre-launch exoplanet target identification for JWST transit spectroscopy. Astrophysics observations with K2 will include studies of young open clusters, bright stars, galaxies, supernovae, and asteroseismology."}
{"bibcode": "1990AJ.....99..924B", "title": "A Survey for Circumstellar Disks around Young Stellar Objects", "abstract": "Continuum observations at 1.3 mm of 86 pre-main sequence stars in the Taurus-Auriga dark clouds show that 42% have detectable emission from small particles. The detected fraction is only slightly smaller for the weak-line and \"naked\" T Tauri stars than for classical T Tauris, indicating that the former stars often have circumstellar material. In both categories, the column densities of particles are too large to be compatible with spherical distributions of circumstellar matter -- the optical extinctions would be too large; the particles are almost certainly in spatially thin, circumstellar disks. Models of the spectral energy distributions from 10 to 1300 μm indicate that for the most part the disks are transparent at 1.3 mm, although the innermost (≪1 AU) regions are opaque even at millimeter wavelenths. The aggregate particle masses are between 10<SUP>-5</SUP> and 10<SUP>-2</SUP> M<SUB>⊙</SUB>. The disk mass does not decrease with increasing stellar age up to at least 10<SUP>7</SUP> years among the stars detected at 1.3 mm. There is some evidence for temperature evolution, in the sense that older disks are colder and less luminous. There is little correlation between disk mass and Hα equivalent width among the detected stars, suggesting that the Hα line is not by itself indicative of disk mass. Spectral indices for several sources between 1.3 and 2.7 mm suggest that the particle emissivities ɛ are weaker functions of frequency ν than is the usual case of interstellar grains. Particle growth via adhesion in the dense disks might explain this result. The typical disk has an angular momentum comparable to that generally accepted for the early solar nebula, but very little stored energy, almost five orders of magnitude smaller than that of the central star. Our results demonstrate that disks more massive than the minimum mass of the proto-solar system commonly accompany the birth of solar-mass stars and suggest that planetary systems are common in the Galaxy.", "database": ["astronomy"], "keywords": ["Molecular Clouds", "Pre-Main Sequence Stars", "Sky Surveys (Astronomy)", "Stellar Envelopes", "T Tauri Stars", "Auriga Constellation", "Computational Astrophysics", "Emission Spectra", "H Alpha Line", "Milky Way Galaxy", "Stellar Evolution", "Taurus Constellation", "Astrophysics", "STARS: PRE-MAIN-SEQUENCE", "STARS: CIRCUMSTELLAR SHELLS"], "year": "1990", "doctype": "article", "citation_count": 1709, "domain_category": "planetary_science", "abstract_clean": "Continuum observations at 1.3 mm of 86 pre-main sequence stars in the Taurus-Auriga dark clouds show that 42% have detectable emission from small particles. The detected fraction is only slightly smaller for the weak-line and \"naked\" T Tauri stars than for classical T Tauris, indicating that the former stars often have circumstellar material. In both categories, the column densities of particles are too large to be compatible with spherical distributions of circumstellar matter -- the optical extinctions would be too large; the particles are almost certainly in spatially thin, circumstellar disks. Models of the spectral energy distributions from 10 to 1300 μm indicate that for the most part the disks are transparent at 1.3 mm, although the innermost (≪1 AU) regions are opaque even at millimeter wavelenths. The aggregate particle masses are between 10-5 and 10-2 M⊙. The disk mass does not decrease with increasing stellar age up to at least 107 years among the stars detected at 1.3 mm. There is some evidence for temperature evolution, in the sense that older disks are colder and less luminous. There is little correlation between disk mass and Hα equivalent width among the detected stars, suggesting that the Hα line is not by itself indicative of disk mass. Spectral indices for several sources between 1.3 and 2.7 mm suggest that the particle emissivities ɛ are weaker functions of frequency ν than is the usual case of interstellar grains. Particle growth via adhesion in the dense disks might explain this result. The typical disk has an angular momentum comparable to that generally accepted for the early solar nebula, but very little stored energy, almost five orders of magnitude smaller than that of the central star. Our results demonstrate that disks more massive than the minimum mass of the proto-solar system commonly accompany the birth of solar-mass stars and suggest that planetary systems are common in the Galaxy."}
{"bibcode": "1980E&PSL..50..139J", "title": "Sm-Nd isotopic evolution of chondrites", "abstract": "The <SUP>143</SUP>Nd/ <SUP>144</SUP>Nd and <SUP>147</SUP>Sm/ <SUP>144</SUP>Nd ratios have been measured in five chondrites and the Juvinas achondrite. The range in <SUP>143</SUP>Nd/ <SUP>144</SUP>Nd for the analyzed meteorite samples is 5.3 ɛ-units (0.511673-0.511944) normalized to <SUP>150</SUP>Nd/ <SUP>142</SUP>Nd= 0.2096 . This is correlated with the variation of 4.2% in <SUP>147</SUP>Sm/ <SUP>144</SUP>Nd (0.1920-0.2000). Much of this spread is due to small-scale heterogeneities in the chondrites and does not appear to reflect the large-scale volumetric averages. It is shown that none of the samples deviate more than 0.5 ɛ-units from a 4.6-AE reference isochron and define an initial <SUP>143</SUP>Nd/ <SUP>144</SUP>Nd ratio at 4.6 AE of 0.505828 ± 9. Insofar as there is a range of values of <SUP>147</SUP>Sm/ <SUP>144</SUP>Nd there is no unique way of picking solar or average chondritic values. From these data we have selected a new set of self-consistent present-day reference values for CHUR (\"chondritic uniform reservoir\") of ( <SUP>143</SUP>Nd/ <SUP>144</SUP>Nd) <SUB>CHUR</SUB><SUP>0</SUP> = 0.511836and( <SUP>147</SUP>Sm/ <SUP>144</SUP>Nd) <SUB>CHUR</SUB><SUP>0</SUP> = 0.1967 . The new <SUP>147</SUP>Sm/ <SUP>144</SUP>Nd value is 1.6% higher than the previous value assigned to CHUR using the Juvinas data of Lugmair. This will cause a small but significant change in the CHUR evolution curve. Some terrestrial samples of Archean age show clear deviations from the new CHUR curve. If the CHUR curve is representative of undifferentiated mantle then it demonstrates that depleted sources were also tapped early in the Archean. Such a depleted layer may represent the early evolution of the source of present-day mid-ocean ridge basalts. There exists a variety of discrepancies with most earlier meteorite data which includes determination of all Nd isotopes and Sm/Nd ratios. These discrepancies require clarification in order to permit reliable interlaboratory comparisons. The new CHUR curve implies substantial changes in model ages for lunar rocks and thus also in the interpretation of early lunar chronology.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Chondrites", "Meteoritic Composition", "Neodymium Isotopes", "Radioactive Isotopes", "Samarium Isotopes", "Achondrites", "Basalt", "Lunar Evolution", "Lunar Rocks", "Planetary Evolution", "Rare Earth Elements", "METEORITES", "CHONDRITES", "EVOLUTION", "ISOTOPES", "NEODYMIUM", "SAMARIUM", "ISOTOPIC RATIOS", "ACHONDRITES", "JUVINAS METEORITE", "ANALYSIS", "DATA", "SAMPLES", "METEORITE", "PROCEDURE", "MURCHISON METEORITE", "ALLENDE METEORITE", "GUARENA METEORITE", "PEACE RIVER METEORITE", "ST. SEVERIN METEORITE", "MOON", "EARTH"], "year": "1980", "doctype": "article", "citation_count": 1689, "domain_category": "planetary_science", "abstract_clean": "The 143Nd/ 144Nd and 147Sm/ 144Nd ratios have been measured in five chondrites and the Juvinas achondrite. The range in 143Nd/ 144Nd for the analyzed meteorite samples is 5.3 ɛ-units (0.511673-0.511944) normalized to 150Nd/ 142Nd= 0.2096 . This is correlated with the variation of 4.2% in 147Sm/ 144Nd (0.1920-0.2000). Much of this spread is due to small-scale heterogeneities in the chondrites and does not appear to reflect the large-scale volumetric averages. It is shown that none of the samples deviate more than 0.5 ɛ-units from a 4.6-AE reference isochron and define an initial 143Nd/ 144Nd ratio at 4.6 AE of 0.505828 ± 9. Insofar as there is a range of values of 147Sm/ 144Nd there is no unique way of picking solar or average chondritic values. From these data we have selected a new set of self-consistent present-day reference values for CHUR (\"chondritic uniform reservoir\") of ( 143Nd/ 144Nd) CHUR0 = 0.511836and( 147Sm/ 144Nd) CHUR0 = 0.1967 . The new 147Sm/ 144Nd value is 1.6% higher than the previous value assigned to CHUR using the Juvinas data of Lugmair. This will cause a small but significant change in the CHUR evolution curve. Some terrestrial samples of Archean age show clear deviations from the new CHUR curve. If the CHUR curve is representative of undifferentiated mantle then it demonstrates that depleted sources were also tapped early in the Archean. Such a depleted layer may represent the early evolution of the source of present-day mid-ocean ridge basalts. There exists a variety of discrepancies with most earlier meteorite data which includes determination of all Nd isotopes and Sm/Nd ratios. These discrepancies require clarification in order to permit reliable interlaboratory comparisons. The new CHUR curve implies substantial changes in model ages for lunar rocks and thus also in the interpretation of early lunar chronology."}
{"bibcode": "1981PThPS..70...35H", "title": "Structure of the Solar Nebula, Growth and Decay of Magnetic Fields and Effects of Magnetic and Turbulent Viscosities on the Nebula", "abstract": "First, distributions of surface densities of dust materials and gases in a preplanetary solar nebula, which give a good fit to the distribution of the planetary mass, are presented and the over-all structure of this nebula, which is in thermal and gravitational equilibrium, is studied. Second, in order to see magnetic effect on the structure, electric conductivity of a gas ionized by cosmic rays and radioactivities contained in dust grains is estimated for each region of the nebula and, then, the growth and decay of seed magnetic fields, which are due to differential rotation of the nebula and to the Joule dissipation, respectively, are calculated. The results indicate that, in regions of the terrestrial planets, magnetic fields decay much faster than they grow and magnetic effects can be ignored, except for the outermost layers of very low density. This is not the case for regions of Uranus and Neptune where magnetic fields can be amplified to considerable extents. Third, the transport of angular momentum due to magnetic and mechanical turbulent viscosities and the resultant redistribution of surface density in the nebula are investigated. The results show that the density redistribution occurs, in general, in a direction to attain a distribution of surface density which has nearly the same r-dependence as that obtained from the present distribution of the planetary mass. This redistribution seems to be possible if it occurs at a formation stage of the nebula where the presence of large viscosities is expected. Finally, a comment is given on the initial condition of a collapsing interstellar cloud from which the solar nebula is formed at the end of the collapse.", "database": ["astronomy", "physics"], "keywords": [], "year": "1981", "doctype": "article", "citation_count": 1685, "domain_category": "planetary_science", "abstract_clean": "First, distributions of surface densities of dust materials and gases in a preplanetary solar nebula, which give a good fit to the distribution of the planetary mass, are presented and the over-all structure of this nebula, which is in thermal and gravitational equilibrium, is studied. Second, in order to see magnetic effect on the structure, electric conductivity of a gas ionized by cosmic rays and radioactivities contained in dust grains is estimated for each region of the nebula and, then, the growth and decay of seed magnetic fields, which are due to differential rotation of the nebula and to the Joule dissipation, respectively, are calculated. The results indicate that, in regions of the terrestrial planets, magnetic fields decay much faster than they grow and magnetic effects can be ignored, except for the outermost layers of very low density. This is not the case for regions of Uranus and Neptune where magnetic fields can be amplified to considerable extents. Third, the transport of angular momentum due to magnetic and mechanical turbulent viscosities and the resultant redistribution of surface density in the nebula are investigated. The results show that the density redistribution occurs, in general, in a direction to attain a distribution of surface density which has nearly the same r-dependence as that obtained from the present distribution of the planetary mass. This redistribution seems to be possible if it occurs at a formation stage of the nebula where the presence of large viscosities is expected. Finally, a comment is given on the initial condition of a collapsing interstellar cloud from which the solar nebula is formed at the end of the collapse."}
{"bibcode": "2012A&A...537A.146E", "title": "Grids of stellar models with rotation. I. Models from 0.8 to 120 M<SUB>⊙</SUB> at solar metallicity (Z = 0.014)", "abstract": "<BR /> Aims: Many topical astrophysical research areas, such as the properties of planet host stars, the nature of the progenitors of different types of supernovae and gamma ray bursts, and the evolution of galaxies, require complete and homogeneous sets of stellar models at different metallicities in order to be studied during the whole of cosmic history. We present here a first set of models for solar metallicity, where the effects of rotation are accounted for in a homogeneous way. <BR /> Methods: We computed a grid of 48 different stellar evolutionary tracks, both rotating and non-rotating, at Z = 0.014, spanning a wide mass range from 0.8 to 120 M<SUB>⊙</SUB>. For each of the stellar masses considered, electronic tables provide data for 400 stages along the evolutionary track and at each stage, a set of 43 physical data are given. These grids thus provide an extensive and detailed data basis for comparisons with the observations. The rotating models start on the zero-age main sequence (ZAMS) with a rotation rate υ<SUB>ini</SUB>/υ<SUB>crit</SUB> = 0.4. The evolution is computed until the end of the central carbon-burning phase, the early asymptotic giant branch (AGB) phase, or the core helium-flash for, respectively, the massive, intermediate, and both low and very low mass stars. The initial abundances are those deduced by Asplund and collaborators, which best fit the observed abundances of massive stars in the solar neighbourhood. We update both the opacities and nuclear reaction rates, and introduce new prescriptions for the mass-loss rates as stars approach the Eddington and/or the critical velocity. We account for both atomic diffusion and magnetic braking in our low-mass star models. <BR /> Results: The present rotating models provide a good description of the average evolution of non-interacting stars. In particular, they reproduce the observed main-sequence width, the positions of the red giant and supergiant stars in the Hertzsprung-Russell (HR) diagram, the observed surface compositions and rotational velocities. Very interestingly, the enhancement of the mass loss during the red-supergiant stage, when the luminosity becomes supra-Eddington in some outer layers, help models above 15-20 M<SUB>⊙</SUB> to lose a significant part of their hydrogen envelope and evolve back into the blue part of the HR diagram. This result has interesting consequences for the blue to red supergiant ratio, the minimum mass for stars to become Wolf-Rayet stars, and the maximum initial mass of stars that explode as type II-P supernovae. <P />Tracks and isochrones are available at the CDS via anonymous ftp to cdsarc.u-strasbg.fr (130.79.128.5) or via <A href=\"http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/537/A146\">http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/537/A146</A>", "database": ["astronomy"], "keywords": ["stars: general", "stars: evolution", "stars: massive", "stars: low-mass", "stars: rotation", "Astrophysics - Solar and Stellar Astrophysics"], "year": "2012", "doctype": "article", "citation_count": 1676, "domain_category": "planetary_science", "abstract_clean": "Aims: Many topical astrophysical research areas, such as the properties of planet host stars, the nature of the progenitors of different types of supernovae and gamma ray bursts, and the evolution of galaxies, require complete and homogeneous sets of stellar models at different metallicities in order to be studied during the whole of cosmic history. We present here a first set of models for solar metallicity, where the effects of rotation are accounted for in a homogeneous way. Methods: We computed a grid of 48 different stellar evolutionary tracks, both rotating and non-rotating, at Z = 0.014, spanning a wide mass range from 0.8 to 120 M⊙. For each of the stellar masses considered, electronic tables provide data for 400 stages along the evolutionary track and at each stage, a set of 43 physical data are given. These grids thus provide an extensive and detailed data basis for comparisons with the observations. The rotating models start on the zero-age main sequence (ZAMS) with a rotation rate υini/υcrit = 0.4. The evolution is computed until the end of the central carbon-burning phase, the early asymptotic giant branch (AGB) phase, or the core helium-flash for, respectively, the massive, intermediate, and both low and very low mass stars. The initial abundances are those deduced by Asplund and collaborators, which best fit the observed abundances of massive stars in the solar neighbourhood. We update both the opacities and nuclear reaction rates, and introduce new prescriptions for the mass-loss rates as stars approach the Eddington and/or the critical velocity. We account for both atomic diffusion and magnetic braking in our low-mass star models. Results: The present rotating models provide a good description of the average evolution of non-interacting stars. In particular, they reproduce the observed main-sequence width, the positions of the red giant and supergiant stars in the Hertzsprung-Russell (HR) diagram, the observed surface compositions and rotational velocities. Very interestingly, the enhancement of the mass loss during the red-supergiant stage, when the luminosity becomes supra-Eddington in some outer layers, help models above 15-20 M⊙ to lose a significant part of their hydrogen envelope and evolve back into the blue part of the HR diagram. This result has interesting consequences for the blue to red supergiant ratio, the minimum mass for stars to become Wolf-Rayet stars, and the maximum initial mass of stars that explode as type II-P supernovae. Tracks and isochrones are available at the CDS via anonymous ftp to cdsarc.u-strasbg.fr (130.79.128.5) or via http://cdsarc.u-strasbg.fr/viz-bin/qcat?J/A+A/537/A146"}
{"bibcode": "1994A&AS..106..275B", "title": "Theoretical isochrones from models with new radiative opacities", "abstract": "In this paper we present large grids of theoretical isochrones for the initial chemical compositions [Z=0.0004, Y=0.23], [Z=0.004, Y=0.24], [Z=0.008, Y=0.25], [Z=0.02, Y=0.28], and [Z= 0.05, Y=0.352] and ages in the range 4 10^6^ yr to 16 10^9^ yr. These isochrones are derived from stellar models computed with the most recent radiative opacities by Iglesias et al. (1992). In addition to this we present another set with chemical composition [Z=0.001, Y=0.23] based on models calculated with the radiative opacities by Huebner et al. (1977). All the stellar models are followed from the zero age main sequence (ZAMS) to the central carbon ignition for massive stars or to the beginning of the thermally pulsing regime of the asymptotic giant branch phase (TP-AGB) for low and intermediate mass stars. For each isochrone, we give the current mass, effective temperatures, bolometric and visual magnitudes, (U-B), (B-V), (V-R), (V-I), (V-J), (V-H), and (V-K) colors, and the luminosity function for the case of the Salpeter law. In addition to this, integrated magnitudes and colors at several characteristic points are also presented together with the mass of the remnant star when appropriate. The main characteristic that makes this set of isochrones very valuable is based on their extension in mass and chemical composition, besides the calculation of late stages of evolution, beyond the red giant tip till the white dwarf stage after the planetary nebula phase.", "database": ["astronomy"], "keywords": ["STARS: EVOLUTION", "INTERIORS", "FUNDAMENTAL PARAMETERS", "HR DIAGRAM"], "year": "1994", "doctype": "article", "citation_count": 1642, "domain_category": "planetary_science", "abstract_clean": "In this paper we present large grids of theoretical isochrones for the initial chemical compositions [Z=0.0004, Y=0.23], [Z=0.004, Y=0.24], [Z=0.008, Y=0.25], [Z=0.02, Y=0.28], and [Z= 0.05, Y=0.352] and ages in the range 4 10^6^ yr to 16 10^9^ yr. These isochrones are derived from stellar models computed with the most recent radiative opacities by Iglesias et al. (1992). In addition to this we present another set with chemical composition [Z=0.001, Y=0.23] based on models calculated with the radiative opacities by Huebner et al. (1977). All the stellar models are followed from the zero age main sequence (ZAMS) to the central carbon ignition for massive stars or to the beginning of the thermally pulsing regime of the asymptotic giant branch phase (TP-AGB) for low and intermediate mass stars. For each isochrone, we give the current mass, effective temperatures, bolometric and visual magnitudes, (U-B), (B-V), (V-R), (V-I), (V-J), (V-H), and (V-K) colors, and the luminosity function for the case of the Salpeter law. In addition to this, integrated magnitudes and colors at several characteristic points are also presented together with the mass of the remnant star when appropriate. The main characteristic that makes this set of isochrones very valuable is based on their extension in mass and chemical composition, besides the calculation of late stages of evolution, beyond the red giant tip till the white dwarf stage after the planetary nebula phase."}
{"bibcode": "1973AJ.....78..929P", "title": "Some Physical parameters of early-type stars", "abstract": "Using recent calibrations of the absolute magnitudes and temperatures and modern model atmospheres, six parameters of early-type stars have been computed. These parameters are the absolute luminosity, the radius, the flux of Lyman continuum photons, the excitation parameter, the fraction of the total energy emitted in the Lyman continuum, and the ratio of the total luminosity of the stars to the expected Lyman-alpha luminosity. The calculations have been performed for spectral types from B3 to O4 and for zero-age main sequence (ZAMS) stars, dwarfs (V), giants (III), and supergiants (I). Some of the above listed parameters have been computed also for central stars of planetary nebulae. A critical comparison with previous calculations is made. A brief discussion about the interpretation of the radio and infrared observations of Hii regions and planetary nebulae is also presented.", "database": ["astronomy"], "keywords": [], "year": "1973", "doctype": "article", "citation_count": 1626, "domain_category": "planetary_science", "abstract_clean": "Using recent calibrations of the absolute magnitudes and temperatures and modern model atmospheres, six parameters of early-type stars have been computed. These parameters are the absolute luminosity, the radius, the flux of Lyman continuum photons, the excitation parameter, the fraction of the total energy emitted in the Lyman continuum, and the ratio of the total luminosity of the stars to the expected Lyman-alpha luminosity. The calculations have been performed for spectral types from B3 to O4 and for zero-age main sequence (ZAMS) stars, dwarfs (V), giants (III), and supergiants (I). Some of the above listed parameters have been computed also for central stars of planetary nebulae. A critical comparison with previous calculations is made. A brief discussion about the interpretation of the radio and infrared observations of Hii regions and planetary nebulae is also presented."}
{"bibcode": "1965ApJ...142..531F", "title": "Thermal Instability.", "abstract": "The stability of a dilute gas in mechanical and thermal equilibrium is studied with a view to applications to non-gravitational condensation phenomena in astronomy. It is shown that, under a wide range of conditions, thermal equilibrium is unstable and can result in the formation of condensations of higher density and lower temperature than are found in the surrounding medium The instability criterion is shown to differ considerably from certain criteria proposed by previous authors. The modifications due to finite speed of sound, to thermal conduction, to a magnetic field, to rotation, to an external gravitational field, and to expansion of the medium are studied. Applications are made to the solar chromosphere and corona, to the interstellar medium in the galactic disk and halo, to planetary nebulae, and to intergalactic matter. It is shown that the principle of thermal instability is closely related to the formation of solar prominences, to condensations in planetary nebulae, and to condensation of galaxies from the intergalactic medium.", "database": ["astronomy"], "keywords": [], "year": "1965", "doctype": "article", "citation_count": 1624, "domain_category": "planetary_science", "abstract_clean": "The stability of a dilute gas in mechanical and thermal equilibrium is studied with a view to applications to non-gravitational condensation phenomena in astronomy. It is shown that, under a wide range of conditions, thermal equilibrium is unstable and can result in the formation of condensations of higher density and lower temperature than are found in the surrounding medium The instability criterion is shown to differ considerably from certain criteria proposed by previous authors. The modifications due to finite speed of sound, to thermal conduction, to a magnetic field, to rotation, to an external gravitational field, and to expansion of the medium are studied. Applications are made to the solar chromosphere and corona, to the interstellar medium in the galactic disk and halo, to planetary nebulae, and to intergalactic matter. It is shown that the principle of thermal instability is closely related to the formation of solar prominences, to condensations in planetary nebulae, and to condensation of galaxies from the intergalactic medium."}
{"bibcode": "2008SSRv..136....5K", "title": "The STEREO Mission: An Introduction", "abstract": "The twin STEREO spacecraft were launched on October 26, 2006, at 00:52 UT from Kennedy Space Center aboard a Delta 7925 launch vehicle. After a series of highly eccentric Earth orbits with apogees beyond the moon, each spacecraft used close flybys of the moon to escape into orbits about the Sun near 1 AU. Once in heliospheric orbit, one spacecraft trails Earth while the other leads. As viewed from the Sun, the two spacecraft separate at approximately 44 to 45 degrees per year. The purposes of the STEREO Mission are to understand the causes and mechanisms of coronal mass ejection (CME) initiation and to follow the propagation of CMEs through the inner heliosphere to Earth. Researchers will use STEREO measurements to study the mechanisms and sites of energetic particle acceleration and to develop three-dimensional (3-D) time-dependent models of the magnetic topology, temperature, density and velocity of the solar wind between the Sun and Earth. To accomplish these goals, each STEREO spacecraft is equipped with an almost identical set of optical, radio and in situ particles and fields instruments provided by U.S. and European investigators. The SECCHI suite of instruments includes two white light coronagraphs, an extreme ultraviolet imager and two heliospheric white light imagers which track CMEs out to 1 AU. The IMPACT suite of instruments measures in situ solar wind electrons, energetic electrons, protons and heavier ions. IMPACT also includes a magnetometer to measure the in situ magnetic field strength and direction. The PLASTIC instrument measures the composition of heavy ions in the ambient plasma as well as protons and alpha particles. The S/WAVES instrument uses radio waves to track the location of CME-driven shocks and the 3-D topology of open field lines along which flow particles produced by solar flares. Each of the four instrument packages produce a small real-time stream of selected data for purposes of predicting space weather events at Earth. NOAA forecasters at the Space Environment Center and others will use these data in their space weather forecasting and their resultant products will be widely used throughout the world. In addition to the four instrument teams, there is substantial participation by modeling and theory oriented teams. All STEREO data are freely available through individual Web sites at the four Principal Investigator institutions as well as at the STEREO Science Center located at NASA Goddard Space Flight Center.", "database": ["astronomy"], "keywords": ["CME", "Solar", "Three-dimensional"], "year": "2008", "doctype": "article", "citation_count": 1597, "domain_category": "planetary_science", "abstract_clean": "The twin STEREO spacecraft were launched on October 26, 2006, at 00:52 UT from Kennedy Space Center aboard a Delta 7925 launch vehicle. After a series of highly eccentric Earth orbits with apogees beyond the moon, each spacecraft used close flybys of the moon to escape into orbits about the Sun near 1 AU. Once in heliospheric orbit, one spacecraft trails Earth while the other leads. As viewed from the Sun, the two spacecraft separate at approximately 44 to 45 degrees per year. The purposes of the STEREO Mission are to understand the causes and mechanisms of coronal mass ejection (CME) initiation and to follow the propagation of CMEs through the inner heliosphere to Earth. Researchers will use STEREO measurements to study the mechanisms and sites of energetic particle acceleration and to develop three-dimensional (3-D) time-dependent models of the magnetic topology, temperature, density and velocity of the solar wind between the Sun and Earth. To accomplish these goals, each STEREO spacecraft is equipped with an almost identical set of optical, radio and in situ particles and fields instruments provided by U.S. and European investigators. The SECCHI suite of instruments includes two white light coronagraphs, an extreme ultraviolet imager and two heliospheric white light imagers which track CMEs out to 1 AU. The IMPACT suite of instruments measures in situ solar wind electrons, energetic electrons, protons and heavier ions. IMPACT also includes a magnetometer to measure the in situ magnetic field strength and direction. The PLASTIC instrument measures the composition of heavy ions in the ambient plasma as well as protons and alpha particles. The S/WAVES instrument uses radio waves to track the location of CME-driven shocks and the 3-D topology of open field lines along which flow particles produced by solar flares. Each of the four instrument packages produce a small real-time stream of selected data for purposes of predicting space weather events at Earth. NOAA forecasters at the Space Environment Center and others will use these data in their space weather forecasting and their resultant products will be widely used throughout the world. In addition to the four instrument teams, there is substantial participation by modeling and theory oriented teams. All STEREO data are freely available through individual Web sites at the four Principal Investigator institutions as well as at the STEREO Science Center located at NASA Goddard Space Flight Center."}
{"bibcode": "2003Msngr.114...20M", "title": "Setting New Standards with HARPS", "abstract": "By October 1st, 2003, ESO's new and unique planet-hunting machine HARPS (High-Accuracy Radial velocity Planetary Searcher) has become operational. The measurements made during the commissioning phase and the first weeks of operation are of outstanding quality. In this article we report among other examples on the first extra-solar planet discovered with HARPS and on the detection of tiny stellar oscillations. The results presented demonstrate that HARPS is currently the most precise Doppler- measurements machine in the world. With this acquisition ESO places itself at the head of a scientific domain, whose interest has continued to grow during the past years.", "database": ["astronomy"], "keywords": [], "year": "2003", "doctype": "article", "citation_count": 1589, "domain_category": "planetary_science", "abstract_clean": "By October 1st, 2003, ESO's new and unique planet-hunting machine HARPS (High-Accuracy Radial velocity Planetary Searcher) has become operational. The measurements made during the commissioning phase and the first weeks of operation are of outstanding quality. In this article we report among other examples on the first extra-solar planet discovered with HARPS and on the detection of tiny stellar oscillations. The results presented demonstrate that HARPS is currently the most precise Doppler- measurements machine in the world. With this acquisition ESO places itself at the head of a scientific domain, whose interest has continued to grow during the past years."}
{"bibcode": "2014PNAS..11115296L", "title": "Sea level and global ice volumes from the Last Glacial Maximum to the Holocene", "abstract": "The major cause of sea-level change during ice ages is the exchange of water between ice and ocean and the planet's dynamic response to the changing surface load. Inversion of ∼1,000 observations for the past 35,000 y from localities far from former ice margins has provided new constraints on the fluctuation of ice volume in this interval. Key results are: (i) a rapid final fall in global sea level of ∼40 m in &lt;2,000 y at the onset of the glacial maximum ∼30,000 y before present (30 ka BP); (ii) a slow fall to -134 m from 29 to 21 ka BP with a maximum grounded ice volume of ∼52 × 10<SUP>6</SUP> km<SUP>3</SUP> greater than today; (iii) after an initial short duration rapid rise and a short interval of near-constant sea level, the main phase of deglaciation occurred from ∼16.5 ka BP to ∼8.2 ka BP at an average rate of rise of 12 mṡka<SUP>-1</SUP> punctuated by periods of greater, particularly at 14.5-14.0 ka BP at ≥40 mmṡy<SUP>-1</SUP> (MWP-1A), and lesser, from 12.5 to 11.5 ka BP (Younger Dryas), rates; (iv) no evidence for a global MWP-1B event at ∼11.3 ka BP; and (v) a progressive decrease in the rate of rise from 8.2 ka to ∼2.5 ka BP, after which ocean volumes remained nearly constant until the renewed sea-level rise at 100-150 y ago, with no evidence of oscillations exceeding ∼15-20 cm in time intervals ≥200 y from 6 to 0.15 ka BP.", "database": ["astronomy", "physics", "general"], "keywords": [], "year": "2014", "doctype": "article", "citation_count": 1581, "domain_category": "planetary_science", "abstract_clean": "The major cause of sea-level change during ice ages is the exchange of water between ice and ocean and the planet's dynamic response to the changing surface load. Inversion of ∼1,000 observations for the past 35,000 y from localities far from former ice margins has provided new constraints on the fluctuation of ice volume in this interval. Key results are: (i) a rapid final fall in global sea level of ∼40 m in <2,000 y at the onset of the glacial maximum ∼30,000 y before present (30 ka BP); (ii) a slow fall to -134 m from 29 to 21 ka BP with a maximum grounded ice volume of ∼52 × 106 km3 greater than today; (iii) after an initial short duration rapid rise and a short interval of near-constant sea level, the main phase of deglaciation occurred from ∼16.5 ka BP to ∼8.2 ka BP at an average rate of rise of 12 mṡka-1 punctuated by periods of greater, particularly at 14.5-14.0 ka BP at ≥40 mmṡy-1 (MWP-1A), and lesser, from 12.5 to 11.5 ka BP (Younger Dryas), rates; (iv) no evidence for a global MWP-1B event at ∼11.3 ka BP; and (v) a progressive decrease in the rate of rise from 8.2 ka to ∼2.5 ka BP, after which ocean volumes remained nearly constant until the renewed sea-level rise at 100-150 y ago, with no evidence of oscillations exceeding ∼15-20 cm in time intervals ≥200 y from 6 to 0.15 ka BP."}
{"bibcode": "1981JGR....86.9776W", "title": "A negative feedback mechanism for the long-term stabilization of the earth's surface temperature", "abstract": "It is suggested that the partial pressure of carbon dioxide in the atmosphere is buffered, over geological time scales, by a negative feedback mechanism, in which the rate of weathering of silicate minerals (followed by deposition of carbonate minerals) depends on surface temperature, which in turn depends on the carbon dioxide partial pressure through the greenhouse effect. Although the quantitative details of this mechanism are speculative, it appears able to partially stabilize the earth's surface temperature against the steady increase of solar luminosity, believed to have occurred since the origin of the solar system.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Earth Surface", "Negative Feedback", "Planetary Evolution", "Stabilization", "Surface Temperature", "Carbon Dioxide", "Long Term Effects", "Partial Pressure", "Silicates", "Solar System", "Stellar Luminosity"], "year": "1981", "doctype": "article", "citation_count": 1576, "domain_category": "planetary_science", "abstract_clean": "It is suggested that the partial pressure of carbon dioxide in the atmosphere is buffered, over geological time scales, by a negative feedback mechanism, in which the rate of weathering of silicate minerals (followed by deposition of carbonate minerals) depends on surface temperature, which in turn depends on the carbon dioxide partial pressure through the greenhouse effect. Although the quantitative details of this mechanism are speculative, it appears able to partially stabilize the earth's surface temperature against the steady increase of solar luminosity, believed to have occurred since the origin of the solar system."}
{"bibcode": "2001ApJ...553L.153H", "title": "Disk Frequencies and Lifetimes in Young Clusters", "abstract": "We report the results of the first sensitive L-band survey of the intermediate-age (2.5-30 Myr) clusters NGC 2264, NGC 2362, and NGC 1960. We use JHKL colors to obtain a census of the circumstellar disk fractions in each cluster. We find disk fractions of 52%+/-10%, 12%+/-4%, and 3%+/-3% for the three clusters, respectively. Together with our previously published JHKL investigations of the younger NGC 2024, Trapezium, and IC 348 clusters, we have completed the first systematic and homogeneous survey for circumstellar disks in a sample of young clusters that both span a significant range in age (0.3-30 Myr) and contain statistically significant numbers of stars whose masses span nearly the entire stellar mass spectrum. Analysis of the combined survey indicates that the cluster disk fraction is initially very high (&gt;=80%) and rapidly decreases with increasing cluster age, such that one-half the stars within the clusters lose their disks in &lt;~3 Myr. Moreover, these observations yield an overall disk lifetime of ~6 Myr in the surveyed cluster sample. This is the timescale for essentially all the stars in a cluster to lose their disks. This should set a meaningful constraint for the planet-building timescale in stellar clusters. The implications of these results for current theories of planet formation are briefly discussed.", "database": ["astronomy"], "keywords": ["Infrared: Stars", "Galaxy: Open Clusters and Associations: General", "Stars: Planetary Systems: Protoplanetary Disks", "Stars: Formation", "Astrophysics"], "year": "2001", "doctype": "article", "citation_count": 1570, "domain_category": "planetary_science", "abstract_clean": "We report the results of the first sensitive L-band survey of the intermediate-age (2.5-30 Myr) clusters NGC 2264, NGC 2362, and NGC 1960. We use JHKL colors to obtain a census of the circumstellar disk fractions in each cluster. We find disk fractions of 52%+/-10%, 12%+/-4%, and 3%+/-3% for the three clusters, respectively. Together with our previously published JHKL investigations of the younger NGC 2024, Trapezium, and IC 348 clusters, we have completed the first systematic and homogeneous survey for circumstellar disks in a sample of young clusters that both span a significant range in age (0.3-30 Myr) and contain statistically significant numbers of stars whose masses span nearly the entire stellar mass spectrum. Analysis of the combined survey indicates that the cluster disk fraction is initially very high (>=80%) and rapidly decreases with increasing cluster age, such that one-half the stars within the clusters lose their disks in <~3 Myr. Moreover, these observations yield an overall disk lifetime of ~6 Myr in the surveyed cluster sample. This is the timescale for essentially all the stars in a cluster to lose their disks. This should set a meaningful constraint for the planet-building timescale in stellar clusters. The implications of these results for current theories of planet formation are briefly discussed."}
{"bibcode": "1974JAtS...31.1791M", "title": "A Hierarchy of Turbulence Closure Models for Planetary Boundary Layers.", "abstract": "Turbulence models centered on hypotheses by Rotta and Kolmogoroff are complex. In the present paper we consider systematic simplifications based on the observation that parameters governing the degree of anisotropy are small. Hopefully, we shall discern a level of complexity which is intuitively attractive and which optimizes computational speed and convenience without unduly sacrificing accuracy.Discussion is focused on density stratified flow due to temperature. However, other dependent variables-such as water vapor and droplet density-can be treated in analogous fashion. It is, in fact, the anticipation of additional physical complexity in modeling turbulent flow fields that partially motivates the interest in an organized process of analytical simplification.For the problem of a planetary boundary layer subject to a diurnally varying surface heat flux or surface temperature, three models of varying complexity have been integrated for 10 days. All of the models incorporate identical empirical constants obtained from neutral flow data alone. The most complex of the three models requires simultaneous solution of 10 partial differential equations for turbulence moments in addition to the equations for the mean velocity components and temperature; the least complex eliminates all of the 10 differential equation whereas a `compromise' model retains two differential equations for total turbulent energy and temperature variance.We conclude that all of the models give nearly the same results. We find the two-differential-equation model particularly attractive.", "database": ["astronomy", "physics", "earth science"], "keywords": [], "year": "1974", "doctype": "article", "citation_count": 1549, "domain_category": "planetary_science", "abstract_clean": "Turbulence models centered on hypotheses by Rotta and Kolmogoroff are complex. In the present paper we consider systematic simplifications based on the observation that parameters governing the degree of anisotropy are small. Hopefully, we shall discern a level of complexity which is intuitively attractive and which optimizes computational speed and convenience without unduly sacrificing accuracy.Discussion is focused on density stratified flow due to temperature. However, other dependent variables-such as water vapor and droplet density-can be treated in analogous fashion. It is, in fact, the anticipation of additional physical complexity in modeling turbulent flow fields that partially motivates the interest in an organized process of analytical simplification.For the problem of a planetary boundary layer subject to a diurnally varying surface heat flux or surface temperature, three models of varying complexity have been integrated for 10 days. All of the models incorporate identical empirical constants obtained from neutral flow data alone. The most complex of the three models requires simultaneous solution of 10 partial differential equations for turbulence moments in addition to the equations for the mean velocity components and temperature; the least complex eliminates all of the 10 differential equation whereas a `compromise' model retains two differential equations for total turbulent energy and temperature variance.We conclude that all of the models give nearly the same results. We find the two-differential-equation model particularly attractive."}
{"bibcode": "1989RvGeo..27..271G", "title": "Dusty plasma in the solar system", "abstract": "The processes that lead to charging of dust grains in a plasma are briefly reviewed. Whereas for single grains the results have been long known, the reduction of the average charge on a grain by 'Debye screening' has only recently been discovered. This reduction can be important in the Jovian ring and in the rings of Uranus. The emerging field of gravitoelectrodynamics which deals with the motion of charged grains in a planetary magnetosphere is then reviewed. Important mechanisms for distributing grains in radial distance are due to stochastic fluctuations of the grain charge and a systematic variation due to motion through plasma gradients. The electrostatic levitation model for the formation of spokes is discussed, and it is shown that the radial transport of dust contained in the spokes may be responsible for the rich radial structure in Saturn's rings. Finally, collective effects in dusty plasmas are discussed which affect various waves, such as density waves in planetary rings and low-frequency plasma waves. The possibility of charged grains forming a Coulomb lattice is briefly described.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Charged Particles", "Interplanetary Dust", "Planetary Magnetospheres", "Planetary Rings", "Plasma Potentials", "Angular Momentum", "Electrodynamics", "Gravitational Effects", "Jupiter Rings", "Planetary Magnetic Fields", "Plasma Temperature", "Uranus Rings", "SOLAR SYSTEM", "DUST", "PLASMA", "CHARGED PARTICLES", "GRAINS", "ELECTRODYNAMICS", "MOTION", "RINGS", "ELECTROSTATIC EFFECTS", "MODELS", "PARAMETERS", "DIAGRAMS", "SPOKES", "FORMATION", "ORIGIN", "RADIAL TRANSPORT", "PLASMA WAVES", "DENSITY WAVES", "CALCULATIONS", "GRAVITY EFFECTS", "ELECTRICAL EFFECTS", "MAGNETOSPHERE", "DISTRIBUTION", "ELECTROMAGNETISM", "ANGULAR MOMENTUM", "EVOLUTION", "Interplanetary Physics: Interplanetary dust"], "year": "1989", "doctype": "article", "citation_count": 1543, "domain_category": "planetary_science", "abstract_clean": "The processes that lead to charging of dust grains in a plasma are briefly reviewed. Whereas for single grains the results have been long known, the reduction of the average charge on a grain by 'Debye screening' has only recently been discovered. This reduction can be important in the Jovian ring and in the rings of Uranus. The emerging field of gravitoelectrodynamics which deals with the motion of charged grains in a planetary magnetosphere is then reviewed. Important mechanisms for distributing grains in radial distance are due to stochastic fluctuations of the grain charge and a systematic variation due to motion through plasma gradients. The electrostatic levitation model for the formation of spokes is discussed, and it is shown that the radial transport of dust contained in the spokes may be responsible for the rich radial structure in Saturn's rings. Finally, collective effects in dusty plasmas are discussed which affect various waves, such as density waves in planetary rings and low-frequency plasma waves. The possibility of charged grains forming a Coulomb lattice is briefly described."}
{"bibcode": "2006LRR.....9....3W", "title": "The Confrontation between General Relativity and Experiment", "abstract": "The status of experimental tests of general relativity and of theoretical frameworks for analyzing them is reviewed. Einstein's equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of special relativity, and the gravitational redshift experiment. Ongoing tests of EEP and of the inverse square law are searching for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light deflection, the Shapiro time delay, the perihelion advance of Mercury, and the Nordtvedt effect in lunar motion. Gravitational wave damping has been detected in an amount that agrees with general relativity to better than half a percent using the Hulse-Taylor binary pulsar, and other binary pulsar systems have yielded other tests, especially of strong-field effects. When direct observation of gravitational radiation from astrophysical sources begins, new tests of general relativity will be possible.", "database": ["astronomy", "physics"], "keywords": ["Binary Pulsar", "Nordtvedt Effect", "Einstein Equivalence Principle (EEP)", "Gravitational Waves", "Gravitational Radiation Damping", "General Relativity and Quantum Cosmology"], "year": "2006", "doctype": "article", "citation_count": 1528, "domain_category": "planetary_science", "abstract_clean": "The status of experimental tests of general relativity and of theoretical frameworks for analyzing them is reviewed. Einstein's equivalence principle (EEP) is well supported by experiments such as the Eötvös experiment, tests of special relativity, and the gravitational redshift experiment. Ongoing tests of EEP and of the inverse square law are searching for new interactions arising from unification or quantum gravity. Tests of general relativity at the post-Newtonian level have reached high precision, including the light deflection, the Shapiro time delay, the perihelion advance of Mercury, and the Nordtvedt effect in lunar motion. Gravitational wave damping has been detected in an amount that agrees with general relativity to better than half a percent using the Hulse-Taylor binary pulsar, and other binary pulsar systems have yielded other tests, especially of strong-field effects. When direct observation of gravitational radiation from astrophysical sources begins, new tests of general relativity will be possible."}
{"bibcode": "2008Sci...320.1034F", "title": "The Microbial Engines That Drive Earth’s Biogeochemical Cycles", "abstract": "Virtually all nonequilibrium electron transfers on Earth are driven by a set of nanobiological machines composed largely of multimeric protein complexes associated with a small number of prosthetic groups. These machines evolved exclusively in microbes early in our planet’s history yet, despite their antiquity, are highly conserved. Hence, although there is enormous genetic diversity in nature, there remains a relatively stable set of core genes coding for the major redox reactions essential for life and biogeochemical cycles. These genes created and coevolved with biogeochemical cycles and were passed from microbe to microbe primarily by horizontal gene transfer. A major challenge in the coming decades is to understand how these machines evolved, how they work, and the processes that control their activity on both molecular and planetary scales.", "database": ["astronomy", "general"], "keywords": ["MICROBIO"], "year": "2008", "doctype": "article", "citation_count": 1515, "domain_category": "planetary_science", "abstract_clean": "Virtually all nonequilibrium electron transfers on Earth are driven by a set of nanobiological machines composed largely of multimeric protein complexes associated with a small number of prosthetic groups. These machines evolved exclusively in microbes early in our planet’s history yet, despite their antiquity, are highly conserved. Hence, although there is enormous genetic diversity in nature, there remains a relatively stable set of core genes coding for the major redox reactions essential for life and biogeochemical cycles. These genes created and coevolved with biogeochemical cycles and were passed from microbe to microbe primarily by horizontal gene transfer. A major challenge in the coming decades is to understand how these machines evolved, how they work, and the processes that control their activity on both molecular and planetary scales."}
{"bibcode": "2001PASP..113.1420V", "title": "Cosmic-Ray Rejection by Laplacian Edge Detection", "abstract": "Conventional algorithms for rejecting cosmic rays in single CCD exposures rely on the contrast between cosmic rays and their surroundings and may produce erroneous results if the point-spread function is smaller than the largest cosmic rays. This paper describes a robust algorithm for cosmic-ray rejection, based on a variation of Laplacian edge detection. The algorithm identifies cosmic rays of arbitrary shapes and sizes by the sharpness of their edges and reliably discriminates between poorly sampled point sources and cosmic rays. Examples of its performance are given for spectroscopic and imaging data, including Hubble Space Telescope Wide Field Planetary Camera 2 images.", "database": ["astronomy"], "keywords": ["Instrumentation: Detectors", "Methods: Data Analysis-techniques: image processing", "Astrophysics"], "year": "2001", "doctype": "article", "citation_count": 1513, "domain_category": "planetary_science", "abstract_clean": "Conventional algorithms for rejecting cosmic rays in single CCD exposures rely on the contrast between cosmic rays and their surroundings and may produce erroneous results if the point-spread function is smaller than the largest cosmic rays. This paper describes a robust algorithm for cosmic-ray rejection, based on a variation of Laplacian edge detection. The algorithm identifies cosmic rays of arbitrary shapes and sizes by the sharpness of their edges and reliably discriminates between poorly sampled point sources and cosmic rays. Examples of its performance are given for spectroscopic and imaging data, including Hubble Space Telescope Wide Field Planetary Camera 2 images."}
{"bibcode": "2003A&A...402..701B", "title": "Evolutionary models for cool brown dwarfs and extrasolar giant planets. The case of HD 209458", "abstract": "We present evolutionary models for cool brown dwarfs and extra-solar giant planets. The models reproduce the main trends of observed methane dwarfs in near-IR color-magnitude diagrams. We also present evolutionary models for irradiated planets, coupling for the first time irradiated atmosphere profiles and inner structures. We focus on HD 209458-like systems and show that irradiation effects can substantially affect the radius of sub-jovian mass giant planets. Irradiation effects, however, cannot alone explain the large observed radius of HD 209458b. Adopting assumptions which optimise irradiation effects and taking into account the extension of the outer atmospheric layers, we still find ~ 20% discrepancy between observed and theoretical radii. An extra source of energy seems to be required to explain the observed value of the first transit planet.", "database": ["astronomy"], "keywords": ["planetary systems", "stars: brown dwarfs", "stars: evolution", "stars: individual (HD 209458)", "Astrophysics"], "year": "2003", "doctype": "article", "citation_count": 1504, "domain_category": "planetary_science", "abstract_clean": "We present evolutionary models for cool brown dwarfs and extra-solar giant planets. The models reproduce the main trends of observed methane dwarfs in near-IR color-magnitude diagrams. We also present evolutionary models for irradiated planets, coupling for the first time irradiated atmosphere profiles and inner structures. We focus on HD 209458-like systems and show that irradiation effects can substantially affect the radius of sub-jovian mass giant planets. Irradiation effects, however, cannot alone explain the large observed radius of HD 209458b. Adopting assumptions which optimise irradiation effects and taking into account the extension of the outer atmospheric layers, we still find ~ 20% discrepancy between observed and theoretical radii. An extra source of energy seems to be required to explain the observed value of the first transit planet."}
{"bibcode": "1980ApJ...241..425G", "title": "Disk-satellite interactions.", "abstract": "The rate at which angular momentum and energy are transferred between a disk and a satellite which orbit a central mass is calculated. It is shown that the angular momentum and energy transfer at Lindblad resonances tends to increase the satellite's orbit to lowest order in eccentricity, whereas the transfer at corotation resonances tends to decrease it. The results are applied to the interaction between Jupiter and the protoplanetary disk. The angular momentum transfer is shown to be so rapid that substantial changes in both the structure of the disk and the orbit of Jupiter must have taken place on a time scale of a few thousand years.", "database": ["astronomy"], "keywords": ["Jupiter (Planet)", "Momentum Transfer", "Natural Satellites", "Planetary Evolution", "Protoplanets", "Angular Momentum", "Celestial Mechanics", "Resonance", "Solar System", "Astronomy", "Jupiter:Protoplanetary Nebula", "Planetary Rings:Planetary Satellites"], "year": "1980", "doctype": "article", "citation_count": 1492, "domain_category": "planetary_science", "abstract_clean": "The rate at which angular momentum and energy are transferred between a disk and a satellite which orbit a central mass is calculated. It is shown that the angular momentum and energy transfer at Lindblad resonances tends to increase the satellite's orbit to lowest order in eccentricity, whereas the transfer at corotation resonances tends to decrease it. The results are applied to the interaction between Jupiter and the protoplanetary disk. The angular momentum transfer is shown to be so rapid that substantial changes in both the structure of the disk and the orbit of Jupiter must have taken place on a time scale of a few thousand years."}
{"bibcode": "1981JGR....86.3039H", "title": "Bidirectional reflectance spectroscopy. I - Theory", "abstract": "An approximate analytic solution is derived for the radiative transfer equation describing particulate surface light scattering, taking into account multiple scattering and mutual shadowing. Analytical expressions for the following quantities are found: bidirectional reflectance, radiance coefficient and factor, the normal, Bond, hemispherical, and physical albedos, integral phase function and phase integral, and limb-darkening profile. Scattering functions for mixtures can be calculated, as well as corrections for comparisons of experimental transmission or reflection spectra with observational planetary spectra. The theory should be useful for the interpretation of reflectance spectroscopy of laboratory surfaces and the photometry of solar system objects.", "database": ["astronomy", "physics", "earth science"], "keywords": ["Astronomical Spectroscopy", "Bidirectional Reflectance", "Planetary Surfaces", "Radiative Transfer", "Spectral Reflectance", "Integral Equations", "Isotropic Media", "Light Scattering", "Limb Darkening", "Optical Reflection", "Scattering Coefficients", "Spectrum Analysis"], "year": "1981", "doctype": "article", "citation_count": 1489, "domain_category": "planetary_science", "abstract_clean": "An approximate analytic solution is derived for the radiative transfer equation describing particulate surface light scattering, taking into account multiple scattering and mutual shadowing. Analytical expressions for the following quantities are found: bidirectional reflectance, radiance coefficient and factor, the normal, Bond, hemispherical, and physical albedos, integral phase function and phase integral, and limb-darkening profile. Scattering functions for mixtures can be calculated, as well as corrections for comparisons of experimental transmission or reflection spectra with observational planetary spectra. The theory should be useful for the interpretation of reflectance spectroscopy of laboratory surfaces and the photometry of solar system objects."}
{"bibcode": "2008Sci...322.1348M", "title": "Direct Imaging of Multiple Planets Orbiting the Star HR 8799", "abstract": "Direct imaging of exoplanetary systems is a powerful technique that can reveal Jupiter-like planets in wide orbits, can enable detailed characterization of planetary atmospheres, and is a key step toward imaging Earth-like planets. Imaging detections are challenging because of the combined effect of small angular separation and large luminosity contrast between a planet and its host star. High-contrast observations with the Keck and Gemini telescopes have revealed three planets orbiting the star HR 8799, with projected separations of 24, 38, and 68 astronomical units. Multi-epoch data show counter clockwise orbital motion for all three imaged planets. The low luminosity of the companions and the estimated age of the system imply planetary masses between 5 and 13 times that of Jupiter. This system resembles a scaled-up version of the outer portion of our solar system.", "database": ["astronomy", "general"], "keywords": ["Astrophysics"], "year": "2008", "doctype": "article", "citation_count": 1487, "domain_category": "planetary_science", "abstract_clean": "Direct imaging of exoplanetary systems is a powerful technique that can reveal Jupiter-like planets in wide orbits, can enable detailed characterization of planetary atmospheres, and is a key step toward imaging Earth-like planets. Imaging detections are challenging because of the combined effect of small angular separation and large luminosity contrast between a planet and its host star. High-contrast observations with the Keck and Gemini telescopes have revealed three planets orbiting the star HR 8799, with projected separations of 24, 38, and 68 astronomical units. Multi-epoch data show counter clockwise orbital motion for all three imaged planets. The low luminosity of the companions and the estimated age of the system imply planetary masses between 5 and 13 times that of Jupiter. This system resembles a scaled-up version of the outer portion of our solar system."}
{"bibcode": "1999A&AS..138..119K", "title": "VALD-2: Progress of the Vienna Atomic Line Data Base", "abstract": "We describe the updated version of the Vienna Atomic Line Data Base (VALD, \\cite[Piskunov et al. 1995)]{pis95} which represents a considerable improvement over the first installation from 1994. The original line lists have been complemented with critically evaluated data obtained from experimental measurements and theoretical calculations which are necessary for computing state-of-the-art line opacities in stellar atmospheres, as well as for synthesizing spectra for high precision analyses. In this paper, we present new and improved data sets for neutral species and ions of Si, P, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ru, Xe, La, Ce, Pr, Nd, Sm, Eu, Gd, Dy, Ho, Er, Tm, Yb, Lu, Re, Pt, Au, Hg, and Pb. For some species data are available in VALD for the first time. We explain our choice of quality rankings by reviewing the literature for the new data and by comparison with source lists included into VALD. For some cases, we produced new line data by weighted averaging of data from different sources with individual error estimates in order to increase the reliability of VALD line lists. Software modifications allow remote users of VALD to specify individual extraction parameters as an alternative to the default settings of the VALD team and to have direct control over the quality ranking of line data. A World-Wide-Web interface is described which provides easy access to all new features. To simplify proper crediting of all authors of atomic data, VALD now includes a compilation of all publications used in each type of reply. Finally, we briefly discuss the future roadmap of VALD developments, including the incorporation of molecular transitions and integration with external data bases. http://www.astro.univie.ac.at/~vald http://www.astro.uu.se/~vald", "database": ["astronomy"], "keywords": ["ATOMIC DATA", "TECHNIQUES: SPECTROSCOPIC", "SUN: ABUNDANCES", "STARS: ABUNDANCES", "STARS: ATMOSPHERES", "STARS: CHEMICALLY PECULIAR"], "year": "1999", "doctype": "article", "citation_count": 1460, "domain_category": "planetary_science", "abstract_clean": "We describe the updated version of the Vienna Atomic Line Data Base (VALD, \\cite[Piskunov et al. 1995)]{pis95} which represents a considerable improvement over the first installation from 1994. The original line lists have been complemented with critically evaluated data obtained from experimental measurements and theoretical calculations which are necessary for computing state-of-the-art line opacities in stellar atmospheres, as well as for synthesizing spectra for high precision analyses. In this paper, we present new and improved data sets for neutral species and ions of Si, P, Sc, Ti, V, Cr, Mn, Fe, Co, Ni, Cu, Zn, Y, Zr, Ru, Xe, La, Ce, Pr, Nd, Sm, Eu, Gd, Dy, Ho, Er, Tm, Yb, Lu, Re, Pt, Au, Hg, and Pb. For some species data are available in VALD for the first time. We explain our choice of quality rankings by reviewing the literature for the new data and by comparison with source lists included into VALD. For some cases, we produced new line data by weighted averaging of data from different sources with individual error estimates in order to increase the reliability of VALD line lists. Software modifications allow remote users of VALD to specify individual extraction parameters as an alternative to the default settings of the VALD team and to have direct control over the quality ranking of line data. A World-Wide-Web interface is described which provides easy access to all new features. To simplify proper crediting of all authors of atomic data, VALD now includes a compilation of all publications used in each type of reply. Finally, we briefly discuss the future roadmap of VALD developments, including the incorporation of molecular transitions and integration with external data bases. http://www.astro.univie.ac.at/~vald http://www.astro.uu.se/~vald"}
{"bibcode": "1999SSRv...90..413R", "title": "Particle acceleration at the Sun and in the heliosphere", "abstract": "Energetic particles are accelerated in rich profusion at sites throughout the heliosphere. They come from solar flares in the low corona, from shock waves driven outward by coronal mass ejections (CMEs), from planetary magnetospheres and bow shocks. They come from corotating interaction regions (CIRs) produced by high-speed streams in the solar wind, and from the heliospheric termination shock at the outer edge of the heliospheric cavity. We sample many populations near Earth, but can distinguish them readily by their element and isotope abundances, ionization states, energy spectra, angular distributions and time behavior. Remote spacecraft have probed the spatial distributions of the particles and examined new sources in situ. Most acceleration sources can be ‘seen’ only by direct observation of the particles; few photons are produced at these sites. Wave-particle interactions are an essential feature in acceleration sources and, for shock acceleration, new evidence of energetic-proton-generated waves has come from abundance variations and from local cross-field scattering. Element abundances often tell us the physics of the source plasma itself, prior to acceleration. By comparing different populations, we learn more about the sources, and about the physics of acceleration and transport, than we can possibly learn from one source alone.", "database": ["astronomy"], "keywords": [], "year": "1999", "doctype": "article", "citation_count": 1436, "domain_category": "planetary_science", "abstract_clean": "Energetic particles are accelerated in rich profusion at sites throughout the heliosphere. They come from solar flares in the low corona, from shock waves driven outward by coronal mass ejections (CMEs), from planetary magnetospheres and bow shocks. They come from corotating interaction regions (CIRs) produced by high-speed streams in the solar wind, and from the heliospheric termination shock at the outer edge of the heliospheric cavity. We sample many populations near Earth, but can distinguish them readily by their element and isotope abundances, ionization states, energy spectra, angular distributions and time behavior. Remote spacecraft have probed the spatial distributions of the particles and examined new sources in situ. Most acceleration sources can be ‘seen’ only by direct observation of the particles; few photons are produced at these sites. Wave-particle interactions are an essential feature in acceleration sources and, for shock acceleration, new evidence of energetic-proton-generated waves has come from abundance variations and from local cross-field scattering. Element abundances often tell us the physics of the source plasma itself, prior to acceleration. By comparing different populations, we learn more about the sources, and about the physics of acceleration and transport, than we can possibly learn from one source alone."}
{"bibcode": "2009ARA&A..47..427H", "title": "Complex Organic Interstellar Molecules", "abstract": "Of the over 150 different molecular species detected in the interstellar and circumstellar media, approximately 50 contain 6 or more atoms. These molecules, labeled complex by astronomers if not by chemists, all contain the element carbon and so can be called organic. In the interstellar medium, complex molecules are detected in the denser sources only. Although, with one exception, complex molecules have only been detected in the gas phase, there is strong evidence that they can be formed in ice mantles on interstellar grains. The nature of the gaseous complex species depends dramatically on the source where they are found: in cold, dense regions they tend to be unsaturated (hydrogen-poor) and exotic, whereas in young stellar objects, they tend to be quite saturated (hydrogen-rich) and terrestrial in nature. Based on both their spectra and chemistry, complex molecules are excellent probes of the physical conditions and history of the sources where they reside. Because they are detected in young stellar objects, complex molecules are expected to be common ingredients for new planetary systems. In this review, we discuss both the observation and chemistry of complex molecules in assorted interstellar regions in the Milky Way.", "database": ["astronomy"], "keywords": [], "year": "2009", "doctype": "article", "citation_count": 1426, "domain_category": "planetary_science", "abstract_clean": "Of the over 150 different molecular species detected in the interstellar and circumstellar media, approximately 50 contain 6 or more atoms. These molecules, labeled complex by astronomers if not by chemists, all contain the element carbon and so can be called organic. In the interstellar medium, complex molecules are detected in the denser sources only. Although, with one exception, complex molecules have only been detected in the gas phase, there is strong evidence that they can be formed in ice mantles on interstellar grains. The nature of the gaseous complex species depends dramatically on the source where they are found: in cold, dense regions they tend to be unsaturated (hydrogen-poor) and exotic, whereas in young stellar objects, they tend to be quite saturated (hydrogen-rich) and terrestrial in nature. Based on both their spectra and chemistry, complex molecules are excellent probes of the physical conditions and history of the sources where they reside. Because they are detected in young stellar objects, complex molecules are expected to be common ingredients for new planetary systems. In this review, we discuss both the observation and chemistry of complex molecules in assorted interstellar regions in the Milky Way."}
{"bibcode": "2005ApJ...622.1102F", "title": "The Planet-Metallicity Correlation", "abstract": "We have recently carried out spectral synthesis modeling to determine T<SUB>eff</SUB>, logg, vsini, and [Fe/H] for 1040 FGK-type stars on the Keck, Lick, and Anglo-Australian Telescope planet search programs. This is the first time that a single, uniform spectroscopic analysis has been made for every star on a large Doppler planet search survey. We identify a subset of 850 stars that have Doppler observations sufficient to detect uniformly all planets with radial velocity semiamplitudes K&gt;30 m s<SUP>-1</SUP> and orbital periods shorter than 4 yr. From this subset of stars, we determine that fewer than 3% of stars with -0.5&lt;[Fe/H]&lt;0.0 have Doppler-detected planets. Above solar metallicity, there is a smooth and rapid rise in the fraction of stars with planets. At [Fe/H]&gt;+0.3 dex, 25% of observed stars have detected gas giant planets. A power-law fit to these data relates the formation probability for gas giant planets to the square of the number of metal atoms. High stellar metallicity also appears to be correlated with the presence of multiple-planet systems and with the total detected planet mass. This data set was examined to better understand the origin of high metallicity in stars with planets. None of the expected fossil signatures of accretion are observed in stars with planets relative to the general sample: (1) metallicity does not appear to increase as the mass of the convective envelopes decreases, (2) subgiants with planets do not show dilution of metallicity, (3) no abundance variations for Na, Si, Ti, or Ni are found as a function of condensation temperature, and (4) no correlations between metallicity and orbital period or eccentricity could be identified. We conclude that stars with extrasolar planets do not have an accretion signature that distinguishes them from other stars; more likely, they are simply born in higher metallicity molecular clouds. <P />Based on observations obtained at Lick and Keck Observatories, operated by the University of California, and the Anglo-Australian Observatories.", "database": ["astronomy"], "keywords": ["Stars: Planetary Systems", "Stars: Abundances", "Stars: Fundamental Parameters"], "year": "2005", "doctype": "article", "citation_count": 1412, "domain_category": "planetary_science", "abstract_clean": "We have recently carried out spectral synthesis modeling to determine Teff, logg, vsini, and [Fe/H] for 1040 FGK-type stars on the Keck, Lick, and Anglo-Australian Telescope planet search programs. This is the first time that a single, uniform spectroscopic analysis has been made for every star on a large Doppler planet search survey. We identify a subset of 850 stars that have Doppler observations sufficient to detect uniformly all planets with radial velocity semiamplitudes K>30 m s-1 and orbital periods shorter than 4 yr. From this subset of stars, we determine that fewer than 3% of stars with -0.5<[Fe/H]<0.0 have Doppler-detected planets. Above solar metallicity, there is a smooth and rapid rise in the fraction of stars with planets. At [Fe/H]>+0.3 dex, 25% of observed stars have detected gas giant planets. A power-law fit to these data relates the formation probability for gas giant planets to the square of the number of metal atoms. High stellar metallicity also appears to be correlated with the presence of multiple-planet systems and with the total detected planet mass. This data set was examined to better understand the origin of high metallicity in stars with planets. None of the expected fossil signatures of accretion are observed in stars with planets relative to the general sample: (1) metallicity does not appear to increase as the mass of the convective envelopes decreases, (2) subgiants with planets do not show dilution of metallicity, (3) no abundance variations for Na, Si, Ti, or Ni are found as a function of condensation temperature, and (4) no correlations between metallicity and orbital period or eccentricity could be identified. We conclude that stars with extrasolar planets do not have an accretion signature that distinguishes them from other stars; more likely, they are simply born in higher metallicity molecular clouds. Based on observations obtained at Lick and Keck Observatories, operated by the University of California, and the Anglo-Australian Observatories."}
{"bibcode": "1995SSRv...71..207L", "title": "The Wind Magnetic Field Investigation", "abstract": "The magnetic field experiment on WIND will provide data for studies of a broad range of scales of structures and fluctuation characteristics of the interplanetary magnetic field throughout the mission, and, where appropriate, relate them to the statics and dynamics of the magnetosphere. The basic instrument of the Magnetic Field Investigation (MFI) is a boom-mounted dual triaxial fluxgate magnetometer and associated electronics. The dual configuration provides redundancy and also permits accurate removal of the dipolar portion of the spacecraft magnetic field. The instrument provides (1) near real-time data at nominally one vector per 92 s as key parameter data for broad dissemination, (2) rapid data at 10.9 vectors s<SUP>-1</SUP> for standard analysis, and (3) occasionally, snapshot (SS) memory data and Fast Fourier Transform data (FFT), both based on 44 vectors s<SUP>-1</SUP>. These measurements will be precise (0.025%), accurate, ultra-sensitive (0.008 nT/step quantization), and where the sensor noise level is &lt;0.006 nT r.m.s. for 0 10 Hz. The digital processing unit utilizes a 12-bit microprocessor controlled analogue-to-digital converter. The instrument features a very wide dynamic range of measurement capability, from ±4 nT up to ±65 536 nT per axis in eight discrete ranges. (The upper range permits complete testing in the Earth's field.) In the FTT mode power spectral density elements are transmitted to the ground as fast as once every 23 s (high rate), and 2.7 min of SS memory time series data, triggered automatically by pre-set command, requires typically about 5.1 hours for transmission. Standard data products are expected to be the following vector field averages: 0.0227-s (detail data from SS), 0.092 s (‘detail’ in standard mode), 3 s, 1 min, and 1 hour, in both GSE and GSM coordinates, as well as the FFT spectral elements. As has been our team's tradition, high instrument reliability is obtained by the use of fully redundant systems and extremely conservative designs. We plan studies of the solar wind: (1) as a collisionless plasma laboratory, at all time scales, macro, meso and micro, but concentrating on the kinetic scale, the highest time resolution of the instrument (=0.022 s), (2) as a consequence of solar energy and mass output, (3) as an external source of plasma that can couple mass, momentum, and energy to the Earth's magnetosphere, and (4) as it is modified as a consequence of its imbedded field interacting with the moon. Since the GEOTAIL Inboard Magnetometer (GIM), which is similar to the MFI instrument, was developed by members of our team, we provide a brief discussion of GIM related science objectives, along with MFI related science goals.", "database": ["astronomy"], "keywords": [], "year": "1995", "doctype": "article", "citation_count": 1397, "domain_category": "planetary_science", "abstract_clean": "The magnetic field experiment on WIND will provide data for studies of a broad range of scales of structures and fluctuation characteristics of the interplanetary magnetic field throughout the mission, and, where appropriate, relate them to the statics and dynamics of the magnetosphere. The basic instrument of the Magnetic Field Investigation (MFI) is a boom-mounted dual triaxial fluxgate magnetometer and associated electronics. The dual configuration provides redundancy and also permits accurate removal of the dipolar portion of the spacecraft magnetic field. The instrument provides (1) near real-time data at nominally one vector per 92 s as key parameter data for broad dissemination, (2) rapid data at 10.9 vectors s-1 for standard analysis, and (3) occasionally, snapshot (SS) memory data and Fast Fourier Transform data (FFT), both based on 44 vectors s-1. These measurements will be precise (0.025%), accurate, ultra-sensitive (0.008 nT/step quantization), and where the sensor noise level is <0.006 nT r.m.s. for 0 10 Hz. The digital processing unit utilizes a 12-bit microprocessor controlled analogue-to-digital converter. The instrument features a very wide dynamic range of measurement capability, from ±4 nT up to ±65 536 nT per axis in eight discrete ranges. (The upper range permits complete testing in the Earth's field.) In the FTT mode power spectral density elements are transmitted to the ground as fast as once every 23 s (high rate), and 2.7 min of SS memory time series data, triggered automatically by pre-set command, requires typically about 5.1 hours for transmission. Standard data products are expected to be the following vector field averages: 0.0227-s (detail data from SS), 0.092 s (‘detail’ in standard mode), 3 s, 1 min, and 1 hour, in both GSE and GSM coordinates, as well as the FFT spectral elements. As has been our team's tradition, high instrument reliability is obtained by the use of fully redundant systems and extremely conservative designs. We plan studies of the solar wind: (1) as a collisionless plasma laboratory, at all time scales, macro, meso and micro, but concentrating on the kinetic scale, the highest time resolution of the instrument (=0.022 s), (2) as a consequence of solar energy and mass output, (3) as an external source of plasma that can couple mass, momentum, and energy to the Earth's magnetosphere, and (4) as it is modified as a consequence of its imbedded field interacting with the moon. Since the GEOTAIL Inboard Magnetometer (GIM), which is similar to the MFI instrument, was developed by members of our team, we provide a brief discussion of GIM related science objectives, along with MFI related science goals."}
{"bibcode": "2010A&ARv..18...67T", "title": "Accurate masses and radii of normal stars: modern results and applications", "abstract": "This article presents and discusses a critical compilation of accurate, fundamental determinations of stellar masses and radii. We have identified 95 detached binary systems containing 190 stars (94 eclipsing systems, and α Centauri) that satisfy our criterion that the mass and radius of both stars be known within errors of ±3% accuracy or better. All of them are non-interacting systems, and so the stars should have evolved as if they were single. This sample more than doubles that of the earlier similar review by Andersen (Astron Astrophys Rev 3:91-126, 1991), extends the mass range at both ends and, for the first time, includes an extragalactic binary. In every case, we have examined the original data and recomputed the stellar parameters with a consistent set of assumptions and physical constants. To these we add interstellar reddening, effective temperature, metal abundance, rotational velocity and apsidal motion determinations when available, and we compute a number of other physical parameters, notably luminosity and distance. These accurate physical parameters reveal the effects of stellar evolution with unprecedented clarity, and we discuss the use of the data in observational tests of stellar evolution models in some detail. Earlier findings of significant structural differences between moderately fast-rotating, mildly active stars and single stars, ascribed to the presence of strong magnetic and spot activity, are confirmed beyond doubt. We also show how the best data can be used to test prescriptions for the subtle interplay between convection, diffusion, and other non-classical effects in stellar models. The amount and quality of the data also allow us to analyse the tidal evolution of the systems in considerable depth, testing prescriptions of rotational synchronisation and orbital circularisation in greater detail than possible before. We show that the formulae for pseudo-synchronisation of stars in eccentric orbits predict the observed rotations quite well, except for very young and/or widely separated stars. Deviations do occur, however, especially for stars with convective envelopes. The superior data set finally demonstrates that apsidal motion rates as predicted from General Relativity plus tidal theory are in good agreement with the best observational data. No reliable binary data exist, which challenge General Relativity to any significant extent. The new data also enable us to derive empirical calibrations of M and R for single (post-) main-sequence stars above {0.6 M_{odot}}. Simple, polynomial functions of T <SUB>eff</SUB>, log g and [Fe/H] yield M and R within errors of 6 and 3%, respectively. Excellent agreement is found with independent determinations for host stars of transiting extrasolar planets, and good agreement with determinations of M and R from stellar models as constrained by trigonometric parallaxes and spectroscopic values of T <SUB>eff</SUB> and [Fe/H]. Finally, we list a set of 23 interferometric binaries with masses known to be better than 3%, but without fundamental radius determinations (except α Aur). We discuss the prospects for improving these and other stellar parameters in the near future.", "database": ["astronomy"], "keywords": ["Stars: fundamental parameters", "Stars: binaries: eclipsing", "Stars: binaries: spectroscopic", "Stars: interiors", "Stars: evolution", "Astrophysics - Solar and Stellar Astrophysics", "Astrophysics - Galaxy Astrophysics"], "year": "2010", "doctype": "article", "citation_count": 1394, "domain_category": "planetary_science", "abstract_clean": "This article presents and discusses a critical compilation of accurate, fundamental determinations of stellar masses and radii. We have identified 95 detached binary systems containing 190 stars (94 eclipsing systems, and α Centauri) that satisfy our criterion that the mass and radius of both stars be known within errors of ±3% accuracy or better. All of them are non-interacting systems, and so the stars should have evolved as if they were single. This sample more than doubles that of the earlier similar review by Andersen (Astron Astrophys Rev 3:91-126, 1991), extends the mass range at both ends and, for the first time, includes an extragalactic binary. In every case, we have examined the original data and recomputed the stellar parameters with a consistent set of assumptions and physical constants. To these we add interstellar reddening, effective temperature, metal abundance, rotational velocity and apsidal motion determinations when available, and we compute a number of other physical parameters, notably luminosity and distance. These accurate physical parameters reveal the effects of stellar evolution with unprecedented clarity, and we discuss the use of the data in observational tests of stellar evolution models in some detail. Earlier findings of significant structural differences between moderately fast-rotating, mildly active stars and single stars, ascribed to the presence of strong magnetic and spot activity, are confirmed beyond doubt. We also show how the best data can be used to test prescriptions for the subtle interplay between convection, diffusion, and other non-classical effects in stellar models. The amount and quality of the data also allow us to analyse the tidal evolution of the systems in considerable depth, testing prescriptions of rotational synchronisation and orbital circularisation in greater detail than possible before. We show that the formulae for pseudo-synchronisation of stars in eccentric orbits predict the observed rotations quite well, except for very young and/or widely separated stars. Deviations do occur, however, especially for stars with convective envelopes. The superior data set finally demonstrates that apsidal motion rates as predicted from General Relativity plus tidal theory are in good agreement with the best observational data. No reliable binary data exist, which challenge General Relativity to any significant extent. The new data also enable us to derive empirical calibrations of M and R for single (post-) main-sequence stars above {0.6 M_{odot}}. Simple, polynomial functions of T eff, log g and [Fe/H] yield M and R within errors of 6 and 3%, respectively. Excellent agreement is found with independent determinations for host stars of transiting extrasolar planets, and good agreement with determinations of M and R from stellar models as constrained by trigonometric parallaxes and spectroscopic values of T eff and [Fe/H]. Finally, we list a set of 23 interferometric binaries with masses known to be better than 3%, but without fundamental radius determinations (except α Aur). We discuss the prospects for improving these and other stellar parameters in the near future."}
{"bibcode": "2006Sci...312..400B", "title": "Global Mineralogical and Aqueous Mars History Derived from OMEGA/Mars Express Data", "abstract": "Global mineralogical mapping of Mars by the Observatoire pour la Mineralogie, l'Eau, les Glaces et l'Activité (OMEGA) instrument on the European Space Agency's Mars Express spacecraft provides new information on Mars' geological and climatic history. Phyllosilicates formed by aqueous alteration very early in the planet's history (the ``phyllocian'' era) are found in the oldest terrains; sulfates were formed in a second era (the ``theiikian'' era) in an acidic environment. Beginning about 3.5 billion years ago, the last era (the ``siderikian'') is dominated by the formation of anhydrous ferric oxides in a slow superficial weathering, without liquid water playing a major role across the planet.", "database": ["astronomy", "general"], "keywords": ["PLANET SCI"], "year": "2006", "doctype": "article", "citation_count": 1391, "domain_category": "planetary_science", "abstract_clean": "Global mineralogical mapping of Mars by the Observatoire pour la Mineralogie, l'Eau, les Glaces et l'Activité (OMEGA) instrument on the European Space Agency's Mars Express spacecraft provides new information on Mars' geological and climatic history. Phyllosilicates formed by aqueous alteration very early in the planet's history (the ``phyllocian'' era) are found in the oldest terrains; sulfates were formed in a second era (the ``theiikian'' era) in an acidic environment. Beginning about 3.5 billion years ago, the last era (the ``siderikian'') is dominated by the formation of anhydrous ferric oxides in a slow superficial weathering, without liquid water playing a major role across the planet."}
{"bibcode": "1992RvMP...64.1045P", "title": "Iterative minimization techniques for ab initio total-energy calculations: molecular dynamics and conjugate gradients", "abstract": "This article describes recent technical developments that have made the total-energy pseudopotential the most powerful ab initio quantum-mechanical modeling method presently available. In addition to presenting technical details of the pseudopotential method, the article aims to heighten awareness of the capabilities of the method in order to stimulate its application to as wide a range of problems in as many scientific disciplines as possible.", "database": ["physics"], "keywords": [], "year": "1992", "doctype": "article", "citation_count": 6048, "domain_category": "multidisciplinary", "abstract_clean": "This article describes recent technical developments that have made the total-energy pseudopotential the most powerful ab initio quantum-mechanical modeling method presently available. In addition to presenting technical details of the pseudopotential method, the article aims to heighten awareness of the capabilities of the method in order to stimulate its application to as wide a range of problems in as many scientific disciplines as possible."}
{"bibcode": "1954PhRv...93...99D", "title": "Coherence in Spontaneous Radiation Processes", "abstract": "By considering a radiating gas as a single quantum-mechanical system, energy levels corresponding to certain correlations between individual molecules are described. Spontaneous emission of radiation in a transition between two such levels leads to the emission of coherent radiation. The discussion is limited first to a gas of dimension small compared with a wavelength. Spontaneous radiation rates and natural line breadths are calculated. For a gas of large extent the effect of photon recoil momentum on coherence is calculated. The effect of a radiation pulse in exciting \"super-radiant\" states is discussed. The angular correlation between successive photons spontaneously emitted by a gas initially in thermal equilibrium is calculated.", "database": ["physics"], "keywords": [], "year": "1954", "doctype": "article", "citation_count": 6044, "domain_category": "multidisciplinary", "abstract_clean": "By considering a radiating gas as a single quantum-mechanical system, energy levels corresponding to certain correlations between individual molecules are described. Spontaneous emission of radiation in a transition between two such levels leads to the emission of coherent radiation. The discussion is limited first to a gas of dimension small compared with a wavelength. Spontaneous radiation rates and natural line breadths are calculated. For a gas of large extent the effect of photon recoil momentum on coherence is calculated. The effect of a radiation pulse in exciting \"super-radiant\" states is discussed. The angular correlation between successive photons spontaneously emitted by a gas initially in thermal equilibrium is calculated."}
{"bibcode": "2006Natur.444..323J", "title": "The plant immune system", "abstract": "Many plant-associated microbes are pathogens that impair plant growth and reproduction. Plants respond to infection using a two-branched innate immune system. The first branch recognizes and responds to molecules common to many classes of microbes, including non-pathogens. The second responds to pathogen virulence factors, either directly or through their effects on host targets. These plant immune systems, and the pathogen molecules to which they respond, provide extraordinary insights into molecular recognition, cell biology and evolution across biological kingdoms. A detailed understanding of plant immune function will underpin crop improvement for food, fibre and biofuels production.", "database": ["general"], "keywords": [], "year": "2006", "doctype": "article", "citation_count": 5939, "domain_category": "multidisciplinary", "abstract_clean": "Many plant-associated microbes are pathogens that impair plant growth and reproduction. Plants respond to infection using a two-branched innate immune system. The first branch recognizes and responds to molecules common to many classes of microbes, including non-pathogens. The second responds to pathogen virulence factors, either directly or through their effects on host targets. These plant immune systems, and the pathogen molecules to which they respond, provide extraordinary insights into molecular recognition, cell biology and evolution across biological kingdoms. A detailed understanding of plant immune function will underpin crop improvement for food, fibre and biofuels production."}
{"bibcode": "1993RvMP...65..851C", "title": "Pattern formation outside of equilibrium", "abstract": "A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q<SUB>0</SUB> and frequency ω<SUB>0</SUB> of the instability. Type I<SUB>s</SUB> systems (ω<SUB>0</SUB>=0, q<SUB>0</SUB>≠0) are stationary in time and periodic in space; type III<SUB>o</SUB> systems (ω<SUB>0</SUB>≠0, q<SUB>0</SUB>=0) are periodic in time and uniform in space; and type I<SUB>o</SUB> systems (ω<SUB>0</SUB>≠0, q<SUB>0</SUB>≠0) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via \"amplitude equations,\" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the \"phase equation\" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing \"real pattern effects\" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying \"temporal chaos,\" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Bénard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future.", "database": ["astronomy", "physics"], "keywords": [], "year": "1993", "doctype": "article", "citation_count": 5787, "domain_category": "multidisciplinary", "abstract_clean": "A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments. Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times. A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q0 and frequency ω0 of the instability. Type Is systems (ω0=0, q0≠0) are stationary in time and periodic in space; type IIIo systems (ω0≠0, q0=0) are periodic in time and uniform in space; and type Io systems (ω0≠0, q0≠0) are periodic in both space and time. Near a continuous (or supercritical) instability, the dynamics may be accurately described via \"amplitude equations,\" whose form is universal for each type of instability. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the \"phase equation\" may be derived, but it is restricted to slow distortions of an ideal pattern. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing \"real pattern effects\" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying \"temporal chaos,\" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented. These include Rayleigh-Bénard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. A concluding section summarizes what has and has not been accomplished, and attempts to assess the prospects for the future."}
{"bibcode": "2013Ecogr..36...27D", "title": "Collinearity: a review of methods to deal with it and a simulation study evaluating their performance", "abstract": "Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the 'folk lore'-thresholds of correlation coefficients between predictor variables of |r| &gt;0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.", "database": ["earth science"], "keywords": [], "year": "2013", "doctype": "article", "citation_count": 5729, "domain_category": "multidisciplinary", "abstract_clean": "Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the 'folk lore'-thresholds of correlation coefficients between predictor variables of |r| >0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them."}
{"bibcode": "2001Sci...292.1897H", "title": "Room-Temperature Ultraviolet Nanowire Nanolasers", "abstract": "Room-temperature ultraviolet lasing in semiconductor nanowire arrays has been demonstrated. The self-organized, &lt;0001&gt; oriented zinc oxide nanowires grown on sapphire substrates were synthesized with a simple vapor transport and condensation process. These wide band-gap semiconductor nanowires form natural laser cavities with diameters varying from 20 to 150 nanometers and lengths up to 10 micrometers. Under optical excitation, surface-emitting lasing action was observed at 385 nanometers, with an emission linewidth less than 0.3 nanometer. The chemical flexibility and the one-dimensionality of the nanowires make them ideal miniaturized laser light sources. These short-wavelength nanolasers could have myriad applications, including optical computing, information storage, and microanalysis.", "database": ["physics", "general"], "keywords": ["APP PHYSICS"], "year": "2001", "doctype": "article", "citation_count": 5474, "domain_category": "multidisciplinary", "abstract_clean": "Room-temperature ultraviolet lasing in semiconductor nanowire arrays has been demonstrated. The self-organized, <0001> oriented zinc oxide nanowires grown on sapphire substrates were synthesized with a simple vapor transport and condensation process. These wide band-gap semiconductor nanowires form natural laser cavities with diameters varying from 20 to 150 nanometers and lengths up to 10 micrometers. Under optical excitation, surface-emitting lasing action was observed at 385 nanometers, with an emission linewidth less than 0.3 nanometer. The chemical flexibility and the one-dimensionality of the nanowires make them ideal miniaturized laser light sources. These short-wavelength nanolasers could have myriad applications, including optical computing, information storage, and microanalysis."}
{"bibcode": "2010Bioin..26.2460E", "title": "Search and clustering orders of magnitude faster than BLAST", "abstract": "Motivation: Biological sequence data is accumulating rapidly, motivating the development of improved high-throughput methods for sequence classification. Results: UBLAST and USEARCH are new algorithms enabling sensitive local and global search of large sequence databases at exceptionally high speeds. They are often orders of magnitude faster than BLAST in practical applications, though sensitivity to distant protein relationships is lower. UCLUST is a new clustering method that exploits USEARCH to assign sequences to clusters. UCLUST offers several advantages over the widely used program CD-HIT, including higher speed, lower memory use, improved sensitivity, clustering at lower identities and classification of much larger datasets. Availability: Binaries are available at no charge for non-commercial use at http://www.drive5.com/usearch Contact: robert@drive5.com Supplementary information: Supplementary data are available at Bioinformatics online.", "database": ["general"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 5270, "domain_category": "multidisciplinary", "abstract_clean": "Motivation: Biological sequence data is accumulating rapidly, motivating the development of improved high-throughput methods for sequence classification. Results: UBLAST and USEARCH are new algorithms enabling sensitive local and global search of large sequence databases at exceptionally high speeds. They are often orders of magnitude faster than BLAST in practical applications, though sensitivity to distant protein relationships is lower. UCLUST is a new clustering method that exploits USEARCH to assign sequences to clusters. UCLUST offers several advantages over the widely used program CD-HIT, including higher speed, lower memory use, improved sensitivity, clustering at lower identities and classification of much larger datasets. Availability: Binaries are available at no charge for non-commercial use at http://www.drive5.com/usearch Contact: robert@drive5.com Supplementary information: Supplementary data are available at Bioinformatics online."}
{"bibcode": "2006PhR...424..175B", "title": "Complex networks: Structure and dynamics", "abstract": "Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks’ dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering.", "database": ["physics"], "keywords": [], "year": "2006", "doctype": "article", "citation_count": 5015, "domain_category": "multidisciplinary", "abstract_clean": "Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks’ dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering."}
{"bibcode": "1954PhRv...94..511B", "title": "A Model for Collision Processes in Gases. I. Small Amplitude Processes in Charged and Neutral One-Component Systems", "abstract": "A kinetic theory approach to collision processes in ionized and neutral gases is presented. This approach is adequate for the unified treatment of the dynamic properties of gases over a continuous range of pressures from the Knudsen limit to the high-pressure limit where the aerodynamic equations are valid. It is also possible to satisfy the correct microscopic boundary conditions. The method consists in altering the collision terms in the Boltzmann equation. The modified collision terms are constructed so that each collision conserves particle number, momentum, and energy; other characteristics such as persistence of velocities and angular dependence may be included. The present article illustrates the technique for a simple model involving the assumption of a collision time independent of velocity; this model is applied to the study of small amplitude oscillations of one-component ionized and neutral gases. The initial value problem for unbounded space is solved by performing a Fourier transformation on the space variables and a Laplace transformation on the time variable. For uncharged gases there results the correct adiabatic limiting law for sound-wave propagation at high pressures and, in addition, one obtains a theory of absorption and dispersion of sound for arbitrary pressures. For ionized gases the difference in the nature of the organization in the low-pressure plasma oscillations and in high-pressure sound-type oscillations is studied. Two important cases are distinguished. If the wavelengths of the oscillations are long compared to either the Debye length or the mean free path, a small change in frequency is obtained as the collision frequency varies from zero to infinity. The accompanying absorption is small; it reaches its maximum value when the collision frequency equals the plasma frequency. The second case refers to waves shorter than both the Debye length and the mean free path; these waves are characterized by a very heavy absorption.", "database": ["physics"], "keywords": [], "year": "1954", "doctype": "article", "citation_count": 5001, "domain_category": "multidisciplinary", "abstract_clean": "A kinetic theory approach to collision processes in ionized and neutral gases is presented. This approach is adequate for the unified treatment of the dynamic properties of gases over a continuous range of pressures from the Knudsen limit to the high-pressure limit where the aerodynamic equations are valid. It is also possible to satisfy the correct microscopic boundary conditions. The method consists in altering the collision terms in the Boltzmann equation. The modified collision terms are constructed so that each collision conserves particle number, momentum, and energy; other characteristics such as persistence of velocities and angular dependence may be included. The present article illustrates the technique for a simple model involving the assumption of a collision time independent of velocity; this model is applied to the study of small amplitude oscillations of one-component ionized and neutral gases. The initial value problem for unbounded space is solved by performing a Fourier transformation on the space variables and a Laplace transformation on the time variable. For uncharged gases there results the correct adiabatic limiting law for sound-wave propagation at high pressures and, in addition, one obtains a theory of absorption and dispersion of sound for arbitrary pressures. For ionized gases the difference in the nature of the organization in the low-pressure plasma oscillations and in high-pressure sound-type oscillations is studied. Two important cases are distinguished. If the wavelengths of the oscillations are long compared to either the Debye length or the mean free path, a small change in frequency is obtained as the collision frequency varies from zero to infinity. The accompanying absorption is small; it reaches its maximum value when the collision frequency equals the plasma frequency. The second case refers to waves shorter than both the Debye length and the mean free path; these waves are characterized by a very heavy absorption."}
{"bibcode": "1998Natur.391..667E", "title": "Extraordinary optical transmission through sub-wavelength hole arrays", "abstract": "The desire to use and control photons in a manner analogous to the control of electrons in solids has inspired great interest in such topics as the localization of light, microcavity quantum electrodynamics and near-field optics. A fundamental constraint in manipulating light is the extremely low transmittivity of apertures smaller than the wavelength of the incident photon. While exploring the optical properties of submicrometre cylindrical cavities in metallic films, we have found that arrays of such holes display highly unusual zero-order transmission spectra (where the incident and detected light are collinear) at wavelengths larger than the array period, beyond which no diffraction occurs. In particular, sharp peaks in transmission are observed at wavelengths as large as ten times the diameter of the cylinders. At these maxima the transmission efficiency can exceed unity (when normalized to the area of the holes), which is orders of magnitude greater than predicted by standard aperture theory. Our experiments provide evidence that these unusual optical properties are due to the coupling of light with plasmons - electronic excitations - on the surface of the periodically patterned metal film. Measurements of transmission as a function of the incident light angle result in a photonic band diagram. These findings may find application in novel photonic devices.", "database": ["physics", "general"], "keywords": [], "year": "1998", "doctype": "article", "citation_count": 4768, "domain_category": "multidisciplinary", "abstract_clean": "The desire to use and control photons in a manner analogous to the control of electrons in solids has inspired great interest in such topics as the localization of light, microcavity quantum electrodynamics and near-field optics. A fundamental constraint in manipulating light is the extremely low transmittivity of apertures smaller than the wavelength of the incident photon. While exploring the optical properties of submicrometre cylindrical cavities in metallic films, we have found that arrays of such holes display highly unusual zero-order transmission spectra (where the incident and detected light are collinear) at wavelengths larger than the array period, beyond which no diffraction occurs. In particular, sharp peaks in transmission are observed at wavelengths as large as ten times the diameter of the cylinders. At these maxima the transmission efficiency can exceed unity (when normalized to the area of the holes), which is orders of magnitude greater than predicted by standard aperture theory. Our experiments provide evidence that these unusual optical properties are due to the coupling of light with plasmons - electronic excitations - on the surface of the periodically patterned metal film. Measurements of transmission as a function of the incident light angle result in a photonic band diagram. These findings may find application in novel photonic devices."}
{"bibcode": "2010Bioin..26..139R", "title": "edgeR: a Bioconductor package for differential expression analysis of digital gene expression data", "abstract": "Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An overdispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of overdispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au", "database": ["general"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 4697, "domain_category": "multidisciplinary", "abstract_clean": "Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An overdispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of overdispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au"}
{"bibcode": "1990RvMP...62..251H", "title": "Reaction-rate theory: fifty years after Kramers", "abstract": "The calculation of rate coefficients is a discipline of nonlinear science of importance to much of physics, chemistry, engineering, and biology. Fifty years after Kramers' seminal paper on thermally activated barrier crossing, the authors report, extend, and interpret much of our current understanding relating to theories of noise-activated escape, for which many of the notable contributions are originating from the communities both of physics and of physical chemistry. Theoretical as well as numerical approaches are discussed for single- and many-dimensional metastable systems (including fields) in gases and condensed phases. The role of many-dimensional transition-state theory is contrasted with Kramers' reaction-rate theory for moderate-to-strong friction; the authors emphasize the physical situation and the close connection between unimolecular rate theory and Kramers' work for weakly damped systems. The rate theory accounting for memory friction is presented, together with a unifying theoretical approach which covers the whole regime of weak-to-moderate-to-strong friction on the same basis (turnover theory). The peculiarities of noise-activated escape in a variety of physically different metastable potential configurations is elucidated in terms of the mean-first-passage-time technique. Moreover, the role and the complexity of escape in driven systems exhibiting possibly multiple, metastable stationary nonequilibrium states is identified. At lower temperatures, quantum tunneling effects start to dominate the rate mechanism. The early quantum approaches as well as the latest quantum versions of Kramers' theory are discussed, thereby providing a description of dissipative escape events at all temperatures. In addition, an attempt is made to discuss prominent experimental work as it relates to Kramers' reaction-rate theory and to indicate the most important areas for future research in theory and experiment.", "database": ["physics"], "keywords": [], "year": "1990", "doctype": "article", "citation_count": 4630, "domain_category": "multidisciplinary", "abstract_clean": "The calculation of rate coefficients is a discipline of nonlinear science of importance to much of physics, chemistry, engineering, and biology. Fifty years after Kramers' seminal paper on thermally activated barrier crossing, the authors report, extend, and interpret much of our current understanding relating to theories of noise-activated escape, for which many of the notable contributions are originating from the communities both of physics and of physical chemistry. Theoretical as well as numerical approaches are discussed for single- and many-dimensional metastable systems (including fields) in gases and condensed phases. The role of many-dimensional transition-state theory is contrasted with Kramers' reaction-rate theory for moderate-to-strong friction; the authors emphasize the physical situation and the close connection between unimolecular rate theory and Kramers' work for weakly damped systems. The rate theory accounting for memory friction is presented, together with a unifying theoretical approach which covers the whole regime of weak-to-moderate-to-strong friction on the same basis (turnover theory). The peculiarities of noise-activated escape in a variety of physically different metastable potential configurations is elucidated in terms of the mean-first-passage-time technique. Moreover, the role and the complexity of escape in driven systems exhibiting possibly multiple, metastable stationary nonequilibrium states is identified. At lower temperatures, quantum tunneling effects start to dominate the rate mechanism. The early quantum approaches as well as the latest quantum versions of Kramers' theory are discussed, thereby providing a description of dissipative escape events at all temperatures. In addition, an attempt is made to discuss prominent experimental work as it relates to Kramers' reaction-rate theory and to indicate the most important areas for future research in theory and experiment."}
{"bibcode": "2012Bioin..28.1647K", "title": "Geneious Basic: An integrated and extendable desktop software platform for the organization and analysis of sequence data", "abstract": "Summary: The two main functions of bioinformatics are the organization and analysis of biological data using computational resources. Geneious Basic has been designed to be an easy-to-use and flexible desktop software application framework for the organization and analysis of biological data, with a focus on molecular sequences and related data types. It integrates numerous industry-standard discovery analysis tools, with interactive visualizations to generate publication-ready images. One key contribution to researchers in the life sciences is the Geneious public application programming interface (API) that affords the ability to leverage the existing framework of the Geneious Basic software platform for virtually unlimited extension and customization. The result is an increase in the speed and quality of development of computation tools for the life sciences, due to the functionality and graphical user interface available to the developer through the public API. Geneious Basic represents an ideal platform for the bioinformatics community to leverage existing components and to integrate their own specific requirements for the discovery, analysis and visualization of biological data. Availability and implementation: Binaries and public API freely available for download at http://www.geneious.com/basic, implemented in Java and supported on Linux, Apple OSX and MS Windows. The software is also available from the Bio-Linux package repository at http://nebc.nerc.ac.uk/news/geneiousonbl. Contact: peter@biomatters.com", "database": ["general"], "keywords": [], "year": "2012", "doctype": "article", "citation_count": 4610, "domain_category": "multidisciplinary", "abstract_clean": "Summary: The two main functions of bioinformatics are the organization and analysis of biological data using computational resources. Geneious Basic has been designed to be an easy-to-use and flexible desktop software application framework for the organization and analysis of biological data, with a focus on molecular sequences and related data types. It integrates numerous industry-standard discovery analysis tools, with interactive visualizations to generate publication-ready images. One key contribution to researchers in the life sciences is the Geneious public application programming interface (API) that affords the ability to leverage the existing framework of the Geneious Basic software platform for virtually unlimited extension and customization. The result is an increase in the speed and quality of development of computation tools for the life sciences, due to the functionality and graphical user interface available to the developer through the public API. Geneious Basic represents an ideal platform for the bioinformatics community to leverage existing components and to integrate their own specific requirements for the discovery, analysis and visualization of biological data. Availability and implementation: Binaries and public API freely available for download at http://www.geneious.com/basic, implemented in Java and supported on Linux, Apple OSX and MS Windows. The software is also available from the Bio-Linux package repository at http://nebc.nerc.ac.uk/news/geneiousonbl. Contact: peter@biomatters.com"}
{"bibcode": "2007Carbo..45.1558S", "title": "Synthesis of graphene-based nanosheets via chemical reduction of exfoliated graphite oxide", "abstract": "Reduction of a colloidal suspension of exfoliated graphene oxide sheets in water with hydrazine hydrate results in their aggregation and subsequent formation of a high-surface-area carbon material which consists of thin graphene-based sheets. The reduced material was characterized by elemental analysis, thermo-gravimetric analysis, scanning electron microscopy, X-ray photoelectron spectroscopy, NMR spectroscopy, Raman spectroscopy, and by electrical conductivity measurements.", "database": ["earth science"], "keywords": [], "year": "2007", "doctype": "article", "citation_count": 4588, "domain_category": "multidisciplinary", "abstract_clean": "Reduction of a colloidal suspension of exfoliated graphene oxide sheets in water with hydrazine hydrate results in their aggregation and subsequent formation of a high-surface-area carbon material which consists of thin graphene-based sheets. The reduced material was characterized by elemental analysis, thermo-gravimetric analysis, scanning electron microscopy, X-ray photoelectron spectroscopy, NMR spectroscopy, Raman spectroscopy, and by electrical conductivity measurements."}
{"bibcode": "1988RSEnv..25..295H", "title": "A soil-adjusted vegetation index (SAVI)", "abstract": "A transformation technique is presented to minimize soil brightness influences from spectral vegetation indices involving red and near-infrared (NIR) wavelengths. Graphically, the transformation involves a shifting of the origin of reflectance spectra plotted in NIR-red wavelength space to account for first-order soil-vegetation interactions and differential red and NIR flux extinction through vegetated canopies. For cotton (Gossypium hirsutum L. var DPI-70) and range grass (Eragrostics lehmanniana Nees) canopies, underlain with different soil backgrounds, the transformation nearly eliminated soil-induced variations in vegetation indices. A physical basis for the soil-adjusted vegetation index (SAVI) is subsequently presented. The SAVI was found to be an important step toward the establishment of simple °lobal\" that can describe dynamic soil-vegetation systems from remotely sensed data.", "database": ["physics", "earth science"], "keywords": [], "year": "1988", "doctype": "article", "citation_count": 4533, "domain_category": "multidisciplinary", "abstract_clean": "A transformation technique is presented to minimize soil brightness influences from spectral vegetation indices involving red and near-infrared (NIR) wavelengths. Graphically, the transformation involves a shifting of the origin of reflectance spectra plotted in NIR-red wavelength space to account for first-order soil-vegetation interactions and differential red and NIR flux extinction through vegetated canopies. For cotton (Gossypium hirsutum L. var DPI-70) and range grass (Eragrostics lehmanniana Nees) canopies, underlain with different soil backgrounds, the transformation nearly eliminated soil-induced variations in vegetation indices. A physical basis for the soil-adjusted vegetation index (SAVI) is subsequently presented. The SAVI was found to be an important step toward the establishment of simple °lobal\" that can describe dynamic soil-vegetation systems from remotely sensed data."}
{"bibcode": "2013MEcEv...4..133N", "title": "A general and simple method for obtaining R<SUP>2</SUP> from generalized linear mixed-effects models", "abstract": "Summary The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models. The presentation of 'variance explained' (R<SUP>2</SUP>) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R<SUP>2</SUP> is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R<SUP>2</SUP> has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R<SUP>2</SUP> can also be a quantity of biological interest. One reason for the under-appreciation of R<SUP>2</SUP> for mixed-effects models lies in the fact that R<SUP>2</SUP> can be defined in a number of ways. Furthermore, most definitions of R<SUP>2</SUP> for mixed-effects have theoretical problems (e.g. decreased or negative R<SUP>2</SUP> values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation). Here, we make a case for the importance of reporting R<SUP>2</SUP> for mixed-effects models. We first provide the common definitions of R<SUP>2</SUP> for LMs and GLMs and discuss the key problems associated with calculating R<SUP>2</SUP> for mixed-effects models. We then recommend a general and simple method for calculating two types of R<SUP>2</SUP> (marginal and conditional R<SUP>2</SUP>) for both LMMs and GLMMs, which are less susceptible to common problems. This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R<SUP>2</SUP> for a wide range of circumstances.", "database": ["earth science"], "keywords": ["coefficient of determination", "goodness-of-fit", "heritability", "information criteria", "intra-class correlation", "linear models", "model fit", "repeatability", "variance explained"], "year": "2013", "doctype": "article", "citation_count": 4408, "domain_category": "multidisciplinary", "abstract_clean": "Summary The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models. The presentation of 'variance explained' (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest. One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation). Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems. This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances."}
{"bibcode": "2008Sci...320..889G", "title": "Transformation of the Nitrogen Cycle:  Recent Trends, Questions, and Potential Solutions", "abstract": "Humans continue to transform the global nitrogen cycle at a record pace, reflecting an increased combustion of fossil fuels, growing demand for nitrogen in agriculture and industry, and pervasive inefficiencies in its use. Much anthropogenic nitrogen is lost to air, water, and land to cause a cascade of environmental and human health problems. Simultaneously, food production in some parts of the world is nitrogen-deficient, highlighting inequities in the distribution of nitrogen-containing fertilizers. Optimizing the need for a key human resource while minimizing its negative consequences requires an integrated interdisciplinary approach and the development of strategies to decrease nitrogen-containing waste.", "database": ["general"], "keywords": [], "year": "2008", "doctype": "article", "citation_count": 4346, "domain_category": "multidisciplinary", "abstract_clean": "Humans continue to transform the global nitrogen cycle at a record pace, reflecting an increased combustion of fossil fuels, growing demand for nitrogen in agriculture and industry, and pervasive inefficiencies in its use. Much anthropogenic nitrogen is lost to air, water, and land to cause a cascade of environmental and human health problems. Simultaneously, food production in some parts of the world is nitrogen-deficient, highlighting inequities in the distribution of nitrogen-containing fertilizers. Optimizing the need for a key human resource while minimizing its negative consequences requires an integrated interdisciplinary approach and the development of strategies to decrease nitrogen-containing waste."}
{"bibcode": "2012Bioin..28.2537P", "title": "GenAlEx 6.5: genetic analysis in Excel. Population genetic software for teaching and research—an update", "abstract": "Summary: GenAlEx: Genetic Analysis in Excel is a cross-platform package for population genetic analyses that runs within Microsoft Excel. GenAlEx offers analysis of diploid codominant, haploid and binary genetic loci and DNA sequences. Both frequency-based (F-statistics, heterozygosity, HWE, population assignment, relatedness) and distance-based (AMOVA, PCoA, Mantel tests, multivariate spatial autocorrelation) analyses are provided. New features include calculation of new estimators of population structure: G′ST, G′′ST, Jost's Dest and F′ST through AMOVA, Shannon Information analysis, linkage disequilibrium analysis for biallelic data and novel heterogeneity tests for spatial autocorrelation analysis. Export to more than 30 other data formats is provided. Teaching tutorials and expanded step-by-step output options are included. The comprehensive guide has been fully revised. Availability and implementation: GenAlEx is written in VBA and provided as a Microsoft Excel Add-in (compatible with Excel 2003, 2007, 2010 on PC; Excel 2004, 2011 on Macintosh). GenAlEx, and supporting documentation and tutorials are freely available at: http://biology.anu.edu.au/GenAlEx. Contact: rod.peakall@anu.edu.au", "database": ["general"], "keywords": [], "year": "2012", "doctype": "article", "citation_count": 4277, "domain_category": "multidisciplinary", "abstract_clean": "Summary: GenAlEx: Genetic Analysis in Excel is a cross-platform package for population genetic analyses that runs within Microsoft Excel. GenAlEx offers analysis of diploid codominant, haploid and binary genetic loci and DNA sequences. Both frequency-based (F-statistics, heterozygosity, HWE, population assignment, relatedness) and distance-based (AMOVA, PCoA, Mantel tests, multivariate spatial autocorrelation) analyses are provided. New features include calculation of new estimators of population structure: G′ST, G′′ST, Jost's Dest and F′ST through AMOVA, Shannon Information analysis, linkage disequilibrium analysis for biallelic data and novel heterogeneity tests for spatial autocorrelation analysis. Export to more than 30 other data formats is provided. Teaching tutorials and expanded step-by-step output options are included. The comprehensive guide has been fully revised. Availability and implementation: GenAlEx is written in VBA and provided as a Microsoft Excel Add-in (compatible with Excel 2003, 2007, 2010 on PC; Excel 2004, 2011 on Macintosh). GenAlEx, and supporting documentation and tutorials are freely available at: http://biology.anu.edu.au/GenAlEx. Contact: rod.peakall@anu.edu.au"}
{"bibcode": "1982PNAS...79.2554H", "title": "Neural Networks and Physical Systems with Emergent Collective Computational Abilities", "abstract": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.", "database": ["physics", "general"], "keywords": [], "year": "1982", "doctype": "article", "citation_count": 4268, "domain_category": "multidisciplinary", "abstract_clean": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."}
{"bibcode": "2011ApSS..257.2717B", "title": "Resolving surface chemical states in XPS analysis of first row transition metals, oxides and hydroxides: Cr, Mn, Fe, Co and Ni", "abstract": "Chemical state X-ray photoelectron spectroscopic analysis of first row transition metals and their oxides and hydroxides is challenging due to the complexity of their 2p spectra resulting from peak asymmetries, complex multiplet splitting, shake-up and plasmon loss structure, and uncertain, overlapping binding energies. Our previous paper [M.C. Biesinger et al., Appl. Surf. Sci. 257 (2010) 887-898.] in which we examined Sc, Ti, V, Cu and Zn species, has shown that all the values of the spectral fitting parameters for each specific species, i.e. binding energy (eV), full wide at half maximum (FWHM) value (eV) for each pass energy, spin-orbit splitting values and asymmetric peak shape fitting parameters, are not all normally provided in the literature and data bases, and are necessary for reproducible, quantitative chemical state analysis. A more consistent, practical and effective approach to curve fitting was developed based on a combination of (1) standard spectra from quality reference samples, (2) a survey of appropriate literature databases and/or a compilation of literature references and (3) specific literature references where fitting procedures are available. This paper extends this approach to the chemical states of Cr, Mn, Fe, Co and Ni metals, and various oxides and hydroxides where intense, complex multiplet splitting in many of the chemical states of these elements poses unique difficulties for chemical state analysis. The curve fitting procedures proposed use the same criteria as proposed previously but with the additional complexity of fitting of multiplet split spectra which has been done based on spectra of numerous reference materials and theoretical XPS modeling of these transition metal species. Binding energies, FWHM values, asymmetric peak shape fitting parameters, multiplet peak separation and peak area percentages are presented. The procedures developed can be utilized to remove uncertainties in the analysis of surface states in nano-particles, corrosion, catalysis and surface-engineered materials.", "database": ["physics"], "keywords": [], "year": "2011", "doctype": "article", "citation_count": 4267, "domain_category": "multidisciplinary", "abstract_clean": "Chemical state X-ray photoelectron spectroscopic analysis of first row transition metals and their oxides and hydroxides is challenging due to the complexity of their 2p spectra resulting from peak asymmetries, complex multiplet splitting, shake-up and plasmon loss structure, and uncertain, overlapping binding energies. Our previous paper [M.C. Biesinger et al., Appl. Surf. Sci. 257 (2010) 887-898.] in which we examined Sc, Ti, V, Cu and Zn species, has shown that all the values of the spectral fitting parameters for each specific species, i.e. binding energy (eV), full wide at half maximum (FWHM) value (eV) for each pass energy, spin-orbit splitting values and asymmetric peak shape fitting parameters, are not all normally provided in the literature and data bases, and are necessary for reproducible, quantitative chemical state analysis. A more consistent, practical and effective approach to curve fitting was developed based on a combination of (1) standard spectra from quality reference samples, (2) a survey of appropriate literature databases and/or a compilation of literature references and (3) specific literature references where fitting procedures are available. This paper extends this approach to the chemical states of Cr, Mn, Fe, Co and Ni metals, and various oxides and hydroxides where intense, complex multiplet splitting in many of the chemical states of these elements poses unique difficulties for chemical state analysis. The curve fitting procedures proposed use the same criteria as proposed previously but with the additional complexity of fitting of multiplet split spectra which has been done based on spectra of numerous reference materials and theoretical XPS modeling of these transition metal species. Binding energies, FWHM values, asymmetric peak shape fitting parameters, multiplet peak separation and peak area percentages are presented. The procedures developed can be utilized to remove uncertainties in the analysis of surface states in nano-particles, corrosion, catalysis and surface-engineered materials."}
{"bibcode": "1999ITMTT..47.2075P", "title": "Magnetism from conductors and enhanced nonlinear phenomena", "abstract": "We show that microstructures built from nonmagnetic conducting sheets exhibit an effective magnetic permeability /spl mu//sub eff/, which can be tuned to values not accessible in naturally occurring materials, including large imaginary components of /spl mu//sub eff/. The microstructure is on a scale much less than the wavelength of radiation, is not resolved by incident microwaves, and uses a very low density of metal so that structures can be extremely lightweight. Most of the structures are resonant due to internal capacitance and inductance, and resonant enhancement combined with compression of electrical energy into a very small volume greatly enhances the energy density at critical locations in the structure, easily by factors of a million and possibly by much more. Weakly nonlinear materials placed at these critical locations will show greatly enhanced effects raising the possibility of manufacturing active structures whose properties can be switched at will between many states.", "database": ["physics"], "keywords": ["Conductors", "Conducting materials", "Microstructure", "Magnetic materials", "Resonance", "Permeability", "Sheet materials", "Energy resolution", "Capacitance", "Inductance"], "year": "1999", "doctype": "article", "citation_count": 4259, "domain_category": "multidisciplinary", "abstract_clean": "We show that microstructures built from nonmagnetic conducting sheets exhibit an effective magnetic permeability /spl mu//sub eff/, which can be tuned to values not accessible in naturally occurring materials, including large imaginary components of /spl mu//sub eff/. The microstructure is on a scale much less than the wavelength of radiation, is not resolved by incident microwaves, and uses a very low density of metal so that structures can be extremely lightweight. Most of the structures are resonant due to internal capacitance and inductance, and resonant enhancement combined with compression of electrical energy into a very small volume greatly enhances the energy density at critical locations in the structure, easily by factors of a million and possibly by much more. Weakly nonlinear materials placed at these critical locations will show greatly enhanced effects raising the possibility of manufacturing active structures whose properties can be switched at will between many states."}
{"bibcode": "2010Natur.463..747M", "title": "The next generation of scenarios for climate change research and assessment", "abstract": "Advances in the science and observation of climate change are providing a clearer understanding of the inherent variability of Earth's climate system and its likely response to human and natural influences. The implications of climate change for the environment and society will depend not only on the response of the Earth system to changes in radiative forcings, but also on how humankind responds through changes in technology, economies, lifestyle and policy. Extensive uncertainties exist in future forcings of and responses to climate change, necessitating the use of scenarios of the future to explore the potential consequences of different response options. To date, such scenarios have not adequately examined crucial possibilities, such as climate change mitigation and adaptation, and have relied on research processes that slowed the exchange of information among physical, biological and social scientists. Here we describe a new process for creating plausible scenarios to investigate some of the most challenging and important questions about climate change confronting the global community.", "database": ["general"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 4252, "domain_category": "multidisciplinary", "abstract_clean": "Advances in the science and observation of climate change are providing a clearer understanding of the inherent variability of Earth's climate system and its likely response to human and natural influences. The implications of climate change for the environment and society will depend not only on the response of the Earth system to changes in radiative forcings, but also on how humankind responds through changes in technology, economies, lifestyle and policy. Extensive uncertainties exist in future forcings of and responses to climate change, necessitating the use of scenarios of the future to explore the potential consequences of different response options. To date, such scenarios have not adequately examined crucial possibilities, such as climate change mitigation and adaptation, and have relied on research processes that slowed the exchange of information among physical, biological and social scientists. Here we describe a new process for creating plausible scenarios to investigate some of the most challenging and important questions about climate change confronting the global community."}
{"bibcode": "2012Natur.486...59C", "title": "Biodiversity loss and its impact on humanity", "abstract": "The most unique feature of Earth is the existence of life, and the most extraordinary feature of life is its diversity. Approximately 9 million types of plants, animals, protists and fungi inhabit the Earth. So, too, do 7 billion people. Two decades ago, at the first Earth Summit, the vast majority of the world's nations declared that human actions were dismantling the Earth's ecosystems, eliminating genes, species and biological traits at an alarming rate. This observation led to the question of how such loss of biological diversity will alter the functioning of ecosystems and their ability to provide society with the goods and services needed to prosper.", "database": ["general"], "keywords": [], "year": "2012", "doctype": "article", "citation_count": 4193, "domain_category": "multidisciplinary", "abstract_clean": "The most unique feature of Earth is the existence of life, and the most extraordinary feature of life is its diversity. Approximately 9 million types of plants, animals, protists and fungi inhabit the Earth. So, too, do 7 billion people. Two decades ago, at the first Earth Summit, the vast majority of the world's nations declared that human actions were dismantling the Earth's ecosystems, eliminating genes, species and biological traits at an alarming rate. This observation led to the question of how such loss of biological diversity will alter the functioning of ecosystems and their ability to provide society with the goods and services needed to prosper."}
{"bibcode": "2021MBioE..38.3022T", "title": "MEGA11: Molecular Evolutionary Genetics Analysis Version 11", "abstract": "The Molecular Evolutionary Genetics Analysis (MEGA) software has matured to contain a large collection of methods and tools of computational molecular evolution. Here, we describe new additions that make MEGA a more comprehensive tool for building timetrees of species, pathogens, and gene families using rapid relaxed-clock methods. Methods for estimating divergence times and confidence intervals are implemented to use probability densities for calibration constraints for node-dating and sequence sampling dates for tip-dating analyses. They are supported by new options for tagging sequences with spatiotemporal sampling information, an expanded interactive Node Calibrations Editor, and an extended Tree Explorer to display timetrees. Also added is a Bayesian method for estimating neutral evolutionary probabilities of alleles in a species using multispecies sequence alignments and a machine learning method to test for the autocorrelation of evolutionary rates in phylogenies. The computer memory requirements for the maximum likelihood analysis are reduced significantly through reprogramming, and the graphical user interface has been made more responsive and interactive for very big data sets. These enhancements will improve the user experience, quality of results, and the pace of biological discovery. Natively compiled graphical user interface and command-line versions of MEGA11 are available for Microsoft Windows, Linux, and macOS from www.megasoftware.net.", "database": ["general"], "keywords": [], "year": "2021", "doctype": "article", "citation_count": 4176, "domain_category": "multidisciplinary", "abstract_clean": "The Molecular Evolutionary Genetics Analysis (MEGA) software has matured to contain a large collection of methods and tools of computational molecular evolution. Here, we describe new additions that make MEGA a more comprehensive tool for building timetrees of species, pathogens, and gene families using rapid relaxed-clock methods. Methods for estimating divergence times and confidence intervals are implemented to use probability densities for calibration constraints for node-dating and sequence sampling dates for tip-dating analyses. They are supported by new options for tagging sequences with spatiotemporal sampling information, an expanded interactive Node Calibrations Editor, and an extended Tree Explorer to display timetrees. Also added is a Bayesian method for estimating neutral evolutionary probabilities of alleles in a species using multispecies sequence alignments and a machine learning method to test for the autocorrelation of evolutionary rates in phylogenies. The computer memory requirements for the maximum likelihood analysis are reduced significantly through reprogramming, and the graphical user interface has been made more responsive and interactive for very big data sets. These enhancements will improve the user experience, quality of results, and the pace of biological discovery. Natively compiled graphical user interface and command-line versions of MEGA11 are available for Microsoft Windows, Linux, and macOS from www.megasoftware.net."}
{"bibcode": "1990Natur.347..354K", "title": "Solid C<SUB>60</SUB>: a new form of carbon", "abstract": "A new form of pure, solid carbon has been synthesized consisting of a somewhat disordered hexagonal close packing of soccer-ball-shaped C<SUB>60</SUB> molecules. Infrared spectra and X-ray diffraction studies of the molecular packing confirm that the molecules have the anticipated 'fullerene' structure. Mass spectroscopy shows that the C<SUB>70</SUB> molecule is present at levels of a few per cent. The solid-state and molecular properties of C<SUB>60</SUB> and its possible role in interstellar space can now be studied in detail.", "database": ["astronomy", "physics", "general"], "keywords": [], "year": "1990", "doctype": "article", "citation_count": 4169, "domain_category": "multidisciplinary", "abstract_clean": "A new form of pure, solid carbon has been synthesized consisting of a somewhat disordered hexagonal close packing of soccer-ball-shaped C60 molecules. Infrared spectra and X-ray diffraction studies of the molecular packing confirm that the molecules have the anticipated 'fullerene' structure. Mass spectroscopy shows that the C70 molecule is present at levels of a few per cent. The solid-state and molecular properties of C60 and its possible role in interstellar space can now be studied in detail."}
{"bibcode": "2011Natur.478...49S", "title": "Persistence of soil organic matter as an ecosystem property", "abstract": "Globally, soil organic matter (SOM) contains more than three times as much carbon as either the atmosphere or terrestrial vegetation. Yet it remains largely unknown why some SOM persists for millennia whereas other SOM decomposes readily--and this limits our ability to predict how soils will respond to climate change. Recent analytical and experimental advances have demonstrated that molecular structure alone does not control SOM stability: in fact, environmental and biological controls predominate. Here we propose ways to include this understanding in a new generation of experiments and soil carbon models, thereby improving predictions of the SOM response to global warming.", "database": ["general"], "keywords": [], "year": "2011", "doctype": "article", "citation_count": 4124, "domain_category": "multidisciplinary", "abstract_clean": "Globally, soil organic matter (SOM) contains more than three times as much carbon as either the atmosphere or terrestrial vegetation. Yet it remains largely unknown why some SOM persists for millennia whereas other SOM decomposes readily--and this limits our ability to predict how soils will respond to climate change. Recent analytical and experimental advances have demonstrated that molecular structure alone does not control SOM stability: in fact, environmental and biological controls predominate. Here we propose ways to include this understanding in a new generation of experiments and soil carbon models, thereby improving predictions of the SOM response to global warming."}
{"bibcode": "2004ChGeo.211...47J", "title": "The application of laser ablation-inductively coupled plasma-mass spectrometry to in situ U-Pb zircon geochronology", "abstract": "This paper reports new developments in in situ U-Pb zircon geochronology using 266 and 213 nm laser ablation-inductively coupled plasma-mass spectrometry (LA-ICP-MS). <P />Standard spot ablation (spot diameters 40-80 μm) was employed, with no sampling strategies employed specifically to minimise elemental fractionation. Instead, He ablation gas and carefully replicated ablation conditions were employed to maintain constant ablation-related elemental fractionation of Pb and U between analyses. Combining these strategies with calibration on a new zircon standard (GJ-1) allows elemental fractionation and instrumental mass bias to be corrected efficiently, and accurate <SUP>206</SUP>Pb/ <SUP>238</SUP>U and <SUP>207</SUP>Pb/ <SUP>235</SUP>U ratios to be measured with short-term precision (2 r.s.d.) of 1.9% and 3.0%, respectively. <P />Long-term precision (2 r.s.d.) of the technique (266 nm ablation), based on 355 analyses of the 91500 zircon (1065 Ma) standard over more than a year, was 3.8%, 4.0% and 1.4% for the <SUP>206</SUP>Pb/ <SUP>238</SUP>U, <SUP>207</SUP>Pb/ <SUP>235</SUP>U and <SUP>207</SUP>Pb/ <SUP>206</SUP>Pb ratios, respectively. Long-term precision (2 r.s.d.) for the <SUP>206</SUP>Pb/ <SUP>238</SUP>U, <SUP>207</SUP>Pb/ <SUP>235</SUP>U and <SUP>207</SUP>Pb/ <SUP>206</SUP>Pb ratios of the Mud Tank zircon (732 Ma) was 3.9%, 4.1% and 1.7%, respectively (359 analyses). Selective integration of time-resolved signals was used to minimise the effect of Pb loss and common Pb enrichments on the measured ages. The precision and accuracy of our data compare very favourably with those obtained using more involved procedures to correct or minimise ablation- and ICP-MS-induced biases. <P />213 nm laser ablation produced comparable precision to 266 nm ablation using generally smaller spot sizes (40-50 vs. 60-80 μm), and offered significant advantages in terms of ablation duration and stability, particularly for small zircons (&lt;60 μm). For the 91500 zircon, but not the Mud Tank zircon, 213 nm ablation also produced significantly older and more accurate Pb/U ages. This suggests that shorter wavelength ablation may have reduced a matrix-dependent elemental fractionation difference between sample and standard. <P />The accuracy and precision of the technique for young zircons are demonstrated by analysis of three zircon populations ranging in age from 417 to 7 Ma. In each case, the zircons have yielded concordant ages or common Pb discordia which give concordia intercept ages that are in agreement with independently determined ages for the same samples. Application of Tera-Wasserburg diagrams [Earth Planet. Sci. Lett. 14 (1972) 281] was found to be the most useful approach to handling common Pb contributions that were not removed by selective integration of signals.", "database": ["physics", "earth science"], "keywords": ["Laser ablation-ICP-MS", "U-Pb geochronology", "Zircon", "Calibration"], "year": "2004", "doctype": "article", "citation_count": 4099, "domain_category": "multidisciplinary", "abstract_clean": "This paper reports new developments in in situ U-Pb zircon geochronology using 266 and 213 nm laser ablation-inductively coupled plasma-mass spectrometry (LA-ICP-MS). Standard spot ablation (spot diameters 40-80 μm) was employed, with no sampling strategies employed specifically to minimise elemental fractionation. Instead, He ablation gas and carefully replicated ablation conditions were employed to maintain constant ablation-related elemental fractionation of Pb and U between analyses. Combining these strategies with calibration on a new zircon standard (GJ-1) allows elemental fractionation and instrumental mass bias to be corrected efficiently, and accurate 206Pb/ 238U and 207Pb/ 235U ratios to be measured with short-term precision (2 r.s.d.) of 1.9% and 3.0%, respectively. Long-term precision (2 r.s.d.) of the technique (266 nm ablation), based on 355 analyses of the 91500 zircon (1065 Ma) standard over more than a year, was 3.8%, 4.0% and 1.4% for the 206Pb/ 238U, 207Pb/ 235U and 207Pb/ 206Pb ratios, respectively. Long-term precision (2 r.s.d.) for the 206Pb/ 238U, 207Pb/ 235U and 207Pb/ 206Pb ratios of the Mud Tank zircon (732 Ma) was 3.9%, 4.1% and 1.7%, respectively (359 analyses). Selective integration of time-resolved signals was used to minimise the effect of Pb loss and common Pb enrichments on the measured ages. The precision and accuracy of our data compare very favourably with those obtained using more involved procedures to correct or minimise ablation- and ICP-MS-induced biases. 213 nm laser ablation produced comparable precision to 266 nm ablation using generally smaller spot sizes (40-50 vs. 60-80 μm), and offered significant advantages in terms of ablation duration and stability, particularly for small zircons (<60 μm). For the 91500 zircon, but not the Mud Tank zircon, 213 nm ablation also produced significantly older and more accurate Pb/U ages. This suggests that shorter wavelength ablation may have reduced a matrix-dependent elemental fractionation difference between sample and standard. The accuracy and precision of the technique for young zircons are demonstrated by analysis of three zircon populations ranging in age from 417 to 7 Ma. In each case, the zircons have yielded concordant ages or common Pb discordia which give concordia intercept ages that are in agreement with independently determined ages for the same samples. Application of Tera-Wasserburg diagrams [Earth Planet. Sci. Lett. 14 (1972) 281] was found to be the most useful approach to handling common Pb contributions that were not removed by selective integration of signals."}
{"bibcode": "1991Sci...254.1178H", "title": "Optical Coherence Tomography", "abstract": "A technique called optical coherence tomography (OCT) has been developed for noninvasive cross-sectional imaging in biological systems. OCT uses low-coherence interferometry to produce a two-dimensional image of optical scattering from internal tissue microstructures in a way that is analogous to ultrasonic pulse-echo imaging. OCT has longitudinal and lateral spatial resolutions of a few micrometers and can detect reflected signals as small as ~10<SUP>-10</SUP> of the incident optical power. Tomographic imaging is demonstrated in vitro in the peripapillary area of the retina and in the coronary artery, two clinically relevant examples that are representative of transparent and turbid media, respectively.", "database": ["physics", "general"], "keywords": [], "year": "1991", "doctype": "article", "citation_count": 4026, "domain_category": "multidisciplinary", "abstract_clean": "A technique called optical coherence tomography (OCT) has been developed for noninvasive cross-sectional imaging in biological systems. OCT uses low-coherence interferometry to produce a two-dimensional image of optical scattering from internal tissue microstructures in a way that is analogous to ultrasonic pulse-echo imaging. OCT has longitudinal and lateral spatial resolutions of a few micrometers and can detect reflected signals as small as ~10-10 of the incident optical power. Tomographic imaging is demonstrated in vitro in the peripapillary area of the retina and in the coronary artery, two clinically relevant examples that are representative of transparent and turbid media, respectively."}
{"bibcode": "2007SSCom.143...47F", "title": "Raman spectroscopy of graphene and graphite: Disorder, electron phonon coupling, doping and nonadiabatic effects", "abstract": "We review recent work on Raman spectroscopy of graphite and graphene. We focus on the origin of the D and G peaks and the second order of the D peak. The G and 2D Raman peaks change in shape, position and relative intensity with number of graphene layers. This reflects the evolution of the electronic structure and electron-phonon interactions. We then consider the effects of doping on the Raman spectra of graphene. The Fermi energy is tuned by applying a gate-voltage. We show that this induces a stiffening of the Raman G peak for both holes and electrons doping. Thus Raman spectroscopy can be efficiently used to monitor number of layers, quality of layers, doping level and confinement.", "database": ["physics"], "keywords": [], "year": "2007", "doctype": "article", "citation_count": 4022, "domain_category": "multidisciplinary", "abstract_clean": "We review recent work on Raman spectroscopy of graphite and graphene. We focus on the origin of the D and G peaks and the second order of the D peak. The G and 2D Raman peaks change in shape, position and relative intensity with number of graphene layers. This reflects the evolution of the electronic structure and electron-phonon interactions. We then consider the effects of doping on the Raman spectra of graphene. The Fermi energy is tuned by applying a gate-voltage. We show that this induces a stiffening of the Raman G peak for both holes and electrons doping. Thus Raman spectroscopy can be efficiently used to monitor number of layers, quality of layers, doping level and confinement."}
{"bibcode": "2010AcCrD..66...12C", "title": "MolProbity: all-atom structure validation for macromolecular crystallography", "abstract": "MolProbityis a structure-validation web service that provides broad-spectrum solidly based evaluation of model quality at both the global and local levels for both proteins and nucleic acids. It relies heavily on the power and sensitivity provided by optimized hydrogen placement and all-atom contact analysis, complemented by updated versions of covalent-geometry and torsion-angle criteria. Some of the local corrections can be performed automatically inMolProbityand all of the diagnostics are presented in chart and graphical forms that help guide manual rebuilding. X-ray crystallography provides a wealth of biologically important molecular data in the form of atomic three-dimensional structures of proteins, nucleic acids and increasingly large complexes in multiple forms and states. Advances in automation, in everything from crystallization to data collection to phasing to model building to refinement, have made solving a structure using crystallography easier than ever. However, despite these improvements, local errors that can affect biological interpretation are widespread at low resolution and even high-resolution structures nearly all contain at least a few local errors such as Ramachandran outliers, flipped branched protein side chains and incorrect sugar puckers. It is critical both for the crystallographer and for the end user that there are easy and reliable methods to diagnose and correct these sorts of errors in structures.MolProbityis the authors' contribution to helping solve this problem and this article reviews its general capabilities, reports on recent enhancements and usage, and presents evidence that the resulting improvements are now beneficially affecting the global database.", "database": ["earth science"], "keywords": [], "year": "2010", "doctype": "article", "citation_count": 3970, "domain_category": "multidisciplinary", "abstract_clean": "MolProbityis a structure-validation web service that provides broad-spectrum solidly based evaluation of model quality at both the global and local levels for both proteins and nucleic acids. It relies heavily on the power and sensitivity provided by optimized hydrogen placement and all-atom contact analysis, complemented by updated versions of covalent-geometry and torsion-angle criteria. Some of the local corrections can be performed automatically inMolProbityand all of the diagnostics are presented in chart and graphical forms that help guide manual rebuilding. X-ray crystallography provides a wealth of biologically important molecular data in the form of atomic three-dimensional structures of proteins, nucleic acids and increasingly large complexes in multiple forms and states. Advances in automation, in everything from crystallization to data collection to phasing to model building to refinement, have made solving a structure using crystallography easier than ever. However, despite these improvements, local errors that can affect biological interpretation are widespread at low resolution and even high-resolution structures nearly all contain at least a few local errors such as Ramachandran outliers, flipped branched protein side chains and incorrect sugar puckers. It is critical both for the crystallographer and for the end user that there are easy and reliable methods to diagnose and correct these sorts of errors in structures.MolProbityis the authors' contribution to helping solve this problem and this article reviews its general capabilities, reports on recent enhancements and usage, and presents evidence that the resulting improvements are now beneficially affecting the global database."}
{"bibcode": "2003ApJ...591.1220L", "title": "Solar System Abundances and Condensation Temperatures of the Elements", "abstract": "Solar photospheric and meteoritic CI chondrite abundance determinations for all elements are summarized and the best currently available photospheric abundances are selected. The meteoritic and solar abundances of a few elements (e.g., noble gases, beryllium, boron, phosphorous, sulfur) are discussed in detail. The photospheric abundances give mass fractions of hydrogen (X=0.7491), helium (Y=0.2377), and heavy elements (Z=0.0133), leading to Z/X=0.0177, which is lower than the widely used Z/X=0.0245 from previous compilations. Recent results from standard solar models considering helium and heavy-element settling imply that photospheric abundances and mass fractions are not equal to protosolar abundances (representative of solar system abundances). Protosolar elemental and isotopic abundances are derived from photospheric abundances by considering settling effects. Derived protosolar mass fractions are X<SUB>0</SUB>=0.7110, Y<SUB>0</SUB>=0.2741, and Z<SUB>0</SUB>=0.0149. The solar system and photospheric abundance tables are used to compute self-consistent sets of condensation temperatures for all elements.", "database": ["astronomy"], "keywords": ["Astrochemistry", "Meteors", "Meteoroids", "Solar System: Formation- Sun: Abundances", "Sun: Photosphere"], "year": "2003", "doctype": "article", "citation_count": 3968, "domain_category": "multidisciplinary", "abstract_clean": "Solar photospheric and meteoritic CI chondrite abundance determinations for all elements are summarized and the best currently available photospheric abundances are selected. The meteoritic and solar abundances of a few elements (e.g., noble gases, beryllium, boron, phosphorous, sulfur) are discussed in detail. The photospheric abundances give mass fractions of hydrogen (X=0.7491), helium (Y=0.2377), and heavy elements (Z=0.0133), leading to Z/X=0.0177, which is lower than the widely used Z/X=0.0245 from previous compilations. Recent results from standard solar models considering helium and heavy-element settling imply that photospheric abundances and mass fractions are not equal to protosolar abundances (representative of solar system abundances). Protosolar elemental and isotopic abundances are derived from photospheric abundances by considering settling effects. Derived protosolar mass fractions are X0=0.7110, Y0=0.2741, and Z0=0.0149. The solar system and photospheric abundance tables are used to compute self-consistent sets of condensation temperatures for all elements."}
{"bibcode": "2013Sci...342..344X", "title": "Long-Range Balanced Electron- and Hole-Transport Lengths in Organic-Inorganic CH<SUB>3</SUB>NH<SUB>3</SUB>PbI<SUB>3</SUB>", "abstract": "Low-temperature solution-processed photovoltaics suffer from low efficiencies because of poor exciton or electron-hole diffusion lengths (typically about 10 nanometers). Recent reports of highly efficient CH<SUB>3</SUB>NH<SUB>3</SUB>PbI<SUB>3</SUB>-based solar cells in a broad range of configurations raise a compelling case for understanding the fundamental photophysical mechanisms in these materials. By applying femtosecond transient optical spectroscopy to bilayers that interface this perovskite with either selective-electron or selective-hole extraction materials, we have uncovered concrete evidence of balanced long-range electron-hole diffusion lengths of at least 100 nanometers in solution-processed CH<SUB>3</SUB>NH<SUB>3</SUB>PbI<SUB>3</SUB>. The high photoconversion efficiencies of these systems stem from the comparable optical absorption length and charge-carrier diffusion lengths, transcending the traditional constraints of solution-processed semiconductors.", "database": ["general"], "keywords": ["APP PHYSICS Chemistry, Materials-Science, Sociology"], "year": "2013", "doctype": "article", "citation_count": 3965, "domain_category": "multidisciplinary", "abstract_clean": "Low-temperature solution-processed photovoltaics suffer from low efficiencies because of poor exciton or electron-hole diffusion lengths (typically about 10 nanometers). Recent reports of highly efficient CH3NH3PbI3-based solar cells in a broad range of configurations raise a compelling case for understanding the fundamental photophysical mechanisms in these materials. By applying femtosecond transient optical spectroscopy to bilayers that interface this perovskite with either selective-electron or selective-hole extraction materials, we have uncovered concrete evidence of balanced long-range electron-hole diffusion lengths of at least 100 nanometers in solution-processed CH3NH3PbI3. The high photoconversion efficiencies of these systems stem from the comparable optical absorption length and charge-carrier diffusion lengths, transcending the traditional constraints of solution-processed semiconductors."}
{"bibcode": "1980CJFAS..37..130V", "title": "The River Continuum Concept", "abstract": "From headwaters to mouth, the physical variables within a river system present a continuous gradient of physical conditions. This gradient should elicit a series of responses within the constituent populations resulting in a continuum of biotic adjustments and consistent patterns of loading, transport, utilization, and storage of organic matter along the length of a river. Based on the energy equilibrium theory of fluvial geomorphologists, we hypothesize that the structural and functional characteristics of stream communities are adapted to conform to the most probable position or mean state of the physical system. We reason that producer and consumer communities characteristic of a given river reach become established in harmony with the dynamic physical conditions of the channel. In natural stream systems, biological communities can be characterized as forming a temporal continuum of synchronized species replacements. This continuous replacement functions to distribute the utilization of energy inputs over time. Thus, the biological system moves towards a balance between a tendency for efficient use of energy inputs through resource partitioning (food, substrate, etc.) and an opposing tendency for a uniform rate of energy processing throughout the year. We theorize that biological communities developed in natural streams assume processing strategies involving minimum energy loss. Downstream communities are fashioned to capitalize on upstream processing inefficiencies. Both the upstream inefficiency (leakage) and the downstream adjustments seem predictable. We propose that this River Continuum Concept provides a framework for integrating predictable and observable biological features of lotic systems. Implications of the concept in the areas of structure, function, and stability of riverine ecosystems are discussed.Key words: river continuum; stream ecosystems; ecosystem structure, function; resource partitioning; ecosystem stability; community succession; river zonation; stream geomorphology", "database": ["earth science"], "keywords": [], "year": "1980", "doctype": "article", "citation_count": 3964, "domain_category": "multidisciplinary", "abstract_clean": "From headwaters to mouth, the physical variables within a river system present a continuous gradient of physical conditions. This gradient should elicit a series of responses within the constituent populations resulting in a continuum of biotic adjustments and consistent patterns of loading, transport, utilization, and storage of organic matter along the length of a river. Based on the energy equilibrium theory of fluvial geomorphologists, we hypothesize that the structural and functional characteristics of stream communities are adapted to conform to the most probable position or mean state of the physical system. We reason that producer and consumer communities characteristic of a given river reach become established in harmony with the dynamic physical conditions of the channel. In natural stream systems, biological communities can be characterized as forming a temporal continuum of synchronized species replacements. This continuous replacement functions to distribute the utilization of energy inputs over time. Thus, the biological system moves towards a balance between a tendency for efficient use of energy inputs through resource partitioning (food, substrate, etc.) and an opposing tendency for a uniform rate of energy processing throughout the year. We theorize that biological communities developed in natural streams assume processing strategies involving minimum energy loss. Downstream communities are fashioned to capitalize on upstream processing inefficiencies. Both the upstream inefficiency (leakage) and the downstream adjustments seem predictable. We propose that this River Continuum Concept provides a framework for integrating predictable and observable biological features of lotic systems. Implications of the concept in the areas of structure, function, and stability of riverine ecosystems are discussed.Key words: river continuum; stream ecosystems; ecosystem structure, function; resource partitioning; ecosystem stability; community succession; river zonation; stream geomorphology"}
{"bibcode": "2013NatNa...8..235F", "title": "Raman spectroscopy as a versatile tool for studying the properties of graphene", "abstract": "Advances in the understanding of Raman processes in graphene have made it an essential tool for studying the properties of this one-atom-thick carbon material.", "database": ["physics"], "keywords": ["Condensed Matter - Materials Science"], "year": "2013", "doctype": "article", "citation_count": 3948, "domain_category": "multidisciplinary", "abstract_clean": "Advances in the understanding of Raman processes in graphene have made it an essential tool for studying the properties of this one-atom-thick carbon material."}
{"bibcode": "1952RSPTB.237...37T", "title": "The Chemical Basis of Morphogenesis", "abstract": "It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.", "database": ["physics", "general"], "keywords": [], "year": "1952", "doctype": "article", "citation_count": 3942, "domain_category": "multidisciplinary", "abstract_clean": "It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading."}
{"bibcode": "2004ApJ...611.1005G", "title": "The Swift Gamma-Ray Burst Mission", "abstract": "The Swift mission, scheduled for launch in 2004, is a multiwavelength observatory for gamma-ray burst (GRB) astronomy. It is a first-of-its-kind autonomous rapid-slewing satellite for transient astronomy and pioneers the way for future rapid-reaction and multiwavelength missions. It will be far more powerful than any previous GRB mission, observing more than 100 bursts yr<SUP>-1</SUP> and performing detailed X-ray and UV/optical afterglow observations spanning timescales from 1 minute to several days after the burst. The objectives are to (1) determine the origin of GRBs, (2) classify GRBs and search for new types, (3) study the interaction of the ultrarelativistic outflows of GRBs with their surrounding medium, and (4) use GRBs to study the early universe out to z&gt;10. The mission is being developed by a NASA-led international collaboration. It will carry three instruments: a new-generation wide-field gamma-ray (15-150 keV) detector that will detect bursts, calculate 1'-4' positions, and trigger autonomous spacecraft slews; a narrow-field X-ray telescope that will give 5\" positions and perform spectroscopy in the 0.2-10 keV band; and a narrow-field UV/optical telescope that will operate in the 170-600 nm band and provide 0.3\" positions and optical finding charts. Redshift determinations will be made for most bursts. In addition to the primary GRB science, the mission will perform a hard X-ray survey to a sensitivity of ~1 mcrab (~2×10<SUP>-11</SUP> ergs cm<SUP>-2</SUP> s<SUP>-1</SUP> in the 15-150 keV band), more than an order of magnitude better than HEAO 1 A-4. A flexible data and operations system will allow rapid follow-up observations of all types of high-energy transients, with rapid data downlink and uplink available through the NASA TDRSS system. Swift transient data will be rapidly distributed to the astronomical community, and all interested observers are encouraged to participate in follow-up measurements. A Guest Investigator program for the mission will provide funding for community involvement. Innovations from the Swift program applicable to the future include (1) a large-area gamma-ray detector using the new CdZnTe detectors, (2) an autonomous rapid-slewing spacecraft, (3) a multiwavelength payload combining optical, X-ray, and gamma-ray instruments, (4) an observing program coordinated with other ground-based and space-based observatories, and (5) immediate multiwavelength data flow to the community. The mission is currently funded for 2 yr of operations, and the spacecraft will have a lifetime to orbital decay of ~8 yr.", "database": ["astronomy"], "keywords": ["Gamma Rays: Bursts", "Space Vehicles: Instruments", "Telescopes", "Astrophysics"], "year": "2004", "doctype": "article", "citation_count": 3880, "domain_category": "multidisciplinary", "abstract_clean": "The Swift mission, scheduled for launch in 2004, is a multiwavelength observatory for gamma-ray burst (GRB) astronomy. It is a first-of-its-kind autonomous rapid-slewing satellite for transient astronomy and pioneers the way for future rapid-reaction and multiwavelength missions. It will be far more powerful than any previous GRB mission, observing more than 100 bursts yr-1 and performing detailed X-ray and UV/optical afterglow observations spanning timescales from 1 minute to several days after the burst. The objectives are to (1) determine the origin of GRBs, (2) classify GRBs and search for new types, (3) study the interaction of the ultrarelativistic outflows of GRBs with their surrounding medium, and (4) use GRBs to study the early universe out to z>10. The mission is being developed by a NASA-led international collaboration. It will carry three instruments: a new-generation wide-field gamma-ray (15-150 keV) detector that will detect bursts, calculate 1'-4' positions, and trigger autonomous spacecraft slews; a narrow-field X-ray telescope that will give 5\" positions and perform spectroscopy in the 0.2-10 keV band; and a narrow-field UV/optical telescope that will operate in the 170-600 nm band and provide 0.3\" positions and optical finding charts. Redshift determinations will be made for most bursts. In addition to the primary GRB science, the mission will perform a hard X-ray survey to a sensitivity of ~1 mcrab (~2×10-11 ergs cm-2 s-1 in the 15-150 keV band), more than an order of magnitude better than HEAO 1 A-4. A flexible data and operations system will allow rapid follow-up observations of all types of high-energy transients, with rapid data downlink and uplink available through the NASA TDRSS system. Swift transient data will be rapidly distributed to the astronomical community, and all interested observers are encouraged to participate in follow-up measurements. A Guest Investigator program for the mission will provide funding for community involvement. Innovations from the Swift program applicable to the future include (1) a large-area gamma-ray detector using the new CdZnTe detectors, (2) an autonomous rapid-slewing spacecraft, (3) a multiwavelength payload combining optical, X-ray, and gamma-ray instruments, (4) an observing program coordinated with other ground-based and space-based observatories, and (5) immediate multiwavelength data flow to the community. The mission is currently funded for 2 yr of operations, and the spacecraft will have a lifetime to orbital decay of ~8 yr."}
{"bibcode": "2002PNAS...99.7821G", "title": "Community structure in social and biological networks", "abstract": "A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.", "database": ["physics", "general"], "keywords": ["Applied Mathematics", "Condensed Matter - Statistical Mechanics", "Condensed Matter - Disordered Systems and Neural Networks"], "year": "2002", "doctype": "article", "citation_count": 3817, "domain_category": "multidisciplinary", "abstract_clean": "A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases."}
{"bibcode": "1984ApJ...285...89D", "title": "Optical Properties of Interstellar Graphite and Silicate Grains", "abstract": "The dielectric functions for graphite and astronomical silicate material are computed using available laboratory and astronomical data. It is noted that the magnetic dipole contribution to absorption in the infrared wavelengths can be important for conducting particles such as graphite. Formulas are given for evaluating electric and magnetic dipole cross-sections for small particles. Absorption cross-sections are evaluated for graphite and silicate particles with sizes between 0.003 and 1.0 microns, and wavelengths from 300 A to 1000 microns. On the basis of polarization profiles computed for both prolate and oblate graphite-silicate spheroids, it is concluded that interstellar silicate grain are predominantly oblate. Extinction curves are calculated for Mathis-Rumpl-Nordsieck graphite-silicate grain mixtures and are compared to observations. The model is found to be in good agreement with available infrared observations.", "database": ["astronomy"], "keywords": ["Granular Materials", "Graphite", "Interstellar Matter", "Optical Properties", "Silicates", "Absorption Cross Sections", "Infrared Astronomy", "Opacity", "Particle Interactions", "Scattering Cross Sections", "Astrophysics"], "year": "1984", "doctype": "article", "citation_count": 3810, "domain_category": "multidisciplinary", "abstract_clean": "The dielectric functions for graphite and astronomical silicate material are computed using available laboratory and astronomical data. It is noted that the magnetic dipole contribution to absorption in the infrared wavelengths can be important for conducting particles such as graphite. Formulas are given for evaluating electric and magnetic dipole cross-sections for small particles. Absorption cross-sections are evaluated for graphite and silicate particles with sizes between 0.003 and 1.0 microns, and wavelengths from 300 A to 1000 microns. On the basis of polarization profiles computed for both prolate and oblate graphite-silicate spheroids, it is concluded that interstellar silicate grain are predominantly oblate. Extinction curves are calculated for Mathis-Rumpl-Nordsieck graphite-silicate grain mixtures and are compared to observations. The model is found to be in good agreement with available infrared observations."}
{"bibcode": "2004Ecol...85.1771B", "title": "Toward a Metabolic Theory of Ecology", "abstract": "Metabolism provides a basis for using first principles of physics, chemistry, and biology to link the biology of individual organisms to the ecology of populations, communities, and ecosystems. Metabolic rate, the rate at which organisms take up, transform, and expend energy and materials, is the most fundamental biological rate. We have developed a quantitative theory for how metabolic rate varies with body size and temperature. Metabolic theory predicts how metabolic rate, by setting the rates of resource uptake from the environment and resource allocation to survival, growth, and reproduction, controls ecological processes at all levels of organization from individuals to the biosphere. Examples include: (1) life history attributes, including development rate, mortality rate, age at maturity, life span, and population growth rate; (2) population interactions, including carrying capacity, rates of competition and predation, and patterns of species diversity; and (3) ecosystem processes, including rates of biomass production and respiration and patterns of trophic dynamics. Data compiled from the ecological literature strongly support the theoretical predictions. Eventually, metabolic theory may provide a conceptual foundation for much of ecology, just as genetic theory provides a foundation for much of evolutionary biology.", "database": ["earth science"], "keywords": ["allometry", "biogeochemical cycles", "body size", "development", "ecological interactions", "ecological theory", "metabolism", "population growth", "production", "stoichiometry", "temperature", "trophic dynamics"], "year": "2004", "doctype": "article", "citation_count": 3801, "domain_category": "multidisciplinary", "abstract_clean": "Metabolism provides a basis for using first principles of physics, chemistry, and biology to link the biology of individual organisms to the ecology of populations, communities, and ecosystems. Metabolic rate, the rate at which organisms take up, transform, and expend energy and materials, is the most fundamental biological rate. We have developed a quantitative theory for how metabolic rate varies with body size and temperature. Metabolic theory predicts how metabolic rate, by setting the rates of resource uptake from the environment and resource allocation to survival, growth, and reproduction, controls ecological processes at all levels of organization from individuals to the biosphere. Examples include: (1) life history attributes, including development rate, mortality rate, age at maturity, life span, and population growth rate; (2) population interactions, including carrying capacity, rates of competition and predation, and patterns of species diversity; and (3) ecosystem processes, including rates of biomass production and respiration and patterns of trophic dynamics. Data compiled from the ecological literature strongly support the theoretical predictions. Eventually, metabolic theory may provide a conceptual foundation for much of ecology, just as genetic theory provides a foundation for much of evolutionary biology."}
{"bibcode": "2012MEcEv...3..217R", "title": "phytools: an R package for phylogenetic comparative biology (and other things)", "abstract": "Summary 1. Here, I present a new, multifunctional phylogenetics package, phytools, for the R statistical computing environment. 2. The focus of the package is on methods for phylogenetic comparative biology; however, it also includes tools for tree inference, phylogeny input/output, plotting, manipulation and several other tasks. 3. I describe and tabulate the major methods implemented in phytools, and in addition provide some demonstration of its use in the form of two illustrative examples. 4. Finally, I conclude by briefly describing an active web-log that I use to document present and future developments for phytools. I also note other web resources for phylogenetics in the R computational environment.", "database": ["earth science"], "keywords": ["blogging", "computational biology", "evolution", "phylogeny", "statistics"], "year": "2012", "doctype": "article", "citation_count": 3765, "domain_category": "multidisciplinary", "abstract_clean": "Summary 1. Here, I present a new, multifunctional phylogenetics package, phytools, for the R statistical computing environment. 2. The focus of the package is on methods for phylogenetic comparative biology; however, it also includes tools for tree inference, phylogeny input/output, plotting, manipulation and several other tasks. 3. I describe and tabulate the major methods implemented in phytools, and in addition provide some demonstration of its use in the form of two illustrative examples. 4. Finally, I conclude by briefly describing an active web-log that I use to document present and future developments for phytools. I also note other web resources for phylogenetics in the R computational environment."}
{"bibcode": "2003SurSR..48...53D", "title": "The surface science of titanium dioxide", "abstract": "Titanium dioxide is the most investigated single-crystalline system in the surface science of metal oxides, and the literature on rutile (1 1 0), (1 0 0), (0 0 1), and anatase surfaces is reviewed. This paper starts with a summary of the wide variety of technical fields where TiO <SUB>2</SUB> is of importance. The bulk structure and bulk defects (as far as relevant to the surface properties) are briefly reviewed. Rules to predict stable oxide surfaces are exemplified on rutile (1 1 0). The surface structure of rutile (1 1 0) is discussed in some detail. Theoretically predicted and experimentally determined relaxations of surface geometries are compared, and defects (step edge orientations, point and line defects, impurities, surface manifestations of crystallographic shear planes—CSPs) are discussed, as well as the image contrast in scanning tunneling microscopy (STM). The controversy about the correct model for the (1×2) reconstruction appears to be settled. Different surface preparation methods, such as reoxidation of reduced crystals, can cause a drastic effect on surface geometries and morphology, and recommendations for preparing different TiO <SUB>2</SUB>(1 1 0) surfaces are given. The structure of the TiO <SUB>2</SUB>(1 0 0)-(1×1) surface is discussed and the proposed models for the (1×3) reconstruction are critically reviewed. Very recent results on anatase (1 0 0) and (1 0 1) surfaces are included. The electronic structure of stoichiometric TiO <SUB>2</SUB> surfaces is now well understood. Surface defects can be detected with a variety of surface spectroscopies. The vibrational structure is dominated by strong Fuchs-Kliewer phonons, and high-resolution electron energy loss spectra often need to be deconvoluted in order to render useful information about adsorbed molecules. The growth of metals (Li, Na, K, Cs, Ca, Al, Ti, V, Nb, Cr, Mo, Mn, Fe, Co, Rh, Ir, Ni, Pd, Pt, Cu, Ag, Au) as well as some metal oxides on TiO <SUB>2</SUB> is reviewed. The tendency to 'wet' the overlayer, the growth morphology, the epitaxial relationship, and the strength of the interfacial oxidation/reduction reaction all follow clear trends across the periodic table, with the reactivity of the overlayer metal towards oxygen being the most decisive factor. Alkali atoms form ordered superstructures at low coverages. Recent progress in understanding the surface structure of metals in the 'strong-metal support interaction' (SMSI) state is summarized. Literature is reviewed on the adsorption and reaction of a wide variety of inorganic molecules (H <SUB>2</SUB>, O <SUB>2</SUB>, H <SUB>2</SUB>O, CO, CO <SUB>2</SUB>, N <SUB>2</SUB>, NH <SUB>3</SUB>, NO <SUB>x</SUB>, sulfur- and halogen-containing molecules, rare gases) as well as organic molecules (carboxylic acids, alcohols, aldehydes and ketones, alkynes, pyridine and its derivates, silanes, methyl halides). The application of TiO <SUB>2</SUB>-based systems in photo-active devices is discussed, and the results on UHV-based photocatalytic studies are summarized. The review ends with a brief conclusion and outlook of TiO <SUB>2</SUB>-based surface science for the future.", "database": ["physics"], "keywords": [], "year": "2003", "doctype": "article", "citation_count": 3659, "domain_category": "multidisciplinary", "abstract_clean": "Titanium dioxide is the most investigated single-crystalline system in the surface science of metal oxides, and the literature on rutile (1 1 0), (1 0 0), (0 0 1), and anatase surfaces is reviewed. This paper starts with a summary of the wide variety of technical fields where TiO 2 is of importance. The bulk structure and bulk defects (as far as relevant to the surface properties) are briefly reviewed. Rules to predict stable oxide surfaces are exemplified on rutile (1 1 0). The surface structure of rutile (1 1 0) is discussed in some detail. Theoretically predicted and experimentally determined relaxations of surface geometries are compared, and defects (step edge orientations, point and line defects, impurities, surface manifestations of crystallographic shear planes—CSPs) are discussed, as well as the image contrast in scanning tunneling microscopy (STM). The controversy about the correct model for the (1×2) reconstruction appears to be settled. Different surface preparation methods, such as reoxidation of reduced crystals, can cause a drastic effect on surface geometries and morphology, and recommendations for preparing different TiO 2(1 1 0) surfaces are given. The structure of the TiO 2(1 0 0)-(1×1) surface is discussed and the proposed models for the (1×3) reconstruction are critically reviewed. Very recent results on anatase (1 0 0) and (1 0 1) surfaces are included. The electronic structure of stoichiometric TiO 2 surfaces is now well understood. Surface defects can be detected with a variety of surface spectroscopies. The vibrational structure is dominated by strong Fuchs-Kliewer phonons, and high-resolution electron energy loss spectra often need to be deconvoluted in order to render useful information about adsorbed molecules. The growth of metals (Li, Na, K, Cs, Ca, Al, Ti, V, Nb, Cr, Mo, Mn, Fe, Co, Rh, Ir, Ni, Pd, Pt, Cu, Ag, Au) as well as some metal oxides on TiO 2 is reviewed. The tendency to 'wet' the overlayer, the growth morphology, the epitaxial relationship, and the strength of the interfacial oxidation/reduction reaction all follow clear trends across the periodic table, with the reactivity of the overlayer metal towards oxygen being the most decisive factor. Alkali atoms form ordered superstructures at low coverages. Recent progress in understanding the surface structure of metals in the 'strong-metal support interaction' (SMSI) state is summarized. Literature is reviewed on the adsorption and reaction of a wide variety of inorganic molecules (H 2, O 2, H 2O, CO, CO 2, N 2, NH 3, NO x, sulfur- and halogen-containing molecules, rare gases) as well as organic molecules (carboxylic acids, alcohols, aldehydes and ketones, alkynes, pyridine and its derivates, silanes, methyl halides). The application of TiO 2-based systems in photo-active devices is discussed, and the results on UHV-based photocatalytic studies are summarized. The review ends with a brief conclusion and outlook of TiO 2-based surface science for the future."}
{"bibcode": "1997PhRvL..78.2690J", "title": "Nonequilibrium Equality for Free Energy Differences", "abstract": "An expression is derived for the equilibrium free energy difference between two configurations of a system, in terms of an ensemble of finite-time measurements of the work performed in parametrically switching from one configuration to the other. Two well-known identities emerge as limiting cases of this result.", "database": ["physics"], "keywords": ["Condensed Matter - Statistical Mechanics", "Physics - Biological Physics", "Physics - Chemical Physics", "Physics - Computational Physics"], "year": "1997", "doctype": "article", "citation_count": 3625, "domain_category": "multidisciplinary", "abstract_clean": "An expression is derived for the equilibrium free energy difference between two configurations of a system, in terms of an ensemble of finite-time measurements of the work performed in parametrically switching from one configuration to the other. Two well-known identities emerge as limiting cases of this result."}
{"bibcode": "1985ADNDT..32....1Y", "title": "Atomic Subshell Photoionization Cross Sections and Asymmetry Parameters: 1 &lt;= Z &lt;= 103", "abstract": "Atomic subshell photoionization cross sections and asymmetry parameters are calculated with the Hartree-Fock-Slater one-electron central potential model (dipole approximation) for all elements Z = 1-103. The cross-section results are plotted for all subshells in the energy region 0-1500 eV, and cross sections and asymmetry parameters are tabulated for selected energies in the region 10.2-8047.8 eV. In addition, more detailed graphs are given for the 4 d ( Z = 39-71) and 5 d ( Z = 64-100) subshell cross sections in the vicinity of the Cooper minimum. These data should be particularly useful for work based on spectroscopic investigations of atomic subshells using synchrotron radiation and/or discrete line sources.", "database": ["astronomy", "physics"], "keywords": [], "year": "1985", "doctype": "article", "citation_count": 3561, "domain_category": "multidisciplinary", "abstract_clean": "Atomic subshell photoionization cross sections and asymmetry parameters are calculated with the Hartree-Fock-Slater one-electron central potential model (dipole approximation) for all elements Z = 1-103. The cross-section results are plotted for all subshells in the energy region 0-1500 eV, and cross sections and asymmetry parameters are tabulated for selected energies in the region 10.2-8047.8 eV. In addition, more detailed graphs are given for the 4 d ( Z = 39-71) and 5 d ( Z = 64-100) subshell cross sections in the vicinity of the Cooper minimum. These data should be particularly useful for work based on spectroscopic investigations of atomic subshells using synchrotron radiation and/or discrete line sources."}
{"bibcode": "2008RvMP...80..517A", "title": "Entanglement in many-body systems", "abstract": "Recent interest in aspects common to quantum information and condensed matter has prompted a flurry of activity at the border of these disciplines that were far distant until a few years ago. Numerous interesting questions have been addressed so far. Here an important part of this field, the properties of the entanglement in many-body systems, are reviewed. The zero and finite temperature properties of entanglement in interacting spin, fermion, and boson model systems are discussed. Both bipartite and multipartite entanglement will be considered. In equilibrium entanglement is shown tightly connected to the characteristics of the phase diagram. The behavior of entanglement can be related, via certain witnesses, to thermodynamic quantities thus offering interesting possibilities for an experimental test. Out of equilibrium entangled states are generated and manipulated by means of many-body Hamiltonians.", "database": ["physics"], "keywords": ["05.30.-d", "03.65.Ud", "Quantum statistical mechanics", "Entanglement and quantum nonlocality", "Quantum Physics", "Condensed Matter - Statistical Mechanics", "High Energy Physics - Theory"], "year": "2008", "doctype": "article", "citation_count": 3526, "domain_category": "multidisciplinary", "abstract_clean": "Recent interest in aspects common to quantum information and condensed matter has prompted a flurry of activity at the border of these disciplines that were far distant until a few years ago. Numerous interesting questions have been addressed so far. Here an important part of this field, the properties of the entanglement in many-body systems, are reviewed. The zero and finite temperature properties of entanglement in interacting spin, fermion, and boson model systems are discussed. Both bipartite and multipartite entanglement will be considered. In equilibrium entanglement is shown tightly connected to the characteristics of the phase diagram. The behavior of entanglement can be related, via certain witnesses, to thermodynamic quantities thus offering interesting possibilities for an experimental test. Out of equilibrium entangled states are generated and manipulated by means of many-body Hamiltonians."}
{"bibcode": "2009Sci...325..419O", "title": "A General Framework for Analyzing Sustainability of Social-Ecological Systems", "abstract": "A major problem worldwide is the potential loss of fisheries, forests, and water resources. Understanding of the processes that lead to improvements in or deterioration of natural resources is limited, because scientific disciplines use different concepts and languages to describe and explain complex social-ecological systems (SESs). Without a common framework to organize findings, isolated knowledge does not cumulate. Until recently, accepted theory has assumed that resource users will never self-organize to maintain their resources and that governments must impose solutions. Research in multiple disciplines, however, has found that some government policies accelerate resource destruction, whereas some resource users have invested their time and energy to achieve sustainability. A general framework is used to identify 10 subsystem variables that affect the likelihood of self-organization in efforts to achieve a sustainable SES.", "database": ["general"], "keywords": ["SOCIOLOGY"], "year": "2009", "doctype": "article", "citation_count": 3523, "domain_category": "multidisciplinary", "abstract_clean": "A major problem worldwide is the potential loss of fisheries, forests, and water resources. Understanding of the processes that lead to improvements in or deterioration of natural resources is limited, because scientific disciplines use different concepts and languages to describe and explain complex social-ecological systems (SESs). Without a common framework to organize findings, isolated knowledge does not cumulate. Until recently, accepted theory has assumed that resource users will never self-organize to maintain their resources and that governments must impose solutions. Research in multiple disciplines, however, has found that some government policies accelerate resource destruction, whereas some resource users have invested their time and energy to achieve sustainability. A general framework is used to identify 10 subsystem variables that affect the likelihood of self-organization in efforts to achieve a sustainable SES."}
{"bibcode": "1998SSRv...85..161G", "title": "Standard Solar Composition", "abstract": "We review the current status of our knowledge of the chemical composition of the Sun, essentially derived from the analysis of the solar photospheric spectrum. The comparison of solar and meteoritic abundances confirms that there is a very good agreement between the two sets of abundances. They are used to construct a Standard Abundance Distribution.", "database": ["astronomy"], "keywords": ["Sun: abundances", "Meteorites: abundances", "Solar spectroscopy"], "year": "1998", "doctype": "article", "citation_count": 3519, "domain_category": "multidisciplinary", "abstract_clean": "We review the current status of our knowledge of the chemical composition of the Sun, essentially derived from the analysis of the solar photospheric spectrum. The comparison of solar and meteoritic abundances confirms that there is a very good agreement between the two sets of abundances. They are used to construct a Standard Abundance Distribution."}
{"bibcode": "2009SIAMR..51..661C", "title": "Power-Law Distributions in Empirical Data", "abstract": "Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.", "database": ["physics"], "keywords": ["power-law distributions", "Pareto", "Zipf", "maximum likelihood", "heavy-tailed distributions", "likelihood ratio test", "model selection", "Physics - Data Analysis", "Statistics and Probability", "Condensed Matter - Disordered Systems and Neural Networks", "Statistics - Applications", "Statistics - Methodology"], "year": "2009", "doctype": "article", "citation_count": 3501, "domain_category": "multidisciplinary", "abstract_clean": "Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out."}
{"bibcode": "2006Sci...311..622N", "title": "Toxic Potential of Materials at the Nanolevel", "abstract": "Nanomaterials are engineered structures with at least one dimension of 100 nanometers or less. These materials are increasingly being used for commercial purposes such as fillers, opacifiers, catalysts, semiconductors, cosmetics, microelectronics, and drug carriers. Materials in this size range may approach the length scale at which some specific physical or chemical interactions with their environment can occur. As a result, their properties differ substantially from those bulk materials of the same composition, allowing them to perform exceptional feats of conductivity, reactivity, and optical sensitivity. Possible undesirable results of these capabilities are harmful interactions with biological systems and the environment, with the potential to generate toxicity. The establishment of principles and test procedures to ensure safe manufacture and use of nanomaterials in the marketplace is urgently required and achievable.", "database": ["physics", "general"], "keywords": ["MAT SCI"], "year": "2006", "doctype": "article", "citation_count": 3409, "domain_category": "multidisciplinary", "abstract_clean": "Nanomaterials are engineered structures with at least one dimension of 100 nanometers or less. These materials are increasingly being used for commercial purposes such as fillers, opacifiers, catalysts, semiconductors, cosmetics, microelectronics, and drug carriers. Materials in this size range may approach the length scale at which some specific physical or chemical interactions with their environment can occur. As a result, their properties differ substantially from those bulk materials of the same composition, allowing them to perform exceptional feats of conductivity, reactivity, and optical sensitivity. Possible undesirable results of these capabilities are harmful interactions with biological systems and the environment, with the potential to generate toxicity. The establishment of principles and test procedures to ensure safe manufacture and use of nanomaterials in the marketplace is urgently required and achievable."}
{"bibcode": "2004ApJ...613..898T", "title": "The Origin of the Mass-Metallicity Relation: Insights from 53,000 Star-forming Galaxies in the Sloan Digital Sky Survey", "abstract": "We utilize Sloan Digital Sky Survey imaging and spectroscopy of ~53,000 star-forming galaxies at z~0.1 to study the relation between stellar mass and gas-phase metallicity. We derive gas-phase oxygen abundances and stellar masses using new techniques that make use of the latest stellar evolutionary synthesis and photoionization models. We find a tight (+/-0.1 dex) correlation between stellar mass and metallicity spanning over 3 orders of magnitude in stellar mass and a factor of 10 in metallicity. The relation is relatively steep from 10<SUP>8.5</SUP> to 10<SUP>10.5</SUP> M<SUB>solar</SUB> h<SUP>-2</SUP><SUB>70</SUB>, in good accord with known trends between luminosity and metallicity, but flattens above 10<SUP>10.5</SUP> M<SUB>solar</SUB>. We use indirect estimates of the gas mass based on the Hα luminosity to compare our data to predictions from simple closed box chemical evolution models. We show that metal loss is strongly anticorrelated with baryonic mass, with low-mass dwarf galaxies being 5 times more metal depleted than L<SUP>*</SUP> galaxies at z~0.1. Evidence for metal depletion is not confined to dwarf galaxies but is found in galaxies with masses as high as 10<SUP>10</SUP> M<SUB>solar</SUB>. We interpret this as strong evidence of both the ubiquity of galactic winds and their effectiveness in removing metals from galaxy potential wells.", "database": ["astronomy"], "keywords": ["Galaxies: Abundances", "Galaxies: Evolution", "Galaxies: Fundamental Parameters", "Galaxies: Statistics", "Astrophysics"], "year": "2004", "doctype": "article", "citation_count": 3399, "domain_category": "multidisciplinary", "abstract_clean": "We utilize Sloan Digital Sky Survey imaging and spectroscopy of ~53,000 star-forming galaxies at z~0.1 to study the relation between stellar mass and gas-phase metallicity. We derive gas-phase oxygen abundances and stellar masses using new techniques that make use of the latest stellar evolutionary synthesis and photoionization models. We find a tight (+/-0.1 dex) correlation between stellar mass and metallicity spanning over 3 orders of magnitude in stellar mass and a factor of 10 in metallicity. The relation is relatively steep from 108.5 to 1010.5 Msolar h-270, in good accord with known trends between luminosity and metallicity, but flattens above 1010.5 Msolar. We use indirect estimates of the gas mass based on the Hα luminosity to compare our data to predictions from simple closed box chemical evolution models. We show that metal loss is strongly anticorrelated with baryonic mass, with low-mass dwarf galaxies being 5 times more metal depleted than L* galaxies at z~0.1. Evidence for metal depletion is not confined to dwarf galaxies but is found in galaxies with masses as high as 1010 Msolar. We interpret this as strong evidence of both the ubiquity of galactic winds and their effectiveness in removing metals from galaxy potential wells."}
{"bibcode": "1974CPL....26..163F", "title": "Raman spectra of pyridine adsorbed at a silver electrode", "abstract": "Raman spectroscopy has been employed for the first time to study the role of adsorption at electrodes. It has been possible to distinguish two types of pyridine adsorption at a silver electrode. The variation in intensity and frequency of some of the bands with potential in the region of the point of zero charge has given further evidence as to the structure of the electrical double layer; it is shown that the interaction of adsorbed pyridine and water must be taken into account.", "database": ["physics"], "keywords": [], "year": "1974", "doctype": "article", "citation_count": 3366, "domain_category": "multidisciplinary", "abstract_clean": "Raman spectroscopy has been employed for the first time to study the role of adsorption at electrodes. It has been possible to distinguish two types of pyridine adsorption at a silver electrode. The variation in intensity and frequency of some of the bands with potential in the region of the point of zero charge has given further evidence as to the structure of the electrical double layer; it is shown that the interaction of adsorbed pyridine and water must be taken into account."}
