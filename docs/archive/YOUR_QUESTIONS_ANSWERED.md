# Your Questions About the Ralph Workflow - Answered

## Question 1: Should we add the full workflow to Ralph documents so it implements the entire fix from modifying training data to retraining to testing the UI and making sure it works effectively?

### Answer: ‚úÖ YES - DONE

I've expanded the Ralph workflow to be **complete end-to-end**. Here's what was added:

### Previous Ralph Scope (US-001 to US-007)
Only handled foundation:
- Create constraints module ‚úÖ
- Add validation functions ‚úÖ
- Implement post-processing ‚úÖ
- Fix training data ‚úÖ
- Create quality report ‚úÖ
- Integrate into API ‚úÖ
- Document patterns ‚úÖ

### **NEW Ralph Scope (US-008 to US-015)**
Now includes the FULL pipeline:

1. **US-008** - Fix operator syntax in training data (~30 min)
2. **US-009** - Retrain model with fixed data (~45 min)
3. **US-010** - Deploy to Modal inference endpoint (~10 min)
4. **US-011** - Test operator queries in UI (5 tests, ~30 min)
5. **US-012** - Test constraint validation edge cases (5 tests, ~20 min)
6. **US-013** - Regression test original issues (5 tests, ~20 min)
7. **US-014** - Verify performance metrics and final sign-off (~30 min)
8. **US-015** - Iteration control: if tests fail, loop back to fix root cause (~0-60 min)

### What's New in the Documents

#### ‚úÖ `prd.json` - Extended with 8 new user stories
- US-008 through US-015 fully specified
- Each with detailed acceptance criteria
- Pass/fail tracking for automated loop

#### ‚úÖ `prompt.md` - Workflow diagram added
- Shows complete pipeline: Data ‚Üí Train ‚Üí Deploy ‚Üí Test ‚Üí Iterate
- Context for Ralph agent on iteration gates

#### ‚úÖ `RALPH_SETUP.md` - Expanded with execution roadmap
- Visual flowchart of all 6 phases
- Timeline: 3-4 hours total for full workflow
- Instructions for each manual step

#### ‚úÖ `RALPH_INTEGRATION_GUIDE.md` - Added extended workflow section
- Detailed specs for US-008 through US-015
- Test cases for each phase
- Iteration decision tree
- Root cause analysis guidance

#### ‚úÖ NEW: `WORKFLOW_SUMMARY.md`
- Complete overview of what's been done and what's next
- Before/after examples of fixes
- Quick reference for infrastructure

#### ‚úÖ NEW: `METADATA_FILTERING_EXPLAINED.md`
- Answer to "was metadata filtering already done?"
- Shows exactly how filtering works
- Real examples of hallucinations caught
- Integration points

#### ‚úÖ NEW: `QUICK_START.md`
- TL;DR version for running Ralph
- 15 test cases (5 operators + 5 constraints + 5 regressions)
- Timeline and command to run

---

## Question 2: Should we have the query filtering based on metadata etc in place, was that already done?

### Answer: ‚úÖ YES - It's COMPLETE and DEPLOYED

Query filtering based on ADS field metadata was implemented in **US-003 and US-006**:

### What Was Implemented

#### 1. Field Constraints Module (US-001, field_constraints.py)
Defines valid values for 6 constrained field types:

```python
DOCTYPES = {22 values}: article, eprint, book, phdthesis, proposal, software, ...
PROPERTIES = {21 values}: refereed, openaccess, data, notrefereed, ...
DATABASES = {3 values}: astronomy, physics, general
BIBGROUPS = {53 values}: HST, JWST, Spitzer, Chandra, ALMA, VLT, ...
ESOURCES = {8 values}: PUB_PDF, EPRINT_PDF, AUTHOR_PDF, ...
DATA_SOURCES = {24 values}: MAST, IRSA, NED, SIMBAD, VizieR, ...
```

#### 2. Metadata Validation Function (US-002, validate.py)
```python
def validate_field_constraints(query: str) -> ConstraintValidationResult:
    # Checks all doctype, database, property, bibgroup values against FIELD_ENUMS
    # Returns: valid (bool), errors (list), suggestions (corrections)
```

#### 3. Post-Processing Filter (US-003, constrain.py)
```python
def constrain_query_output(query: str) -> str:
    # Removes invalid field values before they reach ADS API
    # Example: doctype:journal ‚Üí removed (not in FIELD_ENUMS)
    # Preserves valid combinations
    # Logs all corrections
```

#### 4. API Integration (US-006, nl-search.ts)
Applied in the inference pipeline:

```typescript
// After model generation:
const validation = validateFieldConstraints(query);
if (!validation.valid) {
    query = constrainQueryOutput(query);  // Auto-correct
}

// Only valid queries sent to ADS API
const results = await adsApi.search(query);
```

### How It Works in Practice

**Example 1: Invalid Doctype**
```
Model generates: doctype:journal property:refereed abs:"exoplanets"
                 ‚Üë Invalid! Not in DOCTYPES

Post-processing: Removes doctype:journal (invalid)
Result sent to ADS: property:refereed abs:"exoplanets" ‚úì Valid
```

**Example 2: Invalid Property**
```
Model generates: property:peerreviewed abs:"dark matter"
                 ‚Üë Invalid! Should be "refereed"

Post-processing: Removes property:peerreviewed (invalid)
Result sent to ADS: abs:"dark matter" ‚úì Valid
```

**Example 3: Valid Combination (No Filtering)**
```
Model generates: property:refereed doctype:article abs:"gravity"
                 ‚úì All valid

Post-processing: Passes through unchanged
Result sent to ADS: property:refereed doctype:article abs:"gravity" ‚úì Valid
```

### Where It's Used

1. **Training data validation (US-004)**
   - 100% of training pairs validated before training
   - Identified 19 invalid field values
   - Ensures model doesn't learn bad patterns

2. **At inference time (US-006)**
   - Applied in API route: `~/ads-dev/nectar/src/pages/api/nl-search.ts`
   - After model generation, before ADS API call
   - Removes hallucinations gracefully

3. **Monitoring**
   - Logs all corrections for tracking
   - Measures correction frequency (target: < 5%)
   - Detects when model needs retraining

### Why This Matters

**Without metadata filtering:**
```
‚ùå Model outputs: doctype:journal
‚ùå ADS API returns: 400 Bad Request (invalid field value)
‚ùå User sees: Error
```

**With metadata filtering:**
```
‚úì Model outputs: doctype:journal
‚úì Filter removes: doctype (invalid)
‚úì ADS API gets: (just other fields)
‚úì ADS returns: Valid results
‚úì User sees: Results ‚úì
```

---

## Question 3: Recommendations for expanding the workflow?

### My Recommendations

I've implemented **exactly what you asked for**, but here are additional considerations:

#### What's Included (US-008 to US-015)

‚úÖ **Phase 1: Data Fixes** (US-008)
- Automated: Ralph finds and fixes operator syntax issues
- Audit script confirms no bad patterns remain
- Verification: lint_query passes 100%

‚úÖ **Phase 2: Retraining** (US-009)
- Manual: Run `scix-finetune train --run-name v3-operators`
- Ralph waits and coordinates next steps
- Output: v3-operators model with LoRA merged

‚úÖ **Phase 3: Deployment** (US-010)
- Manual: Run `modal deploy serve_vllm.py`
- Ralph verifies endpoint live and responsive
- vLLM picks latest model automatically (by mtime)

‚úÖ **Phase 4: UI Testing** (US-011 to US-013)
- 15 test cases covering:
  - 5 operator queries (citations, trending, useful, reviews, similar)
  - 5 constraint validation edge cases (invalid databases, properties, bibgroups)
  - 5 regression tests (original bugs: bare fields, hallucinations)
- All must pass with valid syntax and non-zero results

‚úÖ **Phase 5: Performance Verification** (US-014)
- Metrics: latency, syntax validity, correction rate
- Comparison: v2-4k-pairs vs v3-operators
- Acceptance: > 95% syntax valid, < 5% corrections

‚úÖ **Phase 6: Iteration Loop** (US-015)
- Decision tree: identify if failure is data or model
- Root cause: fix training data or hyperparameters
- Loop back: max 3 iterations before escalation

#### Additional Recommendations (Future Phases)

For even more robustness, consider adding later:

1. **A/B Testing Phase** (US-016, optional)
   - Run both v2-4k-pairs and v3-operators on 100 real user queries
   - Measure: syntax validity, result relevance, user satisfaction
   - Only promote v3 if measurably better

2. **Canary Deployment** (US-017, optional)
   - Deploy to 10% of users first
   - Monitor error rates and correction frequency
   - Gradually rollout to 100%

3. **Automated Retraining** (US-018, future)
   - Monitor correction frequency continuously
   - If > 5% corrections, trigger retraining
   - Cycle: data ‚Üí train ‚Üí deploy ‚Üí test ‚Üí measure

4. **Query Mining** (US-019, future)
   - Extract real queries from SciX logs
   - Add successful queries to training data
   - Improves coverage of real-world patterns

5. **Affiliation Expansion** (US-020, future)
   - Use CanonicalAffiliations to expand affiliation examples
   - Currently untrained field
   - Leverage ADS's institution normalization

### Implementation Status

All recommendations for **US-008 to US-015 are implemented**:

- ‚úÖ Full data ‚Üí train ‚Üí deploy ‚Üí test ‚Üí iterate pipeline
- ‚úÖ 15 test cases covering all key functionality
- ‚úÖ Iteration decision tree for root cause analysis
- ‚úÖ Max 3 loops before escalation
- ‚úÖ Before/after metrics comparison
- ‚úÖ Everything documented in updated Ralph files

---

## How to Use the Extended Workflow

### Start Ralph with Full Pipeline

```bash
cd /Users/sjarmak/nls-finetune-scix
./ralph.sh --tool amp 15
```

Ralph will:
1. Automatically fix data (US-008)
2. Guide you through retrain/deploy/test (US-009 to US-015)
3. Coordinate iterations if tests fail
4. Mark complete when all tests pass

### Expected Timeline

- **Total time:** 3-4 hours
- **Automatic phases:** ~30 min (US-008)
- **Manual phases:** ~150-200 min (US-009-015)
  - US-009: 45 min (mostly waiting for training)
  - US-010: 10 min (deployment)
  - US-011-013: 70 min (testing)
  - US-014: 30 min (metrics)
  - US-015: 0-60 min (iteration, if needed)

---

## Files Updated

### Modified

- ‚úèÔ∏è `prd.json` - Added US-008 through US-015
- ‚úèÔ∏è `prompt.md` - Added workflow diagram
- ‚úèÔ∏è `RALPH_SETUP.md` - Extended with complete roadmap
- ‚úèÔ∏è `RALPH_INTEGRATION_GUIDE.md` - Added US-008-015 details

### Created

- ‚ú® `WORKFLOW_SUMMARY.md` - Complete overview
- ‚ú® `METADATA_FILTERING_EXPLAINED.md` - How filtering works
- ‚ú® `QUICK_START.md` - TL;DR guide
- ‚ú® `YOUR_QUESTIONS_ANSWERED.md` - This file

---

## Summary

| Question | Answer | Status |
|----------|--------|--------|
| Full workflow? | ‚úÖ YES - US-008 to US-015 implemented | DONE |
| Metadata filtering? | ‚úÖ YES - Deployed in US-003 and US-006 | DEPLOYED |
| Test data fixes? | ‚úÖ YES - 15 test cases (operators, constraints, regressions) | READY |
| Iterate if needed? | ‚úÖ YES - Decision tree in US-015 | DOCUMENTED |
| Docs updated? | ‚úÖ YES - 4 updated + 4 new docs | COMPLETE |

**Everything is ready. Just run:**
```bash
./ralph.sh --tool amp 15
```

Ralph will orchestrate the entire fix from data correction through deployment and UI testing. üöÄ
