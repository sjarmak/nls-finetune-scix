{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciX Enrichment NER Model Training\n",
    "\n",
    "Fine-tune **SciBERT** (`allenai/scibert_scivocab_uncased`) on the SciX enrichment dataset\n",
    "for **token-classification NER** with BIO tags.\n",
    "\n",
    "## Entity types\n",
    "- **topic** (UAT, SWEET, GCMD vocabularies)\n",
    "- **institution** (ROR)\n",
    "- **author**\n",
    "- **date_range**\n",
    "\n",
    "## BIO label set (9 tags)\n",
    "```\n",
    "O, B-topic, I-topic, B-institution, I-institution,\n",
    "B-author, I-author, B-date_range, I-date_range\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "1. **Runtime**: GPU (T4 or A100) via `Runtime > Change runtime type`\n",
    "2. **Data**: Upload `enrichment_train.jsonl` and `enrichment_val.jsonl` via Google Drive or direct upload\n",
    "3. **Training script**: `scripts/train_enrichment_model.py` from the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch accelerate numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available:  {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU:             {gpu_name}\")\n",
    "    print(f\"GPU memory:      {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload / Download Data\n",
    "\n",
    "Choose **one** of the options below:\n",
    "- **Option A**: Upload from Google Drive (recommended for persistence)\n",
    "- **Option B**: Direct file upload from your local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Adjust these paths to match your Drive folder structure\n",
    "DRIVE_DATA_DIR = \"/content/drive/MyDrive/scix-enrichment\"\n",
    "\n",
    "TRAIN_FILE = f\"{DRIVE_DATA_DIR}/enrichment_train.jsonl\"\n",
    "VAL_FILE = f\"{DRIVE_DATA_DIR}/enrichment_val.jsonl\"\n",
    "OUTPUT_DIR = f\"{DRIVE_DATA_DIR}/enrichment_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Direct Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if uploading directly\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Select enrichment_train.jsonl and enrichment_val.jsonl\n",
    "#\n",
    "# TRAIN_FILE = \"enrichment_train.jsonl\"\n",
    "# VAL_FILE = \"enrichment_val.jsonl\"\n",
    "# OUTPUT_DIR = \"output/enrichment_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for label, fpath in [(\"Train\", TRAIN_FILE), (\"Val\", VAL_FILE)]:\n",
    "    p = Path(fpath)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{label} file not found: {fpath}\")\n",
    "    count = sum(1 for line in open(p) if line.strip())\n",
    "    print(f\"{label}: {count} records ({fpath})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Script\n",
    "\n",
    "The cell below contains the full training logic inlined from\n",
    "`scripts/train_enrichment_model.py` so the notebook is self-contained.\n",
    "\n",
    "It converts character-level span annotations to BIO-tagged token sequences,\n",
    "then fine-tunes SciBERT with HuggingFace Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_enrichment_model.py\n",
    "\"\"\"Training script for the SciX enrichment NER model.\n",
    "\n",
    "Fine-tunes SciBERT (allenai/scibert_scivocab_uncased) on enrichment_labels\n",
    "for token-classification NER with BIO tags.\n",
    "\n",
    "BIO label set (9 tags):\n",
    "    B-topic, I-topic, B-institution, I-institution,\n",
    "    B-author, I-author, B-date_range, I-date_range, O\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "ENTITY_TYPES = (\"topic\", \"institution\", \"author\", \"date_range\")\n",
    "\n",
    "BIO_LABELS: list[str] = [\"O\"]\n",
    "for _etype in ENTITY_TYPES:\n",
    "    BIO_LABELS.append(f\"B-{_etype}\")\n",
    "    BIO_LABELS.append(f\"I-{_etype}\")\n",
    "\n",
    "LABEL2ID: dict[str, int] = {label: i for i, label in enumerate(BIO_LABELS)}\n",
    "ID2LABEL: dict[int, str] = {i: label for i, label in enumerate(BIO_LABELS)}\n",
    "NUM_LABELS: int = len(BIO_LABELS)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainConfig:\n",
    "    model_name: str = \"allenai/scibert_scivocab_uncased\"\n",
    "    train_file: str = \"data/enrichment_train.jsonl\"\n",
    "    val_file: str = \"data/enrichment_val.jsonl\"\n",
    "    output_dir: str = \"output/enrichment_model\"\n",
    "    learning_rate: float = 2e-5\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 10\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_seq_length: int = 256\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    early_stopping_patience: int = 3\n",
    "    seed: int = 42\n",
    "    fp16: bool = False\n",
    "    bf16: bool = False\n",
    "    logging_steps: int = 50\n",
    "    eval_steps: int = 200\n",
    "    save_steps: int = 200\n",
    "    save_total_limit: int = 3\n",
    "    log_format: str = \"json\"\n",
    "\n",
    "\n",
    "def load_enrichment_records(path: Path) -> list[dict[str, Any]]:\n",
    "    records: list[dict[str, Any]] = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "\n",
    "def _char_labels(text: str, spans: list[dict[str, Any]]) -> list[str]:\n",
    "    char_tags = [\"O\"] * len(text)\n",
    "    for span in spans:\n",
    "        start = span.get(\"start\", 0)\n",
    "        end = span.get(\"end\", 0)\n",
    "        entity_type = span.get(\"type\", \"\")\n",
    "        if entity_type not in ENTITY_TYPES:\n",
    "            continue\n",
    "        if start >= end or start >= len(text):\n",
    "            continue\n",
    "        end = min(end, len(text))\n",
    "        if char_tags[start] != \"O\":\n",
    "            continue\n",
    "        char_tags[start] = f\"B-{entity_type}\"\n",
    "        for ci in range(start + 1, end):\n",
    "            if char_tags[ci] == \"O\":\n",
    "                char_tags[ci] = f\"I-{entity_type}\"\n",
    "    return char_tags\n",
    "\n",
    "\n",
    "def align_labels_to_tokens(\n",
    "    text: str,\n",
    "    spans: list[dict[str, Any]],\n",
    "    tokenizer: Any,\n",
    "    max_length: int = 256,\n",
    ") -> dict[str, Any]:\n",
    "    char_tags = _char_labels(text, spans)\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    offsets = encoding.pop(\"offset_mapping\")\n",
    "    labels: list[int] = []\n",
    "    for token_idx, (start, end) in enumerate(offsets):\n",
    "        if start == 0 and end == 0:\n",
    "            labels.append(-100)\n",
    "            continue\n",
    "        tag = char_tags[start] if start < len(char_tags) else \"O\"\n",
    "        labels.append(LABEL2ID.get(tag, LABEL2ID[\"O\"]))\n",
    "    encoding[\"labels\"] = labels\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    records: list[dict[str, Any]],\n",
    "    tokenizer: Any,\n",
    "    max_length: int = 256,\n",
    ") -> Any:\n",
    "    from datasets import Dataset\n",
    "    all_input_ids: list[list[int]] = []\n",
    "    all_attention_masks: list[list[int]] = []\n",
    "    all_labels: list[list[int]] = []\n",
    "    for record in records:\n",
    "        text = record.get(\"text\", \"\")\n",
    "        spans = record.get(\"spans\", [])\n",
    "        if not text:\n",
    "            continue\n",
    "        encoded = align_labels_to_tokens(text, spans, tokenizer, max_length)\n",
    "        all_input_ids.append(encoded[\"input_ids\"])\n",
    "        all_attention_masks.append(encoded[\"attention_mask\"])\n",
    "        all_labels.append(encoded[\"labels\"])\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks,\n",
    "        \"labels\": all_labels,\n",
    "    })\n",
    "\n",
    "\n",
    "def build_compute_metrics(id2label: dict[int, str]) -> Any:\n",
    "    import numpy as np\n",
    "    def compute_metrics(eval_pred: Any) -> dict[str, float]:\n",
    "        predictions, label_ids = eval_pred\n",
    "        preds = np.argmax(predictions, axis=-1)\n",
    "        true_labels: list[str] = []\n",
    "        pred_labels: list[str] = []\n",
    "        for seq_true, seq_pred in zip(label_ids, preds):\n",
    "            for t, p in zip(seq_true, seq_pred):\n",
    "                if t == -100:\n",
    "                    continue\n",
    "                true_labels.append(id2label.get(int(t), \"O\"))\n",
    "                pred_labels.append(id2label.get(int(p), \"O\"))\n",
    "        entity_labels = {lbl for lbl in set(true_labels) | set(pred_labels) if lbl != \"O\"}\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        per_label: dict[str, dict[str, int]] = {}\n",
    "        for label in entity_labels:\n",
    "            per_label[label] = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "        for true, pred in zip(true_labels, pred_labels):\n",
    "            if true == pred and true != \"O\":\n",
    "                tp += 1\n",
    "                if true in per_label:\n",
    "                    per_label[true][\"tp\"] += 1\n",
    "            elif pred != \"O\" and true != pred:\n",
    "                fp += 1\n",
    "                if pred in per_label:\n",
    "                    per_label[pred][\"fp\"] += 1\n",
    "                if true != \"O\" and true in per_label:\n",
    "                    per_label[true][\"fn\"] += 1\n",
    "            elif true != \"O\" and pred != true:\n",
    "                fn += 1\n",
    "                if true in per_label:\n",
    "                    per_label[true][\"fn\"] += 1\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        metrics: dict[str, float] = {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1\": round(f1, 4),\n",
    "        }\n",
    "        for label in sorted(entity_labels):\n",
    "            ltp = per_label[label][\"tp\"]\n",
    "            lfp = per_label[label][\"fp\"]\n",
    "            lfn = per_label[label][\"fn\"]\n",
    "            lp = ltp / (ltp + lfp) if (ltp + lfp) > 0 else 0.0\n",
    "            lr = ltp / (ltp + lfn) if (ltp + lfn) > 0 else 0.0\n",
    "            lf1 = 2 * lp * lr / (lp + lr) if (lp + lr) > 0 else 0.0\n",
    "            metrics[f\"{label}_f1\"] = round(lf1, 4)\n",
    "        return metrics\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def save_training_log(\n",
    "    output_dir: Path,\n",
    "    config: TrainConfig,\n",
    "    train_result: Any,\n",
    "    eval_metrics: dict[str, float] | None,\n",
    "    elapsed_seconds: float,\n",
    ") -> None:\n",
    "    log: dict[str, Any] = {\n",
    "        \"model_name\": config.model_name,\n",
    "        \"num_labels\": NUM_LABELS,\n",
    "        \"bio_labels\": BIO_LABELS,\n",
    "        \"label2id\": LABEL2ID,\n",
    "        \"id2label\": {str(k): v for k, v in ID2LABEL.items()},\n",
    "        \"hyperparameters\": {\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"num_epochs\": config.num_epochs,\n",
    "            \"warmup_ratio\": config.warmup_ratio,\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "            \"max_seq_length\": config.max_seq_length,\n",
    "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "            \"seed\": config.seed,\n",
    "            \"fp16\": config.fp16,\n",
    "            \"bf16\": config.bf16,\n",
    "        },\n",
    "        \"training\": {},\n",
    "        \"elapsed_seconds\": round(elapsed_seconds, 2),\n",
    "    }\n",
    "    if train_result is not None:\n",
    "        metrics = getattr(train_result, \"metrics\", {})\n",
    "        log[\"training\"] = {\n",
    "            k: round(v, 6) if isinstance(v, float) else v\n",
    "            for k, v in metrics.items()\n",
    "        }\n",
    "    if eval_metrics is not None:\n",
    "        log[\"eval\"] = {\n",
    "            k: round(v, 6) if isinstance(v, float) else v\n",
    "            for k, v in eval_metrics.items()\n",
    "        }\n",
    "    log_path = output_dir / \"training_log.json\"\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def train(config: TrainConfig) -> int:\n",
    "    start_time = time.time()\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoModelForTokenClassification,\n",
    "        AutoTokenizer,\n",
    "        EarlyStoppingCallback,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "    train_path = Path(config.train_file)\n",
    "    val_path = Path(config.val_file)\n",
    "    if not train_path.exists():\n",
    "        print(f\"Error: training file not found: {train_path}\")\n",
    "        return 1\n",
    "    if not val_path.exists():\n",
    "        print(f\"Error: validation file not found: {val_path}\")\n",
    "        return 1\n",
    "    output_dir = Path(config.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Loading training data from {train_path}...\")\n",
    "    train_records = load_enrichment_records(train_path)\n",
    "    print(f\"  {len(train_records)} training records\")\n",
    "    print(f\"Loading validation data from {val_path}...\")\n",
    "    val_records = load_enrichment_records(val_path)\n",
    "    print(f\"  {len(val_records)} validation records\")\n",
    "    print(f\"Loading tokenizer: {config.model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_name, model_max_length=config.max_seq_length,\n",
    "    )\n",
    "    print(\"Tokenizing and aligning BIO labels (train)...\")\n",
    "    train_dataset = prepare_dataset(train_records, tokenizer, config.max_seq_length)\n",
    "    print(f\"  {len(train_dataset)} training examples\")\n",
    "    print(\"Tokenizing and aligning BIO labels (val)...\")\n",
    "    val_dataset = prepare_dataset(val_records, tokenizer, config.max_seq_length)\n",
    "    print(f\"  {len(val_dataset)} validation examples\")\n",
    "    print(f\"Loading model: {config.model_name}...\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        config.model_name,\n",
    "        num_labels=NUM_LABELS,\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "    )\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  {trainable_params:,} trainable / {total_params:,} total parameters\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size * 2,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        warmup_ratio=config.warmup_ratio,\n",
    "        weight_decay=config.weight_decay,\n",
    "        fp16=config.fp16,\n",
    "        bf16=config.bf16,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=config.logging_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.save_steps,\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        seed=config.seed,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    callbacks = []\n",
    "    if config.early_stopping_patience > 0:\n",
    "        callbacks.append(\n",
    "            EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)\n",
    "        )\n",
    "    compute_metrics_fn = build_compute_metrics(ID2LABEL)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"  Model:          {config.model_name}\")\n",
    "    print(f\"  Labels:         {NUM_LABELS} BIO tags\")\n",
    "    print(f\"  Train examples: {len(train_dataset)}\")\n",
    "    print(f\"  Val examples:   {len(val_dataset)}\")\n",
    "    print(f\"  Epochs:         {config.num_epochs}\")\n",
    "    print(f\"  Batch size:     {config.batch_size}\")\n",
    "    print(f\"  Learning rate:  {config.learning_rate}\")\n",
    "    print(f\"  Early stopping: patience={config.early_stopping_patience}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    train_result = trainer.train()\n",
    "    print(\"\\nRunning final evaluation...\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    for key, value in sorted(eval_metrics.items()):\n",
    "        formatted = f\"{value:.4f}\" if isinstance(value, float) else str(value)\n",
    "        print(f\"  {key}: {formatted}\")\n",
    "    print(f\"\\nSaving model and tokenizer to {output_dir}...\")\n",
    "    trainer.save_model(str(output_dir))\n",
    "    tokenizer.save_pretrained(str(output_dir))\n",
    "    elapsed = time.time() - start_time\n",
    "    save_training_log(output_dir, config, train_result, eval_metrics, elapsed)\n",
    "    print(f\"Training log saved to {output_dir / 'training_log.json'}\")\n",
    "    print(f\"\\nDone! Elapsed: {elapsed:.1f}s\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def parse_args() -> TrainConfig:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train the SciX enrichment NER model (SciBERT token classification).\"\n",
    "    )\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"allenai/scibert_scivocab_uncased\")\n",
    "    parser.add_argument(\"--train-file\", type=str, required=True)\n",
    "    parser.add_argument(\"--val-file\", type=str, required=True)\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"output/enrichment_model\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16)\n",
    "    parser.add_argument(\"--num-epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--warmup-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--max-seq-length\", type=int, default=256)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=1)\n",
    "    parser.add_argument(\"--early-stopping-patience\", type=int, default=3)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--fp16\", action=\"store_true\")\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\")\n",
    "    parser.add_argument(\"--logging-steps\", type=int, default=50)\n",
    "    parser.add_argument(\"--eval-steps\", type=int, default=200)\n",
    "    parser.add_argument(\"--save-steps\", type=int, default=200)\n",
    "    parser.add_argument(\"--save-total-limit\", type=int, default=3)\n",
    "    args = parser.parse_args()\n",
    "    return TrainConfig(\n",
    "        model_name=args.model_name,\n",
    "        train_file=args.train_file,\n",
    "        val_file=args.val_file,\n",
    "        output_dir=args.output_dir,\n",
    "        learning_rate=args.learning_rate,\n",
    "        batch_size=args.batch_size,\n",
    "        num_epochs=args.num_epochs,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        weight_decay=args.weight_decay,\n",
    "        max_seq_length=args.max_seq_length,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        early_stopping_patience=args.early_stopping_patience,\n",
    "        seed=args.seed,\n",
    "        fp16=args.fp16,\n",
    "        bf16=args.bf16,\n",
    "        logging_steps=args.logging_steps,\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_steps=args.save_steps,\n",
    "        save_total_limit=args.save_total_limit,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(train(parse_args()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Training\n",
    "\n",
    "Detect GPU type and set the appropriate mixed-precision flag:\n",
    "- **T4**: Use `--fp16` (no bf16 support)\n",
    "- **A100/H100**: Use `--bf16` (more stable and efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Auto-detect GPU precision support\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    # A100/H100/L4 support bf16; T4/V100/P100 use fp16\n",
    "    if any(arch in gpu_name for arch in [\"A100\", \"H100\", \"L4\", \"L40\"]):\n",
    "        PRECISION_FLAG = \"--bf16\"\n",
    "    else:\n",
    "        PRECISION_FLAG = \"--fp16\"\n",
    "    print(f\"GPU: {gpu_name} -> using {PRECISION_FLAG}\")\n",
    "else:\n",
    "    PRECISION_FLAG = \"\"\n",
    "    print(\"No GPU detected, training without mixed precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_enrichment_model.py \\\n",
    "    --model-name allenai/scibert_scivocab_uncased \\\n",
    "    --train-file {TRAIN_FILE} \\\n",
    "    --val-file {VAL_FILE} \\\n",
    "    --output-dir {OUTPUT_DIR} \\\n",
    "    --num-epochs 10 \\\n",
    "    --batch-size 16 \\\n",
    "    --learning-rate 2e-5 \\\n",
    "    --warmup-ratio 0.1 \\\n",
    "    --early-stopping-patience 3 \\\n",
    "    {PRECISION_FLAG}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "log_path = Path(OUTPUT_DIR) / \"training_log.json\"\n",
    "if log_path.exists():\n",
    "    with open(log_path) as f:\n",
    "        log = json.load(f)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Training Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Model:   {log['model_name']}\")\n",
    "    print(f\"Labels:  {log['num_labels']} BIO tags\")\n",
    "    print(f\"Elapsed: {log['elapsed_seconds']:.1f}s\")\n",
    "    print()\n",
    "\n",
    "    if \"eval\" in log:\n",
    "        print(\"Evaluation metrics:\")\n",
    "        for k, v in sorted(log[\"eval\"].items()):\n",
    "            formatted = f\"{v:.4f}\" if isinstance(v, float) else str(v)\n",
    "            print(f\"  {k}: {formatted}\")\n",
    "else:\n",
    "    print(f\"Training log not found at {log_path}\")\n",
    "    print(\"Training may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Checkpoint to Google Drive\n",
    "\n",
    "Copy the trained model to a persistent Google Drive location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "DRIVE_SAVE_DIR = \"/content/drive/MyDrive/scix-enrichment/enrichment_model\"\n",
    "drive_path = Path(DRIVE_SAVE_DIR)\n",
    "model_path = Path(OUTPUT_DIR)\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"Model directory not found: {model_path}\")\n",
    "    print(\"Run training first (Section 4).\")\n",
    "else:\n",
    "    drive_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy all model files\n",
    "    files_copied = []\n",
    "    for src_file in model_path.iterdir():\n",
    "        if src_file.is_file():\n",
    "            dst_file = drive_path / src_file.name\n",
    "            shutil.copy2(src_file, dst_file)\n",
    "            size_mb = dst_file.stat().st_size / (1024 * 1024)\n",
    "            files_copied.append((src_file.name, size_mb))\n",
    "\n",
    "    print(f\"Saved {len(files_copied)} files to {DRIVE_SAVE_DIR}:\")\n",
    "    for name, size in sorted(files_copied):\n",
    "        print(f\"  {name} ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Push to HuggingFace Hub (Optional)\n",
    "\n",
    "Requires `HF_TOKEN` environment variable with a write-access token.\n",
    "Get one from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\nHF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\nHF_REPO_ID = \"your-username/scix-enrichment-ner\"  # Change to your repo\n\nif not HF_TOKEN:\n    print(\"HF_TOKEN not set. Skipping HuggingFace Hub push.\")\n    print(\"To push, set HF_TOKEN:\")\n    print('  import os; os.environ[\"HF_TOKEN\"] = \"<your-token>\"')\nelse:\n    from transformers import AutoModelForTokenClassification, AutoTokenizer\n\n    model_path = Path(OUTPUT_DIR)\n    if not model_path.exists():\n        print(f\"Model not found at {model_path}. Run training first.\")\n    else:\n        print(f\"Loading model from {model_path}...\")\n        model = AutoModelForTokenClassification.from_pretrained(str(model_path))\n        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n\n        print(f\"Pushing to {HF_REPO_ID}...\")\n        model.push_to_hub(HF_REPO_ID, token=HF_TOKEN)\n        tokenizer.push_to_hub(HF_REPO_ID, token=HF_TOKEN)\n        print(f\"Model pushed to https://huggingface.co/{HF_REPO_ID}\")"
  }
 ]
}