# NLS Fine-tune SciX - Hybrid NER Pipeline

## Codebase Patterns

- **Operator Gating Rule**: Only set IntentSpec.operator when EXPLICIT patterns match ("papers citing X", "references of X"), NOT for generic words like "citing" or "references" used as topics
- **FIELD_ENUMS Validation**: All doctype/property/database/bibgroup/esources/data values MUST be validated against FIELD_ENUMS before assembly
- **Quote Multi-word Phrases**: Single-word values can be bare, multi-word phrases MUST be quoted in ADS syntax
- **No LLM for Operators**: Operators become enum decisions, not generated text. LLM only for paper reference resolution

## Project Context

This Ralph cycle implements a **hybrid NER + fine-tuned model pipeline** to replace end-to-end generation that caused malformed operators like `citations(abs:referencesabs:...)`.

**Previous approach (archived)**: End-to-end fine-tuned Qwen3-1.7B model that learned to conflate natural language words with ADS operator syntax.

**New approach**: 
1. NER extracts structured IntentSpec
2. Few-shot retrieval from gold_examples.json for pattern guidance
3. Deterministic template assembly with FIELD_ENUMS validation
4. Fine-tuned model ONLY as fallback for ambiguous paper references

## Key Learnings from Previous Iteration

- Training data quality is the root cause of malformed operators
- 7K training examples with only 4.4% operator coverage conflated NL with ADS syntax
- Post-processing filters (constrain.py) are band-aids, not solutions
- Need deterministic query building, not end-to-end generation

---

## Progress Log

(Entries will be appended below as stories are completed)

---
