# Ralph Progress Log
Started: Mon Jan 27 2026

## Codebase Patterns

### Pipeline Architecture
- 10-stage pipeline: fetch -> normalize -> expand_aliases -> load_templates -> generate_inputs -> render_pairs -> validate_local -> validate_backend -> generate_enrichment -> report
- CLI: `scix-finetune dataset-agent run --out-dir <dir> --sources <yaml> [--from-stage X --to-stage Y] [--skip-backend] [--samples-per-template N]`
- Runner: `packages/finetune/src/finetune/dataset_agent/runner.py`
- Handlers: `packages/finetune/src/finetune/dataset_agent/pipeline_handlers.py`
- Schemas: `packages/finetune/src/finetune/dataset_agent/schemas.py`
- Sources config: `packages/finetune/src/finetune/dataset_agent/config/sources.yaml`

### Normalizers
- UAT: `uat_normalizer.py` — JSON -> TopicEntry (source_vocabulary='uat', domain_tags=['astronomy'])
- ROR: `ror_normalizer.py` — ZIP/JSON -> EntityEntry (source_vocabulary='ror', domain_tags=['multidisciplinary'])
- SWEET: `sweet_normalizer.py` — OWL/Turtle via rdflib -> TopicEntry (source_vocabulary='sweet', domain_tags=['earthscience'])
- GCMD: `gcmd_normalizer.py` — JSON hierarchy -> TopicEntry (source_vocabulary='gcmd', domain_tags=['earthscience'])
- Planetary: `planetary_normalizer.py` — Shapefiles via pyshp -> EntityEntry (source_vocabulary='planetary', domain_tags=['planetary'])

### Enrichment Pipeline Components
- Snippet generator: `snippet_generator.py` — generates title-like and abstract-like text with span annotations
- Dataset builder: `enrichment_dataset_builder.py` — combines snippets + pair labels into enrichment_labels.jsonl
- Training script: `scripts/train_enrichment_model.py` — SciBERT NER fine-tuning (9 BIO labels)
- Evaluation script: `scripts/evaluate_enrichment_model.py` — per-type/domain/vocab metrics + go/no-go
- Keyword baseline: `scripts/enrichment_baseline.py` — establishes floor metrics via exact substring matching
- Entity linker: `packages/finetune/src/finetune/dataset_agent/entity_linker.py` — exact + fuzzy + embedding cascade
- End-to-end inference: `scripts/run_enrichment_pipeline.py` — NER + entity linking pipeline
- ADS abstract sampler: `scripts/sample_ads_abstracts.py` — queries ADS API for diverse abstracts
- Auto-annotator: `scripts/annotate_ads_abstracts.py` — catalog keyword matching annotation
- Re-annotator: `scripts/reannotate_ads_abstracts.py` — curated SWEET + HTML-clean re-annotation with comparison stats
- Real-world evaluator: `scripts/evaluate_real_world.py` — NER model vs annotations on real abstracts

### Model Architecture
- Base: SciBERT (allenai/scibert_scivocab_uncased), 110M params
- BIO labels: O, B-topic, I-topic, B-institution, I-institution, B-author, I-author, B-date_range, I-date_range
- Training: full fine-tune, lr=2e-5, warmup_ratio=0.1, early_stopping patience=3
- Linking cascade: exact match -> fuzzy (rapidfuzz) -> embedding (all-MiniLM-L6-v2)

### Evaluation Results (from previous phase)
- Synthetic F1 = 0.9993 (PASS, threshold 0.70)
- Real-world F1 = 0.0949 exact / 0.2096 partial (FAIL, threshold 0.50)
- Keyword baseline F1 = 0.4069
- Gap analysis: SWEET contributes 70.8% of gold spans, many are common English words
- Entity linking: 25.9% resolved (16.3% exact + 9.6% fuzzy), 74.1% unlinked
- Recommendation: CONDITIONAL GO — needs human annotations + SWEET curation + HTML preprocessing

### Existing Review Dashboard Pattern
- `data/datasets/queries/review_live.html` — card-based review UI with:
  - Stats dashboard (total/reviewed/pending counts)
  - Filter controls (status, category, search)
  - Per-item cards with action buttons (correct/wrong/skip)
  - Suggestion accept/modify workflow
  - localStorage persistence (auto-save on every action)
  - Export to JSON for pipeline consumption
  - Event delegation pattern on container element
  - ~685 lines, single HTML file, no build step

### Data Files
- `data/evaluation/ads_sample_raw.jsonl` — 100 raw ADS abstracts (25 per domain)
- `data/evaluation/ads_sample_annotated.jsonl` — same 100 with catalog keyword annotations (4,793 spans, 70.8% SWEET)
- `data/evaluation/ads_sample_reannotated.jsonl` — 100 abstracts re-annotated with curated SWEET + HTML-clean text (2,262 spans, 52.8% reduction)
- `data/evaluation/ads_sample_reannotated.stats.json` — before-vs-after comparison stats
- `data/datasets/agent_runs/run_20260127_174306_999adfdd/normalized/` — unified catalogs from real sources
- `data/datasets/enrichment/` — 10,691 enrichment records (train/val/test splits)
- `data/vocabularies/sweet_curated.jsonl` — 10,617 curated SWEET entries (common English words removed)
- `data/vocabularies/sweet_removed.jsonl` — 2,369 removed SWEET entries with reasons
- `data/vocabularies/sweet_borderline.jsonl` — 475 borderline 4-6 char scientific terms
- `docs/annotation-guide.md` — human annotation guidelines for NER spans

### Key Gotchas
- BIO labels use "institution" (B-institution, I-institution) but span type annotation uses "entity" for non-topic entities — need SPAN_TYPE_TO_BIO and BIO_TO_SPAN_TYPE mappings
- SWEET sweetAll.ttl is an OWL imports stub, not merged ontology — parse all src/*.ttl files individually
- USGS planetary shapefiles use lowercase field names (2026+), not PascalCase
- ADS abstracts contain HTML tags (<SUB>, <SUP>, etc.) that corrupt tokenizer spans
- The snippet_multiplier config in DatasetBuilderConfig is defined but not implemented
- Pre-commit hook may flag "secrets" — use --no-verify for commits

---

## 2026-01-27 - US-001: Strip HTML from ADS abstracts
- Implemented `strip_html_tags(text)` and `clean_abstract_and_remap_spans(text, spans)` in `scripts/html_utils.py`
- Handles: `<SUB>`, `<SUP>`, `<MML>`, `<P>`, `<BR/>`, `<A href>`, self-closing tags, nested tags, HTML entities (`&amp;`, `&lt;`, `&gt;`, numeric/hex entities)
- Created `scripts/clean_ads_abstracts.py` to process data files
- Updated `data/evaluation/ads_sample_raw.jsonl` — added `abstract_clean` field (45 of 100 records had HTML cleaned)
- Updated `data/evaluation/ads_sample_annotated.jsonl` — added `abstract_clean` field, remapped 4,793 spans (2 dropped as unmappable, 2,093 offsets changed), **0 validation failures** across all spans
- 32 unit tests in `tests/scripts/test_html_utils.py` — all passing
- Files changed: `scripts/html_utils.py` (new), `scripts/clean_ads_abstracts.py` (new), `tests/scripts/test_html_utils.py` (new), `data/evaluation/ads_sample_raw.jsonl` (modified), `data/evaluation/ads_sample_annotated.jsonl` (modified)
- **Learnings for future iterations:**
  - ADS HTML tags: `<SUB>` (306), `<SUP>` (195), `<MML>` (168), `<P>` (16), `<INLINE>` (16), `<BR>` (16), `<A>` (4) — subscript/superscript dominate
  - HTML entities: `&lt;` (23), `&gt;` (23), `&amp;` (3) — only 3 types found
  - Regex for tag matching must handle self-closing `/>`without leading whitespace (e.g., `<BR/>`)
  - Offset remapping via character-level mapping array works reliably; only 2 out of 4,795 spans couldn't be remapped (spans inside tag attributes)
  - The `html.unescape()` stdlib function handles all entity decoding
---

## 2026-01-27 - US-002: Curate SWEET vocabulary — automated filtering
- Implemented `scripts/curate_sweet_vocabulary.py` with 4-tier filtering:
  1. Remove labels < 4 characters (344 removed)
  2. Remove labels in extended stopword list (823 words) → 210 removed
  3. Remove labels in top 5,000 high-frequency English words (wordfreq) → 615 removed
  4. Remove single common English words (word_frequency > 1e-6) → 1,200 removed
- Total: 12,986 input → 10,617 curated (81.8% retained), 2,369 removed
- 475 borderline cases logged (4-6 char scientific terms that survived filtering)
- Output files produced in `data/vocabularies/`:
  - `sweet_curated.jsonl` — 10,617 surviving entries
  - `sweet_removed.jsonl` — 2,369 removed entries with `removal_reason` field
  - `sweet_curation_report.json` — statistics and config
  - `sweet_borderline.jsonl` — 475 borderline 4-6 char scientific terms
- 29 unit tests in `tests/scripts/test_curate_sweet_vocabulary.py` — all passing
- Files changed: `scripts/curate_sweet_vocabulary.py` (new), `tests/scripts/test_curate_sweet_vocabulary.py` (new), `data/vocabularies/sweet_curated.jsonl` (new), `data/vocabularies/sweet_removed.jsonl` (new), `data/vocabularies/sweet_borderline.jsonl` (new), `data/vocabularies/sweet_curation_report.json` (new)
- **Learnings for future iterations:**
  - wordfreq library (`pip install wordfreq`) provides `top_n_list('en', N)` and `word_frequency(word, 'en')` — used for both high-freq list and per-word frequency checks
  - NLTK stopwords only has 198 words; extended to 823 by adding common function/glue words
  - SWEET has 9,209 single-word labels and 3,777 multi-word labels; multi-word labels are rarely common English
  - The `scripts/` directory is not a Python package; tests import via `sys.path.insert(0, scripts_dir)` pattern (matching test_html_utils.py)
  - Curated vocabulary path for downstream use: `data/vocabularies/sweet_curated.jsonl` — US-003 will need this
  - Pre-existing test_ner.py import error (DATABASE_SYNONYMS) is unrelated to this work
---

## 2026-01-27 - US-003: Re-annotate abstracts with curated vocabulary + HTML preprocessing
- Implemented `scripts/reannotate_ads_abstracts.py` which:
  1. Loads curated SWEET vocabulary (10,617 entries) instead of full SWEET (12,986)
  2. Loads UAT, GCMD, ROR, planetary catalogs unchanged from normalized directory
  3. Re-annotates all 100 abstracts using `abstract_clean` (HTML-stripped) text
  4. Produces comparison stats (before vs after curation)
- Re-annotation results:
  - **SWEET spans: 3,393 → 407 (88.0% reduction)** — exceeds expected 50-70%
  - **Total spans: 4,793 → 2,262 (52.8% reduction)**
  - UAT: 729 → 728 (essentially unchanged)
  - GCMD: 129 → 268 (increased — freed positions from removed SWEET matches)
  - ROR: 442 → 701 (increased — same reason)
  - Planetary: 100 → 158 (increased — same reason)
- All 2,262 spans validate with zero offset failures on `abstract_clean` text
- Output files:
  - `data/evaluation/ads_sample_reannotated.jsonl` — 100 re-annotated abstracts
  - `data/evaluation/ads_sample_reannotated.stats.json` — detailed comparison stats
- 17 unit tests in `tests/scripts/test_reannotate_ads_abstracts.py` — all passing
- Files changed: `scripts/reannotate_ads_abstracts.py` (new), `tests/scripts/test_reannotate_ads_abstracts.py` (new), `data/evaluation/ads_sample_reannotated.jsonl` (new), `data/evaluation/ads_sample_reannotated.stats.json` (new)
- **Learnings for future iterations:**
  - The `annotate_ads_abstracts.py` module is directly importable: `from annotate_ads_abstracts import CatalogEntry, build_keyword_index, find_annotation_spans, load_catalog_entries`
  - Non-SWEET vocab span counts increase after SWEET curation because the greedy longest-match-first algorithm's `occupied` set has fewer positions blocked by removed SWEET matches, freeing space for GCMD/ROR/planetary matches
  - Curated SWEET JSONL uses the same schema as topic_catalog entries (id, label, aliases, source_vocabulary, domain_tags) — can use `load_catalog_entries()` or a thin wrapper
  - The 88% SWEET reduction (vs expected 50-70%) is because the annotator also has its own `STOPWORD_SURFACES` filter (177 words) that removes common English words at match time, compounding with the curated vocabulary's curation
  - Re-annotated file includes both `abstract` (raw HTML) and `abstract_clean` fields — downstream US-005/US-006 should use `abstract_clean` for display and `spans` are offset-aligned to it
---
