# Ralph Progress Log
Started: Mon Jan 27 2026

## Codebase Patterns

### Pipeline Architecture
- 10-stage pipeline: fetch -> normalize -> expand_aliases -> load_templates -> generate_inputs -> render_pairs -> validate_local -> validate_backend -> generate_enrichment -> report
- CLI: `scix-finetune dataset-agent run --out-dir <dir> --sources <yaml> [--from-stage X --to-stage Y] [--skip-backend] [--samples-per-template N]`
- Runner: `packages/finetune/src/finetune/dataset_agent/runner.py`
- Handlers: `packages/finetune/src/finetune/dataset_agent/pipeline_handlers.py`
- Schemas: `packages/finetune/src/finetune/dataset_agent/schemas.py`
- Sources config: `packages/finetune/src/finetune/dataset_agent/config/sources.yaml`

### Normalizers
- UAT: `uat_normalizer.py` — JSON → TopicEntry (source_vocabulary='uat', domain_tags=['astronomy'])
- ROR: `ror_normalizer.py` — ZIP/JSON → EntityEntry (source_vocabulary='ror', domain_tags=['multidisciplinary'])
- SWEET: `sweet_normalizer.py` — OWL/Turtle via rdflib → TopicEntry (source_vocabulary='sweet', domain_tags=['earthscience'])
- GCMD: `gcmd_normalizer.py` — JSON hierarchy → TopicEntry (source_vocabulary='gcmd', domain_tags=['earthscience'])
- Planetary: `planetary_normalizer.py` — Shapefiles via pyshp → EntityEntry (source_vocabulary='planetary', domain_tags=['planetary'])

### Enrichment Pipeline Components
- Snippet generator: `snippet_generator.py` — generates title-like and abstract-like text with span annotations
- Dataset builder: `enrichment_dataset_builder.py` — combines snippets + pair labels into enrichment_labels.jsonl
- Training script: `scripts/train_enrichment_model.py` — SciBERT NER fine-tuning (9 BIO labels)
- Evaluation script: `scripts/evaluate_enrichment_model.py` — per-type/domain/vocab metrics + go/no-go
- Keyword baseline: `scripts/enrichment_baseline.py` — establishes floor metrics via exact substring matching

### Model Architecture (from docs/enrichment-model-selection.md)
- Task: Joint NER + Entity Linking
- NER model: SciBERT (allenai/scibert_scivocab_uncased), 110M params
- BIO labels: O, B-topic, I-topic, B-institution, I-institution, B-author, I-author, B-date_range, I-date_range
- Training: full fine-tune, lr=2e-5, warmup_ratio=0.1, early_stopping patience=3
- Linking cascade: exact match → fuzzy (rapidfuzz) → embedding (all-MiniLM-L6-v2)

### Span Type Convention
- Enrichment dataset uses `"topic"` and `"entity"` as span types (NOT `"institution"`)
- When loading entity catalogs for matching, always use `entry_type="entity"`
- The BIO labels use "institution" (B-institution, I-institution) but the span type annotation uses "entity"
- CRITICAL: Training script `_char_labels` must map "entity" → "institution" via SPAN_TYPE_TO_BIO; evaluation script must reverse-map via BIO_TO_SPAN_TYPE
- Keyword baseline floor: overall F1=0.41 (P=0.26, R=1.00) — NER model target is F1 >= 0.70
- NER model result: overall F1=0.9993 (P=0.9993, R=0.9993) — GO recommendation
- Training on MPS (Apple Silicon) works without issues; early stopping triggers after ~1.5 epochs

### Dependencies Added in Previous Phase
- rdflib >= 7.0.0 (SWEET parsing)
- pyshp >= 2.3.0 (planetary shapefiles)
- New this phase: rapidfuzz (fuzzy matching), sentence-transformers (embedding linking)

### Test Coverage
- 998 unit tests passing across 22 test files
- All tests use mocked/synthetic data — this phase tests with real data

### Real Data Findings (US-001)
- SWEET sweetAll.ttl is an OWL imports stub, NOT a merged ontology — must parse all src/*.ttl files individually
- USGS planetary shapefiles (2026+) use lowercase field names: `name`, `clean_name`, `type`, `approval`, `diameter`, `center_lat`, `center_lon` — not PascalCase `Feature_Name` etc.
- USGS ALL_NOMENCLATURE.zip no longer exists — download per-body files from S3: `MARS_nomenclature_center_pts.zip`, `MOON_nomenclature_center_pts.zip`
- Per-body shapefiles lack `Feature_ID` and `Target` fields — extract feature ID from `link` URL, infer target from source_id
- `uv pip install typer` may be needed separately if editable install breaks typer resolution

---

## 2026-01-27 17:45 — US-001
- **Implemented**: Run pipeline fetch + normalize with all 5 real sources
- **Results**:
  - UAT: 2,411 concepts (astronomy)
  - ROR: 118,492 institutions (multidisciplinary)
  - SWEET: 12,986 Earth science concepts (from 226 .ttl files)
  - GCMD: 3,155 keywords (Earth science hierarchy)
  - Planetary Mars: 2,045 features (craters, mons, vallis, etc.)
  - Planetary Moon: 9,084 features
  - Merged topic_catalog.jsonl: 18,552 entries
  - Merged entity_catalog.jsonl: 129,621 entries
  - Source manifest: 6 sources with checksums, licenses, provenance
- **Files changed**:
  - `packages/finetune/pyproject.toml` — Added rdflib, pyshp dependencies
  - `packages/finetune/src/finetune/cli/main.py` — Registered dataset-agent CLI subcommand
  - `packages/finetune/src/finetune/dataset_agent/config/sources.yaml` — Fixed SWEET files glob, split planetary into Mars+Moon with corrected S3 URLs
  - `packages/finetune/src/finetune/dataset_agent/planetary_normalizer.py` — Added lowercase field aliases, feature ID extraction from link URL, target inference from source_id
  - `packages/finetune/src/finetune/dataset_agent/sweet_normalizer.py` — Parse all .ttl files instead of prioritizing sweetAll.ttl imports stub
- **Learnings for future iterations**:
  - USGS data format changed since pipeline was built — always verify real data field names
  - SWEET ontology is distributed across 226 files, not one merged file
  - The git downloader correctly handles `files: [src/*.ttl]` glob patterns
  - Per-body planetary sources work well — can add Mercury, Venus etc. by just adding sources.yaml entries
  - Pipeline completes fetch+normalize in ~90 seconds (including downloads)
---

## 2026-01-27 13:05 — US-002
- **Implemented**: Run full pipeline with small sample + snippet generation + enrichment dataset build
- **Approach**: Wrote `scripts/run_us002_pipeline.py` that:
  1. Continues pipeline stages 3-10 (expand_aliases through report) using existing US-001 run dir
  2. Generates snippets from real unified catalogs using multi-round generation (8 rounds, different seeds)
  3. Builds unified enrichment dataset with train/val/test splits
- **Results**:
  - Pipeline stages: 25 templates loaded, 250 inputs generated, 240 valid pairs (10 quarantined for unknown_field), 130 pair-based enrichment labels
  - Snippets: 8,936 unique snippets (7,700 titles + 1,236 abstracts) with 11,127 spans
  - Enrichment dataset: 8,936 total records (7,148 train / 893 val / 895 test)
  - All 5 source vocabularies represented: uat (4,356), sweet (3,412), ror (1,278), planetary (1,267), gcmd (814)
  - All 4 domains covered: astronomy (3,245), earthscience (3,146), multidisciplinary (1,278), planetary (1,267)
  - Span types: 8,582 topic spans, 2,545 entity spans
- **Files changed**:
  - `scripts/run_us002_pipeline.py` — New continuation script for US-002
  - `packages/finetune/src/finetune/domains/scix/keyword_constraints.py` — Fixed import ordering (pre-existing lint issue)
- **Learnings for future iterations**:
  - The pipeline CLI always creates a new run directory — to continue from a previous run, use a custom script that loads state from the existing dir
  - Snippet generator produces ~1,100-1,200 unique snippets per round (23 title + 8 abstract templates with random catalog sampling)
  - Multi-round generation with different seeds is the way to scale up snippet diversity — 8 rounds yielded ~8.9K unique snippets
  - The `snippet_multiplier` config in DatasetBuilderConfig is defined but not implemented — multi-round generation at the caller level is the workaround
  - The enrichment_dataset_builder deduplicates by text hash, so round overlap is handled automatically
  - 10 pairs quarantined for `unknown_field` — likely template references to fields not in the validation allowlist
---

## 2026-01-27 13:15 — US-003
- **Implemented**: Full-scale enrichment dataset generation (10K+ examples)
- **Approach**: Created `scripts/run_us003_fullscale.py` that:
  1. Reuses normalized catalogs from US-001 run directory
  2. Runs multi-round snippet generation (8 rounds of 3000 titles + 3000 abstracts)
  3. Early-stops when both total count and per-vocabulary minimums are met
  4. Builds unified enrichment dataset with train/val/test splits
  5. Copies to `data/datasets/enrichment/` for Colab upload
  6. Validates all acceptance criteria including 20-sample span spot-check
- **Results**:
  - Total: 10,691 enrichment records (8,552 train / 1,069 val / 1,070 test)
  - Vocabulary coverage: UAT (10,410 spans), SWEET (8,082), GCMD (1,944), ROR (1,542), Planetary (1,541)
  - All 5 vocabularies >= 500 examples
  - Domain coverage: astronomy (3,869), earthscience (3,739), multidisciplinary (1,542), planetary (1,541)
  - Label types: 10,218 topic spans, 3,083 entity spans
  - Text types: 9,220 titles, 1,471 abstracts
  - Span accuracy: 20/20 spot-checked samples passed byte-exact validation
  - 8 rounds sufficient (early exit at round 8 of 15 max)
- **Files changed**:
  - `scripts/run_us003_fullscale.py` — New full-scale generation script
  - `data/datasets/enrichment/` — Output directory with train/val/test splits + coverage report
- **Learnings for future iterations**:
  - 3000 titles + 3000 abstracts per round yields ~1,300-1,400 unique snippets per round (higher per-round counts compensate for dedup losses)
  - 8 rounds with 3000+3000 is enough to exceed 10K (vs. 8 rounds with 2500+2500 yielded only ~8.9K)
  - Early exit logic saves unnecessary rounds when coverage targets are already met
  - The vocabulary coverage counts in the coverage report are SPAN-level (not record-level), so counts can exceed total records (multiple spans per record referencing different vocabs)
  - GCMD has the lowest coverage because it has fewer unique concepts (3,155) vs. UAT (2,411 but more templates match) and SWEET (12,986)
---

## 2026-01-27 — US-004
- **Implemented**: Colab notebook for SciBERT NER training
- **Approach**: Created self-contained `notebooks/train_enrichment_model.ipynb` that:
  1. Installs deps (transformers, datasets, torch, accelerate, numpy)
  2. Offers Google Drive mount or direct upload for data files
  3. Validates data files exist and shows record counts
  4. Writes the full training script inline via `%%writefile` (self-contained — no repo clone needed)
  5. Auto-detects GPU type and selects `--fp16` (T4) or `--bf16` (A100/H100)
  6. Runs training with all standard hyperparameters (lr=2e-5, epochs=10, batch=16, warmup=0.1, early_stopping=3)
  7. Displays training results from training_log.json
  8. Saves checkpoint to Google Drive
  9. Optionally pushes to HuggingFace Hub (gated on HF_TOKEN)
- **Files changed**:
  - `notebooks/train_enrichment_model.ipynb` — New Colab notebook (21 cells)
- **Learnings for future iterations**:
  - The notebook uses `%%writefile` to create the training script locally in Colab so it's fully self-contained without needing to clone the repo
  - GPU precision detection: T4/V100/P100 use fp16; A100/H100/L4/L40 use bf16
  - The training script is inlined verbatim from `scripts/train_enrichment_model.py` to ensure consistency
  - Google Drive path convention: `/content/drive/MyDrive/scix-enrichment/` for data and model outputs
---

## 2026-01-27 — US-005
- **Implemented**: Keyword baseline evaluation on real enrichment test data
- **Approach**: Ran `scripts/enrichment_baseline.py` with real test file (1,070 records) and unified catalogs (18,552 topics + 129,621 entities = 286,733 keyword surface forms)
- **Bug Fix**: Entity catalog entries were loaded with `entry_type="institution"` but the enrichment dataset uses `"entity"` as the span type — fixed to use `"entity"` for consistency with the snippet generator
- **Results**:
  - **Overall**: P=0.2556 R=0.9963 F1=0.4069 (1,361 TP, 3,963 FP, 5 FN)
  - **By type**: topic P=0.2389 R=0.9953 F1=0.3853 | entity P=0.3435 R=1.0000 F1=0.5113
  - **By vocabulary**: UAT F1=0.6422, GCMD F1=0.5570, ROR F1=0.3412, SWEET F1=0.2480, Planetary F1=1.0000
  - **By domain**: astronomy F1=0.6422, earthscience F1=0.2778, multidisciplinary F1=0.3412, planetary F1=1.0000
  - The keyword baseline has near-perfect recall (~99.6%) but poor precision (~25.6%) — produces ~4x as many predictions as gold spans
  - SWEET has the worst precision (0.1418) because many SWEET concepts are short common words that match everywhere
  - Planetary has perfect F1 because feature names (craters, mons) are unique and don't match spuriously
  - This establishes the floor: a trained NER model should significantly improve precision while maintaining high recall
- **Files changed**:
  - `scripts/enrichment_baseline.py` — Fixed entity type label from "institution" to "entity" (matching snippet generator convention)
  - `reports/enrichment_baseline.json` — Real baseline metrics output
- **Learnings for future iterations**:
  - The enrichment dataset uses `"entity"` (not `"institution"`) as the span type for entities — all entity catalog loading must use `"entity"` type to match
  - Keyword baseline is heavily recall-biased: with 286K surface forms, almost every gold span is found, but precision suffers from massive false positive counts
  - SWEET ontology terms are the biggest source of false positives (2,651 FPs for 438 TPs) — many Earth science terms are common English words
  - The baseline gives a clear F1 floor of ~0.41 that the trained NER model should beat (target: F1 >= 0.70)
---

## 2026-01-27 — US-006
- **Implemented**: Trained SciBERT NER model locally and evaluated on real test data
- **Bug Fix**: Training script `_char_labels` silently skipped all "entity" spans because `ENTITY_TYPES` only contained "institution". Added `SPAN_TYPE_TO_BIO` mapping (`"entity"` → `"institution"`) in training script and `BIO_TO_SPAN_TYPE` reverse mapping in evaluation script.
- **Training**: SciBERT (allenai/scibert_scivocab_uncased) fine-tuned on 8,552 training records, 1,069 validation records. Early stopping triggered after ~1.5 epochs. Trained on MPS (Apple Silicon) in ~9 minutes.
- **Results**:
  - **Overall**: P=0.9993, R=0.9993, F1=0.9993 (1,365 TP, 1 FP, 1 FN out of 1,366 gold spans)
  - **Macro F1 (by type)**: 0.9995
  - **By type**: topic F1=0.9991 | entity F1=1.0000
  - **By vocabulary**: UAT F1=1.0000, GCMD F1=1.0000, ROR F1=1.0000, Planetary F1=1.0000, SWEET F1=0.9989
  - **By domain**: astronomy F1=1.0000, earthscience F1=0.9991, multidisciplinary F1=1.0000, planetary F1=1.0000
  - **Baseline comparison**: NER model F1=0.9993 vs keyword baseline F1=0.4069 — precision improved from 0.2556 to 0.9993 (+0.7437)
  - **Go/No-Go**: **GO** — far exceeds the F1 >= 0.70 threshold
  - Only 1 error: single-character topic "a" at wrong offset (ambiguous single-letter entity)
- **Note**: The near-perfect F1 on synthetic test data is expected — both train and test come from the same template-based generator. Real-world performance on ADS abstracts (US-011) will be the true test.
- **Files changed**:
  - `scripts/train_enrichment_model.py` — Added SPAN_TYPE_TO_BIO mapping for entity→institution in _char_labels
  - `scripts/evaluate_enrichment_model.py` — Added BIO_TO_SPAN_TYPE mapping for institution→entity in predict_spans_for_text
  - `reports/enrichment_model_eval.json` — Real evaluation metrics (not synthetic)
  - `reports/enrichment_model_eval.md` — Full narrative report with baseline comparison, breakdowns, examples, and go/no-go
- **Learnings for future iterations**:
  - The entity/institution type mismatch is the most critical gotcha in this pipeline — always verify that span types in the dataset match the BIO label schema
  - SciBERT fine-tuning on synthetic enrichment data converges very fast (1-2 epochs) because the templates are relatively repetitive
  - MPS (Apple Silicon) training works without issues for SciBERT-sized models; no fp16/bf16 needed
  - Near-perfect synthetic metrics are expected; the real-world gap (US-011) will be significant
  - The single error case (single-letter "a" as a SWEET topic) highlights that very short entity names are problematic
---

## 2026-01-27 — US-007
- **Implemented**: Entity linker module with exact + fuzzy (Levenshtein) matching
- **Approach**: Created `entity_linker.py` with:
  1. `build_linking_index(topic_catalog_path, entity_catalog_path)` — builds a pre-computed lookup from JSONL catalogs
  2. `link_span(surface, span_type, index)` — two-stage cascade: exact match (confidence=1.0) then fuzzy match (confidence=0.8)
  3. `save_linking_index()` / `load_linking_index()` — JSON serialization for index reuse
  4. Type filtering: topics only match topic catalog, entities only match entity catalog
  5. Case-insensitive exact match against both labels and aliases
  6. Fuzzy match uses `rapidfuzz.fuzz.ratio` with normalized similarity >= 0.85 threshold
- **Dependencies**: Added `rapidfuzz>=3.0.0` to pyproject.toml
- **Test coverage**: 36 unit tests covering exact match, fuzzy match, no match, case insensitivity, alias resolution, type filtering, index building, persistence, and LinkResult serialization
- **Files changed**:
  - `packages/finetune/src/finetune/dataset_agent/entity_linker.py` — New entity linker module
  - `packages/finetune/tests/test_entity_linker.py` — 36 unit tests
  - `packages/finetune/pyproject.toml` — Added rapidfuzz dependency
  - `scripts/ralph/prd.json` — Marked US-007 as passing
- **Learnings for future iterations**:
  - The `LinkingIndex` stores all surface forms lowercased in both `exact_map` (dict for O(1) lookup) and `entries` (list for linear fuzzy scan)
  - Fuzzy scan iterates all entries — for the full catalogs (~148K entries, ~286K surface forms) this may need optimization; consider pre-filtering by first character or length
  - The `_IndexEntry` is stored as plain dicts internally (not dataclasses) for JSON serialization simplicity
  - `_surface_forms()` deduplicates label from aliases — if the label appears in aliases, it's only indexed once
  - Type filtering is applied in both exact and fuzzy paths — a topic surface will never match as entity and vice versa
  - US-008 will extend this module with embedding-based fallback (sentence-transformers)
---

## 2026-01-27 — US-008
- **Implemented**: Embedding-based entity linking fallback and full three-stage cascade
- **Approach**: Extended `entity_linker.py` with:
  1. `build_embedding_index(catalog_entries, model_name)` — encodes all surface forms (label + aliases) using a sentence transformer, stores L2-normalized embeddings + metadata
  2. `build_embedding_index_from_catalogs(topic_path, entity_path, model_name)` — convenience wrapper to build from JSONL catalog files
  3. `link_span_embedding(surface, span_type, embedding_index, threshold)` — cosine similarity matching with type filtering, confidence = cosine_sim
  4. `save_embedding_index(index, directory)` / `load_embedding_index(directory)` — persistence as numpy .npy + JSON metadata
  5. `link_span_cascade(surface, span_type, linking_index, embedding_index)` — full cascade: exact → fuzzy → embedding
  6. `CatalogEntry` and `EmbeddingIndex` dataclasses for the embedding subsystem
- **Dependencies**: Added `sentence-transformers>=2.2.0` to pyproject.toml
- **Test coverage**: 25 new tests (61 total for entity_linker):
  - `TestBuildEmbeddingIndex` (5 tests): building, empty catalog, model name storage, metadata fields, L2 normalization
  - `TestLinkSpanEmbedding` (6 tests): match above threshold, no match below threshold, type filtering, empty surface, empty index, confidence is cosine similarity
  - `TestEmbeddingPersistence` (7 tests): roundtrip save/load, file existence, JSON validity, loaded index works, nonexistent raises, parent dir creation, shape preservation
  - `TestCascade` (6 tests): exact first, fuzzy second, embedding fallback, no match, without embedding index, without embedding no match
  - `TestCascadeIntegration` (1 test): 100 synthetic spans (25 exact, 25 fuzzy, 50 embedding/nomatch), verifies cascade ordering
- **Files changed**:
  - `packages/finetune/src/finetune/dataset_agent/entity_linker.py` — Extended with embedding linking, cascade, persistence
  - `packages/finetune/tests/test_entity_linker.py` — 25 new tests (61 total)
  - `packages/finetune/pyproject.toml` — Added sentence-transformers dependency
  - `scripts/ralph/prd.json` — Marked US-008 as passing
- **Learnings for future iterations**:
  - The `_load_sentence_transformer` function uses lazy import so the module works without sentence-transformers installed (for exact/fuzzy-only usage)
  - Embedding index stores L2-normalized vectors so cosine similarity = dot product (efficient for batch lookup)
  - Tests use a mock SentenceTransformer with deterministic random embeddings (seeded RandomState) — avoids downloading the real model in CI
  - The embedding index is persisted as two files: `embedding_vectors.npy` (numpy) + `embedding_metadata.json` (JSON) — kept separate for efficiency
  - Type filtering in embedding is done post-similarity by masking non-matching types to -1.0 before argmax
  - The cascade function accepts optional `embedding_index` (None = skip embedding stage) for backwards compatibility
  - For the full catalogs (~148K entries, ~286K surface forms), building the embedding index will be slow but only needs to happen once
---
