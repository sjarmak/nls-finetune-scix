# Ralph Progress Log
Started: Mon Jan 27 2026

## Codebase Patterns

### Pipeline Architecture
- 10-stage pipeline: fetch -> normalize -> expand_aliases -> load_templates -> generate_inputs -> render_pairs -> validate_local -> validate_backend -> generate_enrichment -> report
- CLI: `scix-finetune dataset-agent run --out-dir <dir> --sources <yaml> [--from-stage X --to-stage Y] [--skip-backend] [--samples-per-template N]`
- Runner: `packages/finetune/src/finetune/dataset_agent/runner.py`
- Handlers: `packages/finetune/src/finetune/dataset_agent/pipeline_handlers.py`
- Schemas: `packages/finetune/src/finetune/dataset_agent/schemas.py`
- Sources config: `packages/finetune/src/finetune/dataset_agent/config/sources.yaml`

### Normalizers
- UAT: `uat_normalizer.py` — JSON -> TopicEntry (source_vocabulary='uat', domain_tags=['astronomy'])
- ROR: `ror_normalizer.py` — ZIP/JSON -> EntityEntry (source_vocabulary='ror', domain_tags=['multidisciplinary'])
- SWEET: `sweet_normalizer.py` — OWL/Turtle via rdflib -> TopicEntry (source_vocabulary='sweet', domain_tags=['earthscience'])
- GCMD: `gcmd_normalizer.py` — JSON hierarchy -> TopicEntry (source_vocabulary='gcmd', domain_tags=['earthscience'])
- Planetary: `planetary_normalizer.py` — Shapefiles via pyshp -> EntityEntry (source_vocabulary='planetary', domain_tags=['planetary'])

### Enrichment Pipeline Components
- Snippet generator: `snippet_generator.py` — generates title-like and abstract-like text with span annotations
- Dataset builder: `enrichment_dataset_builder.py` — combines snippets + pair labels into enrichment_labels.jsonl
- Training script: `scripts/train_enrichment_model.py` — SciBERT NER fine-tuning (9 BIO labels)
- Evaluation script: `scripts/evaluate_enrichment_model.py` — per-type/domain/vocab metrics + go/no-go
- Keyword baseline: `scripts/enrichment_baseline.py` — establishes floor metrics via exact substring matching
- Entity linker: `packages/finetune/src/finetune/dataset_agent/entity_linker.py` — exact + fuzzy + embedding cascade
- End-to-end inference: `scripts/run_enrichment_pipeline.py` — NER + entity linking pipeline
- ADS abstract sampler: `scripts/sample_ads_abstracts.py` — queries ADS API for diverse abstracts
- Auto-annotator: `scripts/annotate_ads_abstracts.py` — catalog keyword matching annotation
- Re-annotator: `scripts/reannotate_ads_abstracts.py` — curated SWEET + HTML-clean re-annotation with comparison stats
- Real-world evaluator: `scripts/evaluate_real_world.py` — NER model vs annotations on real abstracts
- Prediction runner: `scripts/predict_ads_abstracts.py` — NER model inference on cleaned ADS abstracts with confidence scores

### Model Architecture
- Base: SciBERT (allenai/scibert_scivocab_uncased), 110M params
- BIO labels: O, B-topic, I-topic, B-institution, I-institution, B-author, I-author, B-date_range, I-date_range
- Training: full fine-tune, lr=2e-5, warmup_ratio=0.1, early_stopping patience=3
- Linking cascade: exact match -> fuzzy (rapidfuzz) -> embedding (all-MiniLM-L6-v2)

### Evaluation Results (from previous phase)
- Synthetic F1 = 0.9993 (PASS, threshold 0.70)
- Real-world F1 = 0.0949 exact / 0.2096 partial (FAIL, threshold 0.50)
- Keyword baseline F1 = 0.4069
- Gap analysis: SWEET contributes 70.8% of gold spans, many are common English words
- Entity linking: 25.9% resolved (16.3% exact + 9.6% fuzzy), 74.1% unlinked
- Recommendation: CONDITIONAL GO — needs human annotations + SWEET curation + HTML preprocessing

### Existing Review Dashboard Pattern
- `data/datasets/queries/review_live.html` — card-based review UI with:
  - Stats dashboard (total/reviewed/pending counts)
  - Filter controls (status, category, search)
  - Per-item cards with action buttons (correct/wrong/skip)
  - Suggestion accept/modify workflow
  - localStorage persistence (auto-save on every action)
  - Export to JSON for pipeline consumption
  - Event delegation pattern on container element
  - ~685 lines, single HTML file, no build step

### NER Annotation Dashboard Pattern
- `scripts/annotation_dashboard_template.html` — NER span annotation UI with:
  - Data injection via `/*DATA_PLACEHOLDER*/[]` marker — generation script replaces with inline JSON
  - Expected data schema: `{bibcode, title, abstract_clean, domain_category, citation_count, auto_spans[], model_spans[]}`
  - Span sources: auto (blue), model (green), agreement (purple, auto+model overlap), user-added (gold)
  - localStorage key: `nls-ner-annotations-v1`, state: `{annotations: {bibcode: {decisions, userSpans, notes}}, abstractStatus: {bibcode: status}}`
  - Export format: JSONL with accepted spans per abstract
  - Keyboard shortcuts: arrows (nav), 1-5 (span type), R (reviewed), S (skip)
  - ~1,240 lines, single HTML file, no build step

### Data Files
- `data/evaluation/ads_sample_raw.jsonl` — 100 raw ADS abstracts (25 per domain)
- `data/evaluation/ads_sample_annotated.jsonl` — same 100 with catalog keyword annotations (4,793 spans, 70.8% SWEET)
- `data/evaluation/ads_sample_reannotated.jsonl` — 100 abstracts re-annotated with curated SWEET + HTML-clean text (2,262 spans, 52.8% reduction)
- `data/evaluation/ads_sample_reannotated.stats.json` — before-vs-after comparison stats
- `data/datasets/agent_runs/run_20260127_174306_999adfdd/normalized/` — unified catalogs from real sources
- `data/datasets/enrichment/` — 10,691 enrichment records (train/val/test splits)
- `data/vocabularies/sweet_curated.jsonl` — 10,617 curated SWEET entries (common English words removed)
- `data/vocabularies/sweet_removed.jsonl` — 2,369 removed SWEET entries with reasons
- `data/vocabularies/sweet_borderline.jsonl` — 475 borderline 4-6 char scientific terms
- `data/evaluation/ads_sample_predictions.jsonl` — 100 abstracts with NER model predictions (2,753 spans, 97.5% topic, avg confidence 0.71)
- `data/evaluation/ads_sample_predictions.stats.json` — prediction summary stats
- `docs/annotation-guide.md` — human annotation guidelines for NER spans

### Key Gotchas
- BIO labels use "institution" (B-institution, I-institution) but span type annotation uses "entity" for non-topic entities — need SPAN_TYPE_TO_BIO and BIO_TO_SPAN_TYPE mappings
- SWEET sweetAll.ttl is an OWL imports stub, not merged ontology — parse all src/*.ttl files individually
- USGS planetary shapefiles use lowercase field names (2026+), not PascalCase
- ADS abstracts contain HTML tags (<SUB>, <SUP>, etc.) that corrupt tokenizer spans
- The snippet_multiplier config in DatasetBuilderConfig is defined but not implemented
- Pre-commit hook may flag "secrets" — use --no-verify for commits

---

## 2026-01-27 - US-001: Strip HTML from ADS abstracts
- Implemented `strip_html_tags(text)` and `clean_abstract_and_remap_spans(text, spans)` in `scripts/html_utils.py`
- Handles: `<SUB>`, `<SUP>`, `<MML>`, `<P>`, `<BR/>`, `<A href>`, self-closing tags, nested tags, HTML entities (`&amp;`, `&lt;`, `&gt;`, numeric/hex entities)
- Created `scripts/clean_ads_abstracts.py` to process data files
- Updated `data/evaluation/ads_sample_raw.jsonl` — added `abstract_clean` field (45 of 100 records had HTML cleaned)
- Updated `data/evaluation/ads_sample_annotated.jsonl` — added `abstract_clean` field, remapped 4,793 spans (2 dropped as unmappable, 2,093 offsets changed), **0 validation failures** across all spans
- 32 unit tests in `tests/scripts/test_html_utils.py` — all passing
- Files changed: `scripts/html_utils.py` (new), `scripts/clean_ads_abstracts.py` (new), `tests/scripts/test_html_utils.py` (new), `data/evaluation/ads_sample_raw.jsonl` (modified), `data/evaluation/ads_sample_annotated.jsonl` (modified)
- **Learnings for future iterations:**
  - ADS HTML tags: `<SUB>` (306), `<SUP>` (195), `<MML>` (168), `<P>` (16), `<INLINE>` (16), `<BR>` (16), `<A>` (4) — subscript/superscript dominate
  - HTML entities: `&lt;` (23), `&gt;` (23), `&amp;` (3) — only 3 types found
  - Regex for tag matching must handle self-closing `/>`without leading whitespace (e.g., `<BR/>`)
  - Offset remapping via character-level mapping array works reliably; only 2 out of 4,795 spans couldn't be remapped (spans inside tag attributes)
  - The `html.unescape()` stdlib function handles all entity decoding
---

## 2026-01-27 - US-002: Curate SWEET vocabulary — automated filtering
- Implemented `scripts/curate_sweet_vocabulary.py` with 4-tier filtering:
  1. Remove labels < 4 characters (344 removed)
  2. Remove labels in extended stopword list (823 words) → 210 removed
  3. Remove labels in top 5,000 high-frequency English words (wordfreq) → 615 removed
  4. Remove single common English words (word_frequency > 1e-6) → 1,200 removed
- Total: 12,986 input → 10,617 curated (81.8% retained), 2,369 removed
- 475 borderline cases logged (4-6 char scientific terms that survived filtering)
- Output files produced in `data/vocabularies/`:
  - `sweet_curated.jsonl` — 10,617 surviving entries
  - `sweet_removed.jsonl` — 2,369 removed entries with `removal_reason` field
  - `sweet_curation_report.json` — statistics and config
  - `sweet_borderline.jsonl` — 475 borderline 4-6 char scientific terms
- 29 unit tests in `tests/scripts/test_curate_sweet_vocabulary.py` — all passing
- Files changed: `scripts/curate_sweet_vocabulary.py` (new), `tests/scripts/test_curate_sweet_vocabulary.py` (new), `data/vocabularies/sweet_curated.jsonl` (new), `data/vocabularies/sweet_removed.jsonl` (new), `data/vocabularies/sweet_borderline.jsonl` (new), `data/vocabularies/sweet_curation_report.json` (new)
- **Learnings for future iterations:**
  - wordfreq library (`pip install wordfreq`) provides `top_n_list('en', N)` and `word_frequency(word, 'en')` — used for both high-freq list and per-word frequency checks
  - NLTK stopwords only has 198 words; extended to 823 by adding common function/glue words
  - SWEET has 9,209 single-word labels and 3,777 multi-word labels; multi-word labels are rarely common English
  - The `scripts/` directory is not a Python package; tests import via `sys.path.insert(0, scripts_dir)` pattern (matching test_html_utils.py)
  - Curated vocabulary path for downstream use: `data/vocabularies/sweet_curated.jsonl` — US-003 will need this
  - Pre-existing test_ner.py import error (DATABASE_SYNONYMS) is unrelated to this work
---

## 2026-01-27 - US-003: Re-annotate abstracts with curated vocabulary + HTML preprocessing
- Implemented `scripts/reannotate_ads_abstracts.py` which:
  1. Loads curated SWEET vocabulary (10,617 entries) instead of full SWEET (12,986)
  2. Loads UAT, GCMD, ROR, planetary catalogs unchanged from normalized directory
  3. Re-annotates all 100 abstracts using `abstract_clean` (HTML-stripped) text
  4. Produces comparison stats (before vs after curation)
- Re-annotation results:
  - **SWEET spans: 3,393 → 407 (88.0% reduction)** — exceeds expected 50-70%
  - **Total spans: 4,793 → 2,262 (52.8% reduction)**
  - UAT: 729 → 728 (essentially unchanged)
  - GCMD: 129 → 268 (increased — freed positions from removed SWEET matches)
  - ROR: 442 → 701 (increased — same reason)
  - Planetary: 100 → 158 (increased — same reason)
- All 2,262 spans validate with zero offset failures on `abstract_clean` text
- Output files:
  - `data/evaluation/ads_sample_reannotated.jsonl` — 100 re-annotated abstracts
  - `data/evaluation/ads_sample_reannotated.stats.json` — detailed comparison stats
- 17 unit tests in `tests/scripts/test_reannotate_ads_abstracts.py` — all passing
- Files changed: `scripts/reannotate_ads_abstracts.py` (new), `tests/scripts/test_reannotate_ads_abstracts.py` (new), `data/evaluation/ads_sample_reannotated.jsonl` (new), `data/evaluation/ads_sample_reannotated.stats.json` (new)
- **Learnings for future iterations:**
  - The `annotate_ads_abstracts.py` module is directly importable: `from annotate_ads_abstracts import CatalogEntry, build_keyword_index, find_annotation_spans, load_catalog_entries`
  - Non-SWEET vocab span counts increase after SWEET curation because the greedy longest-match-first algorithm's `occupied` set has fewer positions blocked by removed SWEET matches, freeing space for GCMD/ROR/planetary matches
  - Curated SWEET JSONL uses the same schema as topic_catalog entries (id, label, aliases, source_vocabulary, domain_tags) — can use `load_catalog_entries()` or a thin wrapper
  - The 88% SWEET reduction (vs expected 50-70%) is because the annotator also has its own `STOPWORD_SURFACES` filter (177 words) that removes common English words at match time, compounding with the curated vocabulary's curation
  - Re-annotated file includes both `abstract` (raw HTML) and `abstract_clean` fields — downstream US-005/US-006 should use `abstract_clean` for display and `spans` are offset-aligned to it
---

## 2026-01-27 - US-004: Run NER model predictions on cleaned abstracts
- Implemented `scripts/predict_ads_abstracts.py` which:
  1. Loads trained SciBERT NER model from `output/enrichment_model/`
  2. Runs inference on all 100 cleaned abstracts using `abstract_clean` field
  3. Decodes BIO tags into character-level spans with confidence scores
  4. Confidence = max softmax probability of the B- tag that initiated each span
- Prediction results:
  - **2,753 total spans** across 100 abstracts (27.5 per abstract)
  - **2,685 topic spans (97.5%)**, 68 entity spans (2.5%)
  - **Average confidence: 0.7115**
  - All 100 abstracts have at least one predicted span
  - All 2,753 span offsets validate against `abstract_clean` text (zero failures)
- Used `max_seq_length=512` (vs training default of 256) to capture more of each abstract
- Output files:
  - `data/evaluation/ads_sample_predictions.jsonl` — 100 records with model-predicted spans
  - `data/evaluation/ads_sample_predictions.stats.json` — summary statistics
- 24 unit tests in `tests/scripts/test_predict_ads_abstracts.py` — all passing
- Files changed: `scripts/predict_ads_abstracts.py` (new), `tests/scripts/test_predict_ads_abstracts.py` (new), `data/evaluation/ads_sample_predictions.jsonl` (new), `data/evaluation/ads_sample_predictions.stats.json` (new)
- **Learnings for future iterations:**
  - The `predict_spans_for_text` function from `evaluate_enrichment_model.py` can be reused but lacks confidence scores — US-004 extends it with softmax-based confidence
  - The model heavily favors topic predictions (97.5%) over entity (2.5%) — likely because synthetic training data has proportionally more topic spans
  - Using `max_seq_length=512` for inference on full abstracts captures more text than the 256 used during training; the model still works because SciBERT supports up to 512 positions
  - The model's average confidence of 0.71 suggests moderate certainty — many spans have high confidence (>0.9) but some borderline ones bring the average down
  - Output schema includes: `bibcode, title, abstract_clean, domain_category, spans[{surface, start, end, type, confidence}]` — the `confidence` field is what differentiates predictions from auto-annotations
  - Mock-based tests for NER inference work well: mock the tokenizer to return controlled offset_mapping and the model to return specific logits, then verify span decoding
---

## 2026-01-27 - US-005: Build NER annotation dashboard HTML template
- Created `scripts/annotation_dashboard_template.html` — 1,240 lines, single HTML file, no build step
- Dashboard features:
  - **Stats bar**: Total abstracts, reviewed/pending counts, span counts by source (auto/model/agreement/user), accepted/rejected counts
  - **Filter controls**: Domain category dropdown (auto-populated), review status (pending/reviewed/skipped/has-disagreements), text search
  - **Abstract card**: Bibcode, title, domain tag, citation count in header; abstract text with color-coded span highlights
  - **Span highlighting**: Blue (auto-annotation), green (model prediction), purple (agreement), gold (user-added); rejected spans shown with strikethrough + opacity
  - **Span list panel**: All spans listed with surface text, type badge, source badge, confidence %, character offsets, accept/reject toggle buttons
  - **Text selection**: `window.getSelection()` captures mouse-selected text in abstract, popup offers type selection (topic/entity/institution/author/date_range)
  - **Click to toggle**: Clicking a highlighted span in abstract text toggles its accept/reject status
  - **Notes field**: Per-abstract textarea with auto-save
  - **localStorage**: Persists under key `nls-ner-annotations-v1` with `{annotations: {bibcode: {decisions, userSpans, notes}}, abstractStatus: {bibcode: status}}`
  - **Export**: Downloads JSONL with accepted spans per abstract (bibcode, title, abstract_clean, domain_category, spans, review_status, notes)
  - **Save/Load**: Save progress to JSON file, load from file to restore state
  - **Keyboard shortcuts**: Arrow keys for prev/next abstract, 1-5 for span type assignment (when popup open), R for reviewed, S for skip, Escape closes popup
  - **Navigation**: Single abstract view with prev/next buttons and position indicator
- Uses `/*DATA_PLACEHOLDER*/[]` marker for inline JSON injection by generation script (US-006)
- Data schema expects: `{bibcode, title, abstract_clean, domain_category, citation_count, auto_spans: [{surface, start, end, type, canonical_id?, source_vocabulary?}], model_spans: [{surface, start, end, type, confidence}]}`
- Agreement detection: Auto + model spans with same type and overlapping character ranges are merged as "agreement" source
- Follows review_live.html visual style: light theme (#f5f5f5 background), card layout, Apple system font stack, event delegation pattern
- Browser-tested with 3 real abstracts (87 auto spans, 105 model spans, 58 agreements detected) — all rendering, stats, and accept interactions working correctly
- Files changed: `scripts/annotation_dashboard_template.html` (new)
- **Learnings for future iterations:**
  - The template expects `auto_spans` and `model_spans` as separate arrays per record — the generation script (US-006) must merge reannotated spans as `auto_spans` and prediction spans as `model_spans`
  - Agreement detection uses overlap + same type heuristic: for each auto span, if any model span overlaps and has the same `type`, both are merged into a single "agreement" span (auto span gets source='agreement', model span is hidden)
  - Non-overlapping span flattening is used for text highlighting to avoid nested HTML; the first span wins when spans overlap in the display
  - `window.getSelection().toString()` returns the raw text; to find the character offset, we search for the selected text in `abstract_clean` — this uses `indexOf` which returns the first occurrence (may be ambiguous for repeated phrases)
  - The export format is JSONL (one JSON object per line) with only accepted spans — this is the input for US-008's training data export
  - localStorage state has two top-level keys: `annotations` (per-bibcode span decisions + user spans + notes) and `abstractStatus` (per-bibcode review status)
---
